{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# ✅ MULTIMODAL FUSION MODEL FOR BEST F1 SCORE\n",
    "# ================================================\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModel, AutoImageProcessor, SwinForImageClassification\n",
    "from torch.optim import AdamW\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# ✅ PATHS & SETUP\n",
    "# ================================================\n",
    "image_dir = \"/kaggle/input/basem/images\"\n",
    "input_csv = \"/kaggle/input/basem/dataset.csv\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ================================================\n",
    "# ✅ LOAD & PREPROCESS CSV\n",
    "# ================================================\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "existing_data = []\n",
    "for _, row in df.iterrows():\n",
    "    image_filename = row['image_path']\n",
    "    full_image_path = os.path.join(image_dir, image_filename)\n",
    "    if os.path.exists(full_image_path):\n",
    "        label_converted = row['label 2'] - 1\n",
    "        existing_data.append({\n",
    "            'Image_path': full_image_path,\n",
    "            'Captions': row['extracted_text'],\n",
    "            'Label_Sentiment': label_converted\n",
    "        })\n",
    "\n",
    "processed_df = pd.DataFrame(existing_data)\n",
    "\n",
    "# ================================================\n",
    "# ✅ TEXT CLEANING\n",
    "# ================================================\n",
    "import unicodedata\n",
    "from bnlp.corpus import stopwords\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# IndicTrans2 English-to-Bangla translation model\n",
    "model_name = \"ai4bharat/indictrans2-en-indic-1B\"\n",
    "tokenizer_indic = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model_indic = AutoModelForSeq2SeqLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "def translate_en_to_bn(text):\n",
    "    # Translate only if English is detected\n",
    "    if re.search(r'[a-zA-Z]', text):\n",
    "        input_text = f\"<2bn> {text}\"\n",
    "        inputs = tokenizer_indic(input_text, return_tensors=\"pt\")\n",
    "        output = model_indic.generate(**inputs, max_length=256)\n",
    "        translated = tokenizer_indic.decode(output[0], skip_special_tokens=True)\n",
    "        return translated\n",
    "    return text\n",
    "\n",
    "def normalize_bangla(text):\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    # Add more normalization rules if needed\n",
    "    return text\n",
    "\n",
    "def remove_bangla_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered = [w for w in words if w not in stopwords]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    # Remove URLs and HTML\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove extra spaces\n",
    "    text = \" \".join(text.split())\n",
    "    # Translate English to Bangla\n",
    "    text = translate_en_to_bn(text)\n",
    "    # Remove irrelevant characters (keep Bangla, numbers, and spaces)\n",
    "    text = re.sub(r'[^\\u0980-\\u09FF0-9 ]+', '', text)\n",
    "    text = normalize_bangla(text)\n",
    "    text = remove_bangla_stopwords(text)\n",
    "    # Remove extra spaces again\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "# ================================================\n",
    "# ✅ DATA SPLITS\n",
    "# ================================================\n",
    "train_df, temp_df = train_test_split(processed_df, test_size=0.3, stratify=processed_df['Label_Sentiment'], random_state=42)\n",
    "test_df, val_df = train_test_split(temp_df, test_size=1/3, stratify=temp_df['Label_Sentiment'], random_state=42)\n",
    "\n",
    "for df_name, df_ in [('train', train_df), ('test', test_df), ('val', val_df)]:\n",
    "    df_['Captions'] = df_['Captions'].astype(str).apply(clean_text)\n",
    "    df_['label'] = df_['Label_Sentiment']\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}, Val samples: {len(val_df)}, Test samples: {len(test_df)}\")\n",
    "print(f\"Class distribution: {train_df['label'].value_counts().sort_index().tolist()}\")\n",
    "\n",
    "# ================================================\n",
    "# ✅ LOAD MODELS\n",
    "# ================================================\n",
    "# Load BanglishBERT for text\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglabert\")\n",
    "bert_model = AutoModel.from_pretrained(\"csebuetnlp/banglabert\")\n",
    "\n",
    "# Load Swin Transformer for images\n",
    "swin_model_name = \"microsoft/swin-base-patch4-window7-224\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(swin_model_name)\n",
    "swin_backbone = SwinForImageClassification.from_pretrained(\n",
    "    swin_model_name,\n",
    "    num_labels=3,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# ================================================\n",
    "# ✅ MULTIMODAL FUSION MODEL\n",
    "# ================================================\n",
    "class MultimodalFusionModel(nn.Module):\n",
    "    def __init__(self, bert_model, swin_model, num_classes=3, dropout_rate=0.3, fusion_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text encoder\n",
    "        self.bert = bert_model\n",
    "        self.text_dropout = nn.Dropout(dropout_rate)\n",
    "        self.text_projector = nn.Linear(bert_model.config.hidden_size, fusion_dim)\n",
    "        \n",
    "        # Image encoder - use Swin backbone without classifier\n",
    "        self.swin_backbone = swin_model.swin\n",
    "        self.image_dropout = nn.Dropout(dropout_rate)\n",
    "        self.image_projector = nn.Linear(swin_model.config.hidden_size, fusion_dim)\n",
    "        \n",
    "        # Fusion layers\n",
    "        self.fusion_dropout = nn.Dropout(dropout_rate)\n",
    "        self.fusion_layer1 = nn.Linear(fusion_dim * 2, fusion_dim)\n",
    "        self.fusion_layer2 = nn.Linear(fusion_dim, fusion_dim // 2)\n",
    "        self.batch_norm = nn.BatchNorm1d(fusion_dim // 2)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(fusion_dim // 2, num_classes)\n",
    "        \n",
    "        # Attention mechanism for fusion\n",
    "        self.attention_weights = nn.Linear(fusion_dim * 2, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        # Text encoding\n",
    "        text_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        text_features = self.text_dropout(text_features)\n",
    "        text_projected = self.text_projector(text_features)\n",
    "        \n",
    "        # Image encoding\n",
    "        image_outputs = self.swin_backbone(pixel_values)\n",
    "        image_features = image_outputs.last_hidden_state.mean(dim=1)  # Global average pooling\n",
    "        image_features = self.image_dropout(image_features)\n",
    "        image_projected = self.image_projector(image_features)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([text_projected, image_projected], dim=1)\n",
    "        \n",
    "        # Attention-based fusion\n",
    "        attention_scores = F.softmax(self.attention_weights(combined_features), dim=1)\n",
    "        text_att = attention_scores[:, 0:1]\n",
    "        image_att = attention_scores[:, 1:2]\n",
    "        \n",
    "        # Weighted fusion\n",
    "        fused_features = text_att * text_projected + image_att * image_projected\n",
    "        \n",
    "        # Additional fusion processing\n",
    "        fusion_out = F.relu(self.fusion_layer1(combined_features))\n",
    "        fusion_out = self.fusion_dropout(fusion_out)\n",
    "        fusion_out = F.relu(self.fusion_layer2(fusion_out))\n",
    "        fusion_out = self.batch_norm(fusion_out)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(fusion_out)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# ================================================\n",
    "# ✅ MULTIMODAL DATASET\n",
    "# ================================================\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, processor, max_length=128, is_train=False):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.is_train = is_train\n",
    "        # Define augmentations for training\n",
    "        self.train_transforms = T.Compose([\n",
    "            T.RandomRotation(15),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "            T.RandomAdjustSharpness(sharpness_factor=2),\n",
    "            # You can add more or adjust parameters as needed\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Text processing\n",
    "        caption = row['Captions']\n",
    "        text_inputs = self.tokenizer(\n",
    "            caption,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # Image processing\n",
    "        image = Image.open(row['Image_path']).convert('RGB')\n",
    "        if self.is_train:\n",
    "            image = self.train_transforms(image)\n",
    "        image_inputs = self.processor(image, return_tensors=\"pt\")\n",
    "        return {\n",
    "            'input_ids': text_inputs['input_ids'].flatten(),\n",
    "            'attention_mask': text_inputs['attention_mask'].flatten(),\n",
    "            'pixel_values': image_inputs['pixel_values'].squeeze(0),\n",
    "            'label': torch.tensor(row['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ================================================\n",
    "# ✅ DATALOADERS\n",
    "# ================================================\n",
    "batch_size = 8\n",
    "\n",
    "train_dataset = MultimodalDataset(train_df, bert_tokenizer, image_processor, is_train=True)\n",
    "val_dataset = MultimodalDataset(val_df, bert_tokenizer, image_processor, is_train=False)\n",
    "test_dataset = MultimodalDataset(test_df, bert_tokenizer, image_processor, is_train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# ================================================\n",
    "# ✅ INITIALIZE MODEL\n",
    "# ================================================\n",
    "model = MultimodalFusionModel(bert_model, swin_backbone, num_classes=3, dropout_rate=0.3).to(device)\n",
    "\n",
    "# ================================================\n",
    "# ✅ LOSS & OPTIMIZER WITH ADVANCED TECHNIQUES\n",
    "# ================================================\n",
    "# Focal Loss for handling class imbalance\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = train_df['label'].value_counts().sort_index().tolist()\n",
    "total_samples = sum(class_counts)\n",
    "class_weights = [total_samples / count for count in class_counts]\n",
    "alpha = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "# Use Focal Loss for better handling of class imbalance\n",
    "criterion = FocalLoss(alpha=alpha, gamma=2.0)\n",
    "\n",
    "# Optimizer with different learning rates for different parts\n",
    "text_params = list(model.bert.parameters())\n",
    "image_params = list(model.swin_backbone.parameters())\n",
    "fusion_params = list(model.text_projector.parameters()) + list(model.image_projector.parameters()) + \\\n",
    "               list(model.fusion_layer1.parameters()) + list(model.fusion_layer2.parameters()) + \\\n",
    "               list(model.classifier.parameters()) + list(model.attention_weights.parameters())\n",
    "\n",
    "optimizer = AdamW([\n",
    "    {'params': text_params, 'lr': 2e-5},\n",
    "    {'params': image_params, 'lr': 1e-5},\n",
    "    {'params': fusion_params, 'lr': 5e-4}\n",
    "], weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n",
    "\n",
    "# ================================================\n",
    "# ✅ TRAINING LOOP WITH ADVANCED TECHNIQUES\n",
    "# ================================================\n",
    "num_epochs = 25\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "best_val_f1 = 0.0\n",
    "\n",
    "print(\"🚀 Starting Multimodal Fusion Training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ============================================================\n",
    "    # TRAINING PHASE\n",
    "    # ============================================================\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_predictions = []\n",
    "    train_labels = []\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask, pixel_values)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        train_predictions.extend(predictions.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "    train_f1 = precision_recall_fscore_support(train_labels, train_predictions, average='weighted')[2]\n",
    "\n",
    "    # ============================================================\n",
    "    # VALIDATION PHASE\n",
    "    # ============================================================\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, pixel_values)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            val_predictions.extend(predictions.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(val_labels, val_predictions, average='weighted')\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f} | Train F1: {train_f1:.4f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # EARLY STOPPING BASED ON F1 SCORE\n",
    "    # ============================================================\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_multimodal_model.pt\")\n",
    "        print(f\"✅ Validation F1 improved to {val_f1:.4f} — model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"⏰ No improvement — patience {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"🛑 Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# ================================================\n",
    "# ✅ FINAL TEST EVALUATION\n",
    "# ================================================\n",
    "print(\"\\n🔍 Loading best model for final evaluation...\")\n",
    "model.load_state_dict(torch.load(\"best_multimodal_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "total_test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Final Test Evaluation\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        logits = model(input_ids, attention_mask, pixel_values)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        total_test_loss += loss.item()\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='weighted')\n",
    "test_precision_macro, test_recall_macro, test_f1_macro, _ = precision_recall_fscore_support(test_labels, test_predictions, average='macro')\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Per-class metrics\n",
    "precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "    test_labels, test_predictions, average=None\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎯 FINAL MULTIMODAL FUSION TEST RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test F1-Score (Weighted): {test_f1:.4f}\")\n",
    "print(f\"Test F1-Score (Macro): {test_f1_macro:.4f}\")\n",
    "print(f\"Test Precision (Weighted): {test_precision:.4f}\")\n",
    "print(f\"Test Recall (Weighted): {test_recall:.4f}\")\n",
    "print(f\"Test Loss: {total_test_loss/len(test_loader):.4f}\")\n",
    "\n",
    "print(\"\\n📈 Per-Class Metrics:\")\n",
    "class_names = ['Negative', 'Neutral', 'Positive']\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"{class_name:>8}: Precision={precision_per_class[i]:.4f}, Recall={recall_per_class[i]:.4f}, F1={f1_per_class[i]:.4f}, Support={support[i]}\")\n",
    "\n",
    "print(f\"\\n🎯 Confusion Matrix:\")\n",
    "print(f\"{'':>10} {'Neg':>6} {'Neu':>6} {'Pos':>6}\")\n",
    "for i, class_name in enumerate(['Negative', 'Neutral', 'Positive']):\n",
    "    print(f\"{class_name:>10} {cm[i][0]:>6} {cm[i][1]:>6} {cm[i][2]:>6}\")\n",
    "\n",
    "print(\"\\n📋 Detailed Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions, target_names=class_names))\n",
    "\n",
    "# ================================================\n",
    "# ✅ SAVE RESULTS\n",
    "# ================================================\n",
    "results = {\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'test_f1_weighted': test_f1,\n",
    "    'test_f1_macro': test_f1_macro,\n",
    "    'test_precision_weighted': test_precision,\n",
    "    'test_recall_weighted': test_recall,\n",
    "    'test_loss': total_test_loss/len(test_loader),\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'per_class_metrics': {\n",
    "        'precision': precision_per_class.tolist(),\n",
    "        'recall': recall_per_class.tolist(),\n",
    "        'f1': f1_per_class.tolist(),\n",
    "        'support': support.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('/kaggle/working/multimodal_fusion_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ MULTIMODAL FUSION MODEL TRAINING COMPLETE!\")\n",
    "print(f\"🏆 Best F1 Score Achieved: {test_f1:.4f}\")\n",
    "print(\"📁 Results saved to 'multimodal_fusion_results.json'\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

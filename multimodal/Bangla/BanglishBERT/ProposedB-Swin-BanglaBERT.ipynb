{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-08T12:13:23.108897Z",
     "iopub.status.busy": "2025-07-08T12:13:23.108581Z",
     "iopub.status.idle": "2025-07-08T13:18:16.717051Z",
     "shell.execute_reply": "2025-07-08T13:18:16.716295Z",
     "shell.execute_reply.started": "2025-07-08T12:13:23.108866Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 12:13:35.887302: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751976816.066833      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751976816.118948      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train samples: 3156, Val samples: 451, Test samples: 902\n",
      "Class distribution: [1404, 1237, 515]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13410748f0e947a0aa6ab1fb1031e359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7c2bce56334442a0eeac6102c489d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1dfd7f181c4f0a93f148baafc9b503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f2e9eb91684007ad26d50973b82769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a895fdd10836434081ed443c0081f7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b036f11b134375b2735b59185035a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9d5a3a5dff437b85c5507292752594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd5294945d149d28df9d49d72394d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988fa375cf0f426db6cd9eb8c5673cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/352M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([3, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Multimodal Fusion Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [08:02<00:00,  1.22s/it]\n",
      "Validation Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:25<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25]\n",
      "  Train Loss: 1.3255 | Train Acc: 0.5776 | Train F1: 0.5931\n",
      "  Val Loss: 0.8414 | Val Acc: 0.7672 | Val F1: 0.7644\n",
      "  LR: 0.000020\n",
      "‚úÖ Validation F1 improved to 0.7644 ‚Äî model saved.\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [07:31<00:00,  1.14s/it]\n",
      "Validation Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:22<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25]\n",
      "  Train Loss: 0.8973 | Train Acc: 0.7579 | Train F1: 0.7613\n",
      "  Val Loss: 0.9583 | Val Acc: 0.7472 | Val F1: 0.7483\n",
      "  LR: 0.000020\n",
      "‚è∞ No improvement ‚Äî patience 1/5\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [07:29<00:00,  1.14s/it]\n",
      "Validation Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:22<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25]\n",
      "  Train Loss: 0.6946 | Train Acc: 0.8533 | Train F1: 0.8533\n",
      "  Val Loss: 1.3288 | Val Acc: 0.8027 | Val F1: 0.8024\n",
      "  LR: 0.000019\n",
      "‚úÖ Validation F1 improved to 0.8024 ‚Äî model saved.\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [07:26<00:00,  1.13s/it]\n",
      "Validation Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:22<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25]\n",
      "  Train Loss: 0.5633 | Train Acc: 0.9059 | Train F1: 0.9056\n",
      "  Val Loss: 1.9379 | Val Acc: 0.7982 | Val F1: 0.7952\n",
      "  LR: 0.000018\n",
      "‚è∞ No improvement ‚Äî patience 1/5\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [07:25<00:00,  1.13s/it]\n",
      "Validation Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:22<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25]\n",
      "  Train Loss: 0.4015 | Train Acc: 0.9436 | Train F1: 0.9435\n",
      "  Val Loss: 2.5544 | Val Acc: 0.8027 | Val F1: 0.8008\n",
      "  LR: 0.000017\n",
      "‚è∞ No improvement ‚Äî patience 2/5\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [07:24<00:00,  1.13s/it]\n",
      "Validation Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:22<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25]\n",
      "  Train Loss: 0.2953 | Train Acc: 0.9670 | Train F1: 0.9670\n",
      "  Val Loss: 2.9373 | Val Acc: 0.8027 | Val F1: 0.7997\n",
      "  LR: 0.000016\n",
      "‚è∞ No improvement ‚Äî patience 3/5\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [07:26<00:00,  1.13s/it]\n",
      "Validation Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:22<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25]\n",
      "  Train Loss: 0.2210 | Train Acc: 0.9785 | Train F1: 0.9784\n",
      "  Val Loss: 3.1611 | Val Acc: 0.7871 | Val F1: 0.7855\n",
      "  LR: 0.000015\n",
      "‚è∞ No improvement ‚Äî patience 4/5\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [07:23<00:00,  1.12s/it]\n",
      "Validation Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:22<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25]\n",
      "  Train Loss: 0.1707 | Train Acc: 0.9826 | Train F1: 0.9826\n",
      "  Val Loss: 3.2450 | Val Acc: 0.7916 | Val F1: 0.7894\n",
      "  LR: 0.000013\n",
      "‚è∞ No improvement ‚Äî patience 5/5\n",
      "üõë Early stopping triggered at epoch 8\n",
      "\n",
      "üîç Loading best model for final evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Test Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 113/113 [00:49<00:00,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üéØ FINAL MULTIMODAL FUSION TEST RESULTS\n",
      "======================================================================\n",
      "Test Accuracy: 0.8093\n",
      "Test F1-Score (Weighted): 0.8061\n",
      "Test F1-Score (Macro): 0.7707\n",
      "Test Precision (Weighted): 0.8127\n",
      "Test Recall (Weighted): 0.8093\n",
      "Test Loss: 1.1270\n",
      "\n",
      "üìà Per-Class Metrics:\n",
      "Negative: Precision=0.8880, Recall=0.8085, F1=0.8464, Support=402\n",
      " Neutral: Precision=0.7648, Recall=0.9122, F1=0.8320, Support=353\n",
      "Positive: Precision=0.7217, Recall=0.5646, F1=0.6336, Support=147\n",
      "\n",
      "üéØ Confusion Matrix:\n",
      "              Neg    Neu    Pos\n",
      "  Negative    325     56     21\n",
      "   Neutral     20    322     11\n",
      "  Positive     21     43     83\n",
      "\n",
      "üìã Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.81      0.85       402\n",
      "     Neutral       0.76      0.91      0.83       353\n",
      "    Positive       0.72      0.56      0.63       147\n",
      "\n",
      "    accuracy                           0.81       902\n",
      "   macro avg       0.79      0.76      0.77       902\n",
      "weighted avg       0.81      0.81      0.81       902\n",
      "\n",
      "\n",
      "======================================================================\n",
      "‚úÖ MULTIMODAL FUSION MODEL TRAINING COMPLETE!\n",
      "üèÜ Best F1 Score Achieved: 0.8061\n",
      "üìÅ Results saved to 'multimodal_fusion_results.json'\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# ‚úÖ MULTIMODAL FUSION MODEL FOR BEST F1 SCORE\n",
    "# ================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModel, AutoImageProcessor, SwinForImageClassification\n",
    "from torch.optim import AdamW\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "# ================================================\n",
    "# ‚úÖ PATHS & SETUP\n",
    "# ================================================\n",
    "image_dir = \"/kaggle/input/basem/images\"\n",
    "input_csv = \"/kaggle/input/basem/dataset.csv\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ================================================\n",
    "# ‚úÖ LOAD & PREPROCESS CSV\n",
    "# ================================================\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "existing_data = []\n",
    "for _, row in df.iterrows():\n",
    "    image_filename = row['image_path']\n",
    "    full_image_path = os.path.join(image_dir, image_filename)\n",
    "    if os.path.exists(full_image_path):\n",
    "        label_converted = row['label 2'] - 1\n",
    "        existing_data.append({\n",
    "            'Image_path': full_image_path,\n",
    "            'Captions': row['extracted_text'],\n",
    "            'Label_Sentiment': label_converted\n",
    "        })\n",
    "\n",
    "processed_df = pd.DataFrame(existing_data)\n",
    "\n",
    "# ================================================\n",
    "# ‚úÖ TEXT CLEANING\n",
    "# ================================================\n",
    "def clean_text(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "# ================================================\n",
    "# ‚úÖ DATA SPLITS\n",
    "# ================================================\n",
    "train_df, temp_df = train_test_split(processed_df, test_size=0.3, stratify=processed_df['Label_Sentiment'], random_state=42)\n",
    "test_df, val_df = train_test_split(temp_df, test_size=1/3, stratify=temp_df['Label_Sentiment'], random_state=42)\n",
    "\n",
    "for df_name, df_ in [('train', train_df), ('test', test_df), ('val', val_df)]:\n",
    "    df_['Captions'] = df_['Captions'].astype(str).apply(clean_text)\n",
    "    df_['label'] = df_['Label_Sentiment']\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}, Val samples: {len(val_df)}, Test samples: {len(test_df)}\")\n",
    "print(f\"Class distribution: {train_df['label'].value_counts().sort_index().tolist()}\")\n",
    "\n",
    "# ================================================\n",
    "# ‚úÖ LOAD MODELS\n",
    "# ================================================\n",
    "# Load BanglishBERT for text\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglabert\")\n",
    "bert_model = AutoModel.from_pretrained(\"csebuetnlp/banglabert\")\n",
    "\n",
    "# Load Swin Transformer for images\n",
    "swin_model_name = \"microsoft/swin-base-patch4-window7-224\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(swin_model_name)\n",
    "swin_backbone = SwinForImageClassification.from_pretrained(\n",
    "    swin_model_name,\n",
    "    num_labels=3,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# ================================================\n",
    "# ‚úÖ MULTIMODAL FUSION MODEL\n",
    "# ================================================\n",
    "class MultimodalFusionModel(nn.Module):\n",
    "    def __init__(self, bert_model, swin_model, num_classes=3, dropout_rate=0.3, fusion_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text encoder\n",
    "        self.bert = bert_model\n",
    "        self.text_dropout = nn.Dropout(dropout_rate)\n",
    "        self.text_projector = nn.Linear(bert_model.config.hidden_size, fusion_dim)\n",
    "        \n",
    "        # Image encoder - use Swin backbone without classifier\n",
    "        self.swin_backbone = swin_model.swin\n",
    "        self.image_dropout = nn.Dropout(dropout_rate)\n",
    "        self.image_projector = nn.Linear(swin_model.config.hidden_size, fusion_dim)\n",
    "        \n",
    "        # Fusion layers\n",
    "        self.fusion_dropout = nn.Dropout(dropout_rate)\n",
    "        self.fusion_layer1 = nn.Linear(fusion_dim * 2, fusion_dim)\n",
    "        self.fusion_layer2 = nn.Linear(fusion_dim, fusion_dim // 2)\n",
    "        self.batch_norm = nn.BatchNorm1d(fusion_dim // 2)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(fusion_dim // 2, num_classes)\n",
    "        \n",
    "        # Attention mechanism for fusion\n",
    "        self.attention_weights = nn.Linear(fusion_dim * 2, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        # Text encoding\n",
    "        text_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        text_features = self.text_dropout(text_features)\n",
    "        text_projected = self.text_projector(text_features)\n",
    "        \n",
    "        # Image encoding\n",
    "        image_outputs = self.swin_backbone(pixel_values)\n",
    "        image_features = image_outputs.last_hidden_state.mean(dim=1)  # Global average pooling\n",
    "        image_features = self.image_dropout(image_features)\n",
    "        image_projected = self.image_projector(image_features)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([text_projected, image_projected], dim=1)\n",
    "        \n",
    "        # # Attention-based fusion\n",
    "        # attention_scores = F.softmax(self.attention_weights(combined_features), dim=1)\n",
    "        # text_att = attention_scores[:, 0:1]\n",
    "        # image_att = attention_scores[:, 1:2]\n",
    "        \n",
    "        # # Weighted fusion\n",
    "        # fused_features = text_att * text_projected + image_att * image_projected\n",
    "        \n",
    "        # Additional fusion processing\n",
    "        fusion_out = F.relu(self.fusion_layer1(combined_features))\n",
    "        fusion_out = self.fusion_dropout(fusion_out)\n",
    "        fusion_out = F.relu(self.fusion_layer2(fusion_out))\n",
    "        fusion_out = self.batch_norm(fusion_out)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(fusion_out)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# ================================================\n",
    "# ‚úÖ MULTIMODAL DATASET\n",
    "# ================================================\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, processor, max_length=128, is_train=False):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.is_train = is_train\n",
    "        # Define augmentations for training\n",
    "        self.train_transforms = T.Compose([\n",
    "            T.RandomRotation(15),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "            T.RandomAdjustSharpness(sharpness_factor=2),\n",
    "            # You can add more or adjust parameters as needed\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Text processing\n",
    "        caption = row['Captions']\n",
    "        text_inputs = self.tokenizer(\n",
    "            caption,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # Image processing\n",
    "        image = Image.open(row['Image_path']).convert('RGB')\n",
    "        if self.is_train:\n",
    "            image = self.train_transforms(image)\n",
    "        image_inputs = self.processor(image, return_tensors=\"pt\")\n",
    "        return {\n",
    "            'input_ids': text_inputs['input_ids'].flatten(),\n",
    "            'attention_mask': text_inputs['attention_mask'].flatten(),\n",
    "            'pixel_values': image_inputs['pixel_values'].squeeze(0),\n",
    "            'label': torch.tensor(row['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ================================================\n",
    "# ‚úÖ DATALOADERS\n",
    "# ================================================\n",
    "batch_size = 8\n",
    "\n",
    "train_dataset = MultimodalDataset(train_df, bert_tokenizer, image_processor, is_train=True)\n",
    "val_dataset = MultimodalDataset(val_df, bert_tokenizer, image_processor, is_train=False)\n",
    "test_dataset = MultimodalDataset(test_df, bert_tokenizer, image_processor, is_train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# ================================================\n",
    "# ‚úÖ INITIALIZE MODEL\n",
    "# ================================================\n",
    "model = MultimodalFusionModel(bert_model, swin_backbone, num_classes=3, dropout_rate=0.3).to(device)\n",
    "\n",
    "# ================================================\n",
    "# ‚úÖ LOSS & OPTIMIZER WITH ADVANCED TECHNIQUES\n",
    "# ================================================\n",
    "# Focal Loss for handling class imbalance\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha[targets]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = train_df['label'].value_counts().sort_index().tolist()\n",
    "total_samples = sum(class_counts)\n",
    "class_weights = [total_samples / count for count in class_counts]\n",
    "alpha = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "# Use Focal Loss for better handling of class imbalance\n",
    "criterion = FocalLoss(alpha=alpha, gamma=2.0)\n",
    "\n",
    "# Optimizer with different learning rates for different parts\n",
    "text_params = list(model.bert.parameters())\n",
    "image_params = list(model.swin_backbone.parameters())\n",
    "fusion_params = list(model.text_projector.parameters()) + list(model.image_projector.parameters()) + \\\n",
    "               list(model.fusion_layer1.parameters()) + list(model.fusion_layer2.parameters()) + \\\n",
    "               list(model.classifier.parameters()) + list(model.attention_weights.parameters())\n",
    "\n",
    "optimizer = AdamW([\n",
    "    {'params': text_params, 'lr': 2e-5},\n",
    "    {'params': image_params, 'lr': 1e-5},\n",
    "    {'params': fusion_params, 'lr': 5e-4}\n",
    "], weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n",
    "\n",
    "# ================================================\n",
    "# ‚úÖ TRAINING LOOP WITH ADVANCED TECHNIQUES\n",
    "# ================================================\n",
    "num_epochs = 25\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "best_val_f1 = 0.0\n",
    "\n",
    "print(\"üöÄ Starting Multimodal Fusion Training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ============================================================\n",
    "    # TRAINING PHASE\n",
    "    # ============================================================\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_predictions = []\n",
    "    train_labels = []\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask, pixel_values)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        train_predictions.extend(predictions.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "    train_f1 = precision_recall_fscore_support(train_labels, train_predictions, average='weighted')[2]\n",
    "\n",
    "    # ============================================================\n",
    "    # VALIDATION PHASE\n",
    "    # ============================================================\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, pixel_values)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            val_predictions.extend(predictions.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(val_labels, val_predictions, average='weighted')\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f} | Train F1: {train_f1:.4f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f} | Val F1: {val_f1:.4f}\")\n",
    "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # EARLY STOPPING BASED ON F1 SCORE\n",
    "    # ============================================================\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_multimodal_model.pt\")\n",
    "        print(f\"‚úÖ Validation F1 improved to {val_f1:.4f} ‚Äî model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"‚è∞ No improvement ‚Äî patience {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"üõë Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# ================================================\n",
    "# ‚úÖ FINAL TEST EVALUATION\n",
    "# ================================================\n",
    "print(\"\\nüîç Loading best model for final evaluation...\")\n",
    "model.load_state_dict(torch.load(\"best_multimodal_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "total_test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Final Test Evaluation\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        logits = model(input_ids, attention_mask, pixel_values)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        total_test_loss += loss.item()\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='weighted')\n",
    "test_precision_macro, test_recall_macro, test_f1_macro, _ = precision_recall_fscore_support(test_labels, test_predictions, average='macro')\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Per-class metrics\n",
    "precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "    test_labels, test_predictions, average=None\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ FINAL MULTIMODAL FUSION TEST RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test F1-Score (Weighted): {test_f1:.4f}\")\n",
    "print(f\"Test F1-Score (Macro): {test_f1_macro:.4f}\")\n",
    "print(f\"Test Precision (Weighted): {test_precision:.4f}\")\n",
    "print(f\"Test Recall (Weighted): {test_recall:.4f}\")\n",
    "print(f\"Test Loss: {total_test_loss/len(test_loader):.4f}\")\n",
    "\n",
    "print(\"\\nüìà Per-Class Metrics:\")\n",
    "class_names = ['Negative', 'Neutral', 'Positive']\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"{class_name:>8}: Precision={precision_per_class[i]:.4f}, Recall={recall_per_class[i]:.4f}, F1={f1_per_class[i]:.4f}, Support={support[i]}\")\n",
    "\n",
    "print(f\"\\nüéØ Confusion Matrix:\")\n",
    "print(f\"{'':>10} {'Neg':>6} {'Neu':>6} {'Pos':>6}\")\n",
    "for i, class_name in enumerate(['Negative', 'Neutral', 'Positive']):\n",
    "    print(f\"{class_name:>10} {cm[i][0]:>6} {cm[i][1]:>6} {cm[i][2]:>6}\")\n",
    "\n",
    "print(\"\\nüìã Detailed Classification Report:\")\n",
    "print(classification_report(test_labels, test_predictions, target_names=class_names))\n",
    "\n",
    "# ================================================\n",
    "# ‚úÖ SAVE RESULTS\n",
    "# ================================================\n",
    "results = {\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'test_f1_weighted': test_f1,\n",
    "    'test_f1_macro': test_f1_macro,\n",
    "    'test_precision_weighted': test_precision,\n",
    "    'test_recall_weighted': test_recall,\n",
    "    'test_loss': total_test_loss/len(test_loader),\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'per_class_metrics': {\n",
    "        'precision': precision_per_class.tolist(),\n",
    "        'recall': recall_per_class.tolist(),\n",
    "        'f1': f1_per_class.tolist(),\n",
    "        'support': support.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('/kaggle/working/multimodal_fusion_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ MULTIMODAL FUSION MODEL TRAINING COMPLETE!\")\n",
    "print(f\"üèÜ Best F1 Score Achieved: {test_f1:.4f}\")\n",
    "print(\"üìÅ Results saved to 'multimodal_fusion_results.json'\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7789382,
     "sourceId": 12355137,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

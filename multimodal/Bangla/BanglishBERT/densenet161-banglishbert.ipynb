{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12355137,"sourceType":"datasetVersion","datasetId":7789382}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =============================================================\n# ‚úÖ 1Ô∏è‚É£ LIBRARIES & SETUP\n# =============================================================\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision import models, transforms\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nimport re\nimport string\n\n# =============================================================\n# ‚úÖ 2Ô∏è‚É£ DEVICE\n# =============================================================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# =============================================================\n# ‚úÖ 3Ô∏è‚É£ DATA PATHS\n# =============================================================\nimage_dir = \"/kaggle/input/basem/images\"\ninput_csv = \"/kaggle/input/basem/dataset.csv\"\n\n# =============================================================\n# ‚úÖ 4Ô∏è‚É£ LOAD DATA\n# =============================================================\ndf = pd.read_csv(input_csv)\n\nexisting_data = []\nfor _, row in df.iterrows():\n    image_filename = row['image_path']\n    full_image_path = os.path.join(image_dir, image_filename)\n    if os.path.exists(full_image_path):\n        label_converted = row['label 2'] - 1\n        existing_data.append({\n            'Image_path': full_image_path,\n            'Captions': row['extracted_text'],\n            'Label': label_converted\n        })\n\nprocessed_df = pd.DataFrame(existing_data)\n\ntrain_df, temp_df = train_test_split(processed_df, test_size=0.3, stratify=processed_df['Label'], random_state=42)\ntest_df, val_df = train_test_split(temp_df, test_size=1/3, stratify=temp_df['Label'], random_state=42)\n\n# =============================================================\n# ‚úÖ 5Ô∏è‚É£ TEXT TOKENIZER & IMAGE TRANSFORMS\n# =============================================================\nbert_tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n\ntransform_train = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\ntransform_val = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# =============================================================\n# ‚úÖ 6Ô∏è‚É£ MULTIMODAL DATASET\n# =============================================================\nclass MultimodalDataset(Dataset):\n    def __init__(self, df, tokenizer, transform, max_length=128):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.transform = transform\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(row['Image_path']).convert('RGB')\n        image = self.transform(image)\n\n        caption = row['Captions']\n        caption = re.sub(r'https?://\\S+|www\\.\\S+', '', caption)\n        caption = re.sub(r'<.*?>', '', caption)\n        caption = caption.translate(str.maketrans('', '', string.punctuation))\n        caption = \" \".join(caption.split())\n\n        inputs = self.tokenizer(\n            caption, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt'\n        )\n\n        label = torch.tensor(row['Label'], dtype=torch.long)\n\n        return {\n            'image': image,\n            'input_ids': inputs['input_ids'].squeeze(),\n            'attention_mask': inputs['attention_mask'].squeeze(),\n            'label': label\n        }\n\n# =============================================================\n# ‚úÖ 7Ô∏è‚É£ DATALOADERS\n# =============================================================\nbatch_size = 16\ntrain_ds = MultimodalDataset(train_df, bert_tokenizer, transform_train)\nval_ds = MultimodalDataset(val_df, bert_tokenizer, transform_val)\ntest_ds = MultimodalDataset(test_df, bert_tokenizer, transform_val)\n\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=2)\ntest_loader = DataLoader(test_ds, batch_size=batch_size, num_workers=2)\n\n# =============================================================\n# ‚úÖ 8Ô∏è‚É£ MULTIMODAL MODEL\n# =============================================================\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, text_model, vision_model, num_classes=3):\n        super().__init__()\n        self.text_model = text_model\n        self.vision_model = vision_model\n\n        text_hidden = text_model.config.hidden_size\n        vision_hidden = vision_model.classifier.in_features\n\n        self.vision_model.classifier = nn.Identity()\n\n        self.classifier = nn.Sequential(\n            nn.Linear(text_hidden + vision_hidden, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, input_ids, attention_mask, images):\n        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n        text_emb = text_outputs.last_hidden_state[:, 0, :]\n\n        vision_emb = self.vision_model(images)\n\n        combined = torch.cat((text_emb, vision_emb), dim=1)\n        logits = self.classifier(combined)\n        return logits\n\n# Load pretrained components\ntext_model = AutoModel.from_pretrained(\"sagorsarker/bangla-bert-base\").to(device)\nvision_model = models.densenet161(pretrained=True)\n\nmodel = MultimodalClassifier(text_model, vision_model, num_classes=3).to(device)\n\n# =============================================================\n# ‚úÖ 9Ô∏è‚É£ LOSS, OPTIMIZER, SCHEDULER\n# =============================================================\nclass_weights = train_df['Label'].value_counts().sort_index().tolist()\ntotal = sum(class_weights)\nweights = [total / c for c in class_weights]\ncriterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(weights).to(device))\noptimizer = AdamW(model.parameters(), lr=2e-5)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\n# =============================================================\n# ‚úÖ üîü TRAINING LOOP\n# =============================================================\nnum_epochs = 20\npatience = 3\npatience_counter = 0\nbest_val_loss = float('inf')\n\nprint(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    preds, labels = [], []\n\n    for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n        images = batch['image'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels_batch = batch['label'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, images)\n        loss = criterion(outputs, labels_batch)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n        labels.extend(labels_batch.cpu().numpy())\n\n    avg_train_loss = total_loss / len(train_loader)\n    train_acc = accuracy_score(labels, preds)\n\n    model.eval()\n    val_loss = 0\n    val_preds, val_labels = [], []\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Val Epoch {epoch+1}\"):\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels_batch = batch['label'].to(device)\n\n            outputs = model(input_ids, attention_mask, images)\n            loss = criterion(outputs, labels_batch)\n            val_loss += loss.item()\n\n            val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n            val_labels.extend(labels_batch.cpu().numpy())\n\n    avg_val_loss = val_loss / len(val_loader)\n    val_acc = accuracy_score(val_labels, val_preds)\n    scheduler.step()\n\n    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f} | Val Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}\")\n\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), \"best_multimodal.pt\")\n        print(\"‚úÖ Saved best model\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(\"üõë Early stopping\")\n            break\n\n# =============================================================\n# ‚úÖ 1Ô∏è‚É£1Ô∏è‚É£ FINAL TEST\n# =============================================================\nmodel.load_state_dict(torch.load(\"best_multimodal.pt\"))\nmodel.eval()\ntest_preds, test_labels = [], []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Test\"):\n        images = batch['image'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels_batch = batch['label'].to(device)\n\n        outputs = model(input_ids, attention_mask, images)\n        test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n        test_labels.extend(labels_batch.cpu().numpy())\n\nacc = accuracy_score(test_labels, test_preds)\nprec, rec, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average='weighted')\ncm = confusion_matrix(test_labels, test_preds)\n\nprint(f\"\\nüìä FINAL MULTIMODAL TEST RESULTS\\nAccuracy: {acc:.4f}\\nPrecision: {prec:.4f}\\nRecall: {rec:.4f}\\nF1: {f1:.4f}\\nConfusion Matrix:\\n{cm}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-08T06:31:37.912639Z","iopub.execute_input":"2025-07-08T06:31:37.912906Z","iopub.status.idle":"2025-07-08T06:40:37.032869Z","shell.execute_reply.started":"2025-07-08T06:31:37.912883Z","shell.execute_reply":"2025-07-08T06:40:37.031779Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/491 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cb2dc52f4d34dbbb5a47bbf4a2f9cd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ce47205e6d34a43bbe5eeff3daa08bf"}},"metadata":{}},{"name":"stderr","text":"2025-07-08 06:32:07.860001: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751956328.082547      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751956328.156186      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/660M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"970e1de9837742cf977dbbb3973c24d7"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet161_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet161_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /root/.cache/torch/hub/checkpoints/densenet161-8d451a50.pth\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 110M/110M [00:00<00:00, 229MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Train: 3156, Val: 451, Test: 902\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [01:27<00:00,  2.25it/s]\nVal Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:08<00:00,  3.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: 0.8863, Acc: 0.6017 | Val Loss: 0.8176, Acc: 0.6563\n‚úÖ Saved best model\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [01:26<00:00,  2.30it/s]\nVal Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:07<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: 0.6202, Acc: 0.7639 | Val Loss: 0.8034, Acc: 0.6984\n‚úÖ Saved best model\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [01:26<00:00,  2.30it/s]\nVal Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:06<00:00,  4.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: 0.3683, Acc: 0.8695 | Val Loss: 1.0281, Acc: 0.6497\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [01:25<00:00,  2.31it/s]\nVal Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:07<00:00,  4.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Train Loss: 0.1876, Acc: 0.9312 | Val Loss: 1.2610, Acc: 0.6275\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [01:26<00:00,  2.30it/s]\nVal Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:08<00:00,  3.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Train Loss: 0.0975, Acc: 0.9715 | Val Loss: 1.4752, Acc: 0.6718\nüõë Early stopping\n","output_type":"stream"},{"name":"stderr","text":"Test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:18<00:00,  3.07it/s]","output_type":"stream"},{"name":"stdout","text":"\nüìä FINAL MULTIMODAL TEST RESULTS\nAccuracy: 0.7461\nPrecision: 0.7547\nRecall: 0.7461\nF1: 0.7462\nConfusion Matrix:\n[[296  80  26]\n [ 35 292  26]\n [ 16  46  85]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1}]}
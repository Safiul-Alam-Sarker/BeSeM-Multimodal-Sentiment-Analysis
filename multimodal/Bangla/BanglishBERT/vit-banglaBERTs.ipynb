{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12355137,"sourceType":"datasetVersion","datasetId":7789382}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================\n# ‚úÖ 1Ô∏è‚É£ LIBRARIES & SETUP\n# ================================================\nimport os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import ViTImageProcessor, ViTModel, AutoTokenizer, AutoModel\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nimport torch.nn as nn\nimport re\nimport string\n\n# ================================================\n# ‚úÖ 2Ô∏è‚É£ PATHS\n# ================================================\nimage_dir = \"/kaggle/input/basem/images\"\ninput_csv = \"/kaggle/input/basem/dataset.csv\"\n\n# ================================================\n# ‚úÖ 3Ô∏è‚É£ LOAD & PREPROCESS CSV\n# ================================================\ndf = pd.read_csv(input_csv)\n\nexisting_data = []\nfor _, row in df.iterrows():\n    image_filename = row['image_path']\n    full_image_path = os.path.join(image_dir, image_filename)\n    if os.path.exists(full_image_path):\n        label_converted = row['label 2'] - 1\n        existing_data.append({\n            'Image_path': full_image_path,\n            'Captions': row['extracted_text'],\n            'Label_Sentiment': label_converted\n        })\n\nprocessed_df = pd.DataFrame(existing_data)\n\n# ================================================\n# ‚úÖ 4Ô∏è‚É£ TEXT CLEANING\n# ================================================\ndef clean_text(text):\n    if pd.isna(text): return \"\"\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub(r'<.*?>', '', text)\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = \" \".join(text.split())\n    return text\n\nprocessed_df['Captions'] = processed_df['Captions'].astype(str).apply(clean_text)\n\n# ================================================\n# ‚úÖ 5Ô∏è‚É£ SPLIT DATA\n# ================================================\ntrain_df, temp_df = train_test_split(processed_df, test_size=0.3, stratify=processed_df['Label_Sentiment'], random_state=42)\ntest_df, val_df = train_test_split(temp_df, test_size=1/3, stratify=temp_df['Label_Sentiment'], random_state=42)\n\n# ================================================\n# ‚úÖ 6Ô∏è‚É£ LOAD MODELS\n# ================================================\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nvit_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\nvit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\").to(device)\nbert_tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\nbert_model = AutoModel.from_pretrained(\"sagorsarker/bangla-bert-base\").to(device)\n\n# ================================================\n# ‚úÖ 7Ô∏è‚É£ DATASET CLASS\n# ================================================\nclass MultimodalDataset(Dataset):\n    def __init__(self, df, vit_processor, bert_tokenizer, max_length=128):\n        self.df = df\n        self.vit_processor = vit_processor\n        self.bert_tokenizer = bert_tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(row['Image_path']).convert('RGB')\n        caption = row['Captions']\n        label = row['Label_Sentiment']\n\n        image_inputs = self.vit_processor(image, return_tensors=\"pt\")\n        pixel_values = image_inputs['pixel_values'].squeeze(0)\n\n        text_inputs = self.bert_tokenizer(\n            caption,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n\n        input_ids = text_inputs['input_ids'].squeeze(0)\n        attention_mask = text_inputs['attention_mask'].squeeze(0)\n\n        return pixel_values, input_ids, attention_mask, label\n\ndef collate_fn(batch):\n    pixel_values, input_ids, attention_mask, labels = zip(*batch)\n    pixel_values = torch.stack(pixel_values)\n    input_ids = torch.stack(input_ids)\n    attention_mask = torch.stack(attention_mask)\n    labels = torch.tensor(labels)\n    return pixel_values, input_ids, attention_mask, labels\n\n# ================================================\n# ‚úÖ 8Ô∏è‚É£ DATALOADERS\n# ================================================\nbatch_size = 8\n\ntrain_loader = DataLoader(MultimodalDataset(train_df, vit_processor, bert_tokenizer), batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(MultimodalDataset(val_df, vit_processor, bert_tokenizer), batch_size=batch_size, collate_fn=collate_fn)\ntest_loader = DataLoader(MultimodalDataset(test_df, vit_processor, bert_tokenizer), batch_size=batch_size, collate_fn=collate_fn)\n\n# ================================================\n# ‚úÖ 9Ô∏è‚É£ MULTIMODAL MODEL\n# ================================================\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, vit_model, bert_model, hidden_size=768, num_classes=3):\n        super().__init__()\n        self.vit = vit_model\n        self.bert = bert_model\n\n        self.fusion = nn.Linear(hidden_size * 2, hidden_size)\n        self.classifier = nn.Linear(hidden_size, num_classes)\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, pixel_values, input_ids, attention_mask):\n        vision_outputs = self.vit(pixel_values=pixel_values)\n        vision_embed = vision_outputs.last_hidden_state[:, 0, :]\n\n        text_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        text_embed = text_outputs.last_hidden_state[:, 0, :]\n\n        fused = torch.cat((vision_embed, text_embed), dim=1)\n        fused = self.dropout(self.fusion(fused))\n        logits = self.classifier(fused)\n\n        return logits\n\n# ================================================\n# ‚úÖ üîü INIT MODEL\n# ================================================\nmodel = MultimodalClassifier(vit_model, bert_model).to(device)\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£1Ô∏è‚É£ LOSS & OPTIMIZER\n# ================================================\nclass_weights = train_df['Label_Sentiment'].value_counts().sort_index().tolist()\ntotal = sum(class_weights)\nweights = [total / c for c in class_weights]\ncriterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(weights).to(device))\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£2Ô∏è‚É£ TRAINING LOOP\n# ================================================\nnum_epochs = 10\npatience = 3\npatience_counter = 0\nbest_val_f1 = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_train_loss = 0\n    train_preds, train_labels = [], []\n\n    for pixel_values, input_ids, attention_mask, labels in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n        pixel_values = pixel_values.to(device)\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        labels = labels.to(device)\n\n        logits = model(pixel_values, input_ids, attention_mask)\n        loss = criterion(logits, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n\n        preds = torch.argmax(logits, dim=1)\n        train_preds.extend(preds.cpu().numpy())\n        train_labels.extend(labels.cpu().numpy())\n\n    avg_train_loss = total_train_loss / len(train_loader)\n\n    model.eval()\n    val_preds, val_labels = [], []\n    with torch.no_grad():\n        for pixel_values, input_ids, attention_mask, labels in tqdm(val_loader, desc=f\"Val Epoch {epoch+1}\"):\n            pixel_values = pixel_values.to(device)\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            labels = labels.to(device)\n\n            logits = model(pixel_values, input_ids, attention_mask)\n            preds = torch.argmax(logits, dim=1)\n            val_preds.extend(preds.cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n\n    val_f1 = precision_recall_fscore_support(val_labels, val_preds, average='weighted')[2]\n    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_train_loss:.4f} | Val F1: {val_f1:.4f}\")\n\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        patience_counter = 0\n        torch.save(model.state_dict(), \"best_multimodal_model.pt\")\n        print(\"‚úÖ Val F1 improved ‚Äî model saved.\")\n    else:\n        patience_counter += 1\n        print(f\"‚è∞ No improvement ‚Äî patience {patience_counter}/{patience}\")\n        if patience_counter >= patience:\n            print(f\"üõë Early stopping at epoch {epoch+1}\")\n            break\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£3Ô∏è‚É£ TEST\n# ================================================\nmodel.load_state_dict(torch.load(\"best_multimodal_model.pt\"))\nmodel.eval()\n\ntest_preds, test_labels = [], []\n\nwith torch.no_grad():\n    for pixel_values, input_ids, attention_mask, labels in tqdm(test_loader, desc=\"Test\"):\n        pixel_values = pixel_values.to(device)\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        labels = labels.to(device)\n\n        logits = model(pixel_values, input_ids, attention_mask)\n        preds = torch.argmax(logits, dim=1)\n        test_preds.extend(preds.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\ntest_f1 = precision_recall_fscore_support(test_labels, test_preds, average='weighted')[2]\ntest_acc = accuracy_score(test_labels, test_preds)\nprint(f\"Test Accuracy: {test_acc:.4f} | Test Weighted F1: {test_f1:.4f}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:31:33.299183Z","iopub.execute_input":"2025-07-08T05:31:33.299452Z","iopub.status.idle":"2025-07-08T06:08:40.070210Z","shell.execute_reply.started":"2025-07-08T05:31:33.299431Z","shell.execute_reply":"2025-07-08T06:08:40.069364Z"}},"outputs":[{"name":"stderr","text":"2025-07-08 05:31:46.122497: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751952706.357950      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751952706.421394      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0821c8b14aa440590f93d857c309034"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"501d6cc660fb48ac9889ac1c73f21c00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1464ea0e0fe245b0be2e935558a7e9af"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/491 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49f00c14455e4ac38c0d18c45777564c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17ee6818926b47d9ab30d8b8337f1df0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/660M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb38522231264130ad3c87cc31c75829"}},"metadata":{}},{"name":"stderr","text":"Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [04:08<00:00,  1.59it/s]\nVal Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:23<00:00,  2.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10] Train Loss: 0.8662 | Val F1: 0.6674\n‚úÖ Val F1 improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:30<00:00,  1.88it/s]\nVal Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10] Train Loss: 0.4885 | Val F1: 0.6648\n‚è∞ No improvement ‚Äî patience 1/3\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:31<00:00,  1.87it/s]\nVal Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10] Train Loss: 0.1950 | Val F1: 0.6600\n‚è∞ No improvement ‚Äî patience 2/3\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:30<00:00,  1.88it/s]\nVal Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10] Train Loss: 0.0697 | Val F1: 0.7068\n‚úÖ Val F1 improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:31<00:00,  1.86it/s]\nVal Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10] Train Loss: 0.0433 | Val F1: 0.6835\n‚è∞ No improvement ‚Äî patience 1/3\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:32<00:00,  1.86it/s]\nVal Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10] Train Loss: 0.0360 | Val F1: 0.7177\n‚úÖ Val F1 improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:33<00:00,  1.85it/s]\nVal Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10] Train Loss: 0.0279 | Val F1: 0.6970\n‚è∞ No improvement ‚Äî patience 1/3\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:28<00:00,  1.89it/s]\nVal Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10] Train Loss: 0.0149 | Val F1: 0.7074\n‚è∞ No improvement ‚Äî patience 2/3\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:32<00:00,  1.86it/s]\nVal Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/10] Train Loss: 0.0134 | Val F1: 0.6835\n‚è∞ No improvement ‚Äî patience 3/3\nüõë Early stopping at epoch 9\n","output_type":"stream"},{"name":"stderr","text":"Test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 113/113 [00:46<00:00,  2.44it/s]","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.7472 | Test Weighted F1: 0.7439\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# Compute all metrics\ntest_accuracy = accuracy_score(test_labels, test_preds)\nprecision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average='weighted')\n\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Precision: {precision:.4f}\")\nprint(f\"Test Recall: {recall:.4f}\")\nprint(f\"Test Weighted F1: {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T06:25:58.016044Z","iopub.execute_input":"2025-07-08T06:25:58.016617Z","iopub.status.idle":"2025-07-08T06:25:58.028693Z","shell.execute_reply.started":"2025-07-08T06:25:58.016580Z","shell.execute_reply":"2025-07-08T06:25:58.027989Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 0.7472\nTest Precision: 0.7442\nTest Recall: 0.7472\nTest Weighted F1: 0.7439\n","output_type":"stream"}],"execution_count":2}]}
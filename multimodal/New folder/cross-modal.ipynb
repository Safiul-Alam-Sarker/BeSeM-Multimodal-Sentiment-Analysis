{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12397007,"sourceType":"datasetVersion","datasetId":7817576}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================\n# ‚úÖ MULTIMODAL FUSION MODEL FOR BEST F1 SCORE\n# ================================================\nimport os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import AutoTokenizer, AutoModel, AutoImageProcessor, SwinForImageClassification\nfrom torch.optim import AdamW\nimport torchvision.transforms as T\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport re\nimport string\nimport json\n\n# ================================================\n# ‚úÖ PATHS & SETUP\n# ================================================\nimage_dir = \"/kaggle/input/basem/images\"\ninput_csv = \"/kaggle/input/basem/dataset.csv\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# ================================================\n# ‚úÖ LOAD & PREPROCESS CSV\n# ================================================\ndf = pd.read_csv(input_csv)\n\nexisting_data = []\nfor _, row in df.iterrows():\n    image_filename = row['image_path']\n    full_image_path = os.path.join(image_dir, image_filename)\n    if os.path.exists(full_image_path):\n        label_converted = row['label 2'] - 1\n        existing_data.append({\n            'Image_path': full_image_path,\n            'Captions': row['extracted_text'],\n            'Label_Sentiment': label_converted\n        })\n\nprocessed_df = pd.DataFrame(existing_data)\n\n# ================================================\n# ‚úÖ TEXT CLEANING\n# ================================================\ndef clean_text(text):\n    if pd.isna(text): return \"\"\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub(r'<.*?>', '', text)\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = \" \".join(text.split())\n    return text\n\n# ================================================\n# ‚úÖ DATA SPLITS\n# ================================================\ntrain_df, temp_df = train_test_split(processed_df, test_size=0.3, stratify=processed_df['Label_Sentiment'], random_state=42)\ntest_df, val_df = train_test_split(temp_df, test_size=1/3, stratify=temp_df['Label_Sentiment'], random_state=42)\n\nfor df_name, df_ in [('train', train_df), ('test', test_df), ('val', val_df)]:\n    df_['Captions'] = df_['Captions'].astype(str).apply(clean_text)\n    df_['label'] = df_['Label_Sentiment']\n\nprint(f\"Train samples: {len(train_df)}, Val samples: {len(val_df)}, Test samples: {len(test_df)}\")\nprint(f\"Class distribution: {train_df['label'].value_counts().sort_index().tolist()}\")\n\n# ================================================\n# ‚úÖ LOAD MODELS\n# ================================================\n# Load BanglishBERT for text\nbert_tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglabert\")\nbert_model = AutoModel.from_pretrained(\"csebuetnlp/banglabert\")\n\n# Load Swin Transformer for images\nswin_model_name = \"microsoft/swin-base-patch4-window7-224\"\nimage_processor = AutoImageProcessor.from_pretrained(swin_model_name)\nswin_backbone = SwinForImageClassification.from_pretrained(\n    swin_model_name,\n    num_labels=3,\n    ignore_mismatched_sizes=True\n)\n\n# ================================================\n# ‚úÖ MULTIMODAL FUSION MODEL\n# ================================================\nclass MultimodalFusionModel(nn.Module):\n    def __init__(self, bert_model, swin_model, num_classes=3, dropout_rate=0.3, fusion_dim=512):\n        super().__init__()\n\n        # Text encoder\n        self.bert = bert_model\n        self.text_dropout = nn.Dropout(dropout_rate)\n        self.text_projector = nn.Linear(bert_model.config.hidden_size, fusion_dim)\n\n        # Image encoder - Swin backbone\n        self.swin_backbone = swin_model.swin\n        self.image_dropout = nn.Dropout(dropout_rate)\n        self.image_projector = nn.Linear(swin_model.config.hidden_size, fusion_dim)\n\n        # ‚úÖ Cross-attention: text query attends to image key/value\n        self.cross_attn = nn.MultiheadAttention(\n            embed_dim=fusion_dim,\n            num_heads=4,\n            batch_first=True  # input shape [B, T, D]\n        )\n\n        # Fusion layers\n        self.fusion_dropout = nn.Dropout(dropout_rate)\n        self.fusion_layer1 = nn.Linear(fusion_dim * 2, fusion_dim)\n        self.fusion_layer2 = nn.Linear(fusion_dim, fusion_dim // 2)\n        self.batch_norm = nn.BatchNorm1d(fusion_dim // 2)\n\n        # Classification head\n        self.classifier = nn.Linear(fusion_dim // 2, num_classes)\n\n    def forward(self, input_ids, attention_mask, pixel_values):\n        # =======================\n        # ‚úÖ Text encoding\n        # =======================\n        text_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n        text_features = self.text_dropout(text_features)\n        text_projected = self.text_projector(text_features)  # [B, 512]\n\n        # =======================\n        # ‚úÖ Image encoding\n        # =======================\n        image_outputs = self.swin_backbone(pixel_values)\n        image_features = image_outputs.last_hidden_state.mean(dim=1)  # Global avg pool\n        image_features = self.image_dropout(image_features)\n        image_projected = self.image_projector(image_features)  # [B, 512]\n\n        # =======================\n        # ‚úÖ Cross-attention: text ‚Üí image\n        # =======================\n        text_query = text_projected.unsqueeze(1)  # [B, 1, 512]\n        image_key_value = image_projected.unsqueeze(1)  # [B, 1, 512]\n\n        attended_text, _ = self.cross_attn(\n            query=text_query,\n            key=image_key_value,\n            value=image_key_value\n        )  # [B, 1, 512]\n\n        attended_text = attended_text.squeeze(1)  # [B, 512]\n\n        # =======================\n        # ‚úÖ Fusion: concat attended text + image\n        # =======================\n        combined_features = torch.cat([attended_text, image_projected], dim=1)  # [B, 1024]\n\n        fusion_out = F.relu(self.fusion_layer1(combined_features))\n        fusion_out = self.fusion_dropout(fusion_out)\n        fusion_out = F.relu(self.fusion_layer2(fusion_out))\n        fusion_out = self.batch_norm(fusion_out)\n\n        logits = self.classifier(fusion_out)\n\n        return logits\n# ================================================\n# ‚úÖ MULTIMODAL DATASET\n# ================================================\nclass MultimodalDataset(Dataset):\n    def __init__(self, df, tokenizer, processor, max_length=128, is_train=False):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.processor = processor\n        self.max_length = max_length\n        self.is_train = is_train\n        # Define augmentations for training\n        self.train_transforms = T.Compose([\n            T.RandomRotation(15),\n            T.RandomHorizontalFlip(),\n            T.ColorJitter(brightness=0.3, contrast=0.3),\n            T.RandomAdjustSharpness(sharpness_factor=2),\n            # You can add more or adjust parameters as needed\n        ])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        # Text processing\n        caption = row['Captions']\n        text_inputs = self.tokenizer(\n            caption,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        # Image processing\n        image = Image.open(row['Image_path']).convert('RGB')\n        if self.is_train:\n            image = self.train_transforms(image)\n        image_inputs = self.processor(image, return_tensors=\"pt\")\n        return {\n            'input_ids': text_inputs['input_ids'].flatten(),\n            'attention_mask': text_inputs['attention_mask'].flatten(),\n            'pixel_values': image_inputs['pixel_values'].squeeze(0),\n            'label': torch.tensor(row['label'], dtype=torch.long)\n        }\n\n# ================================================\n# ‚úÖ DATALOADERS\n# ================================================\nbatch_size = 8\n\ntrain_dataset = MultimodalDataset(train_df, bert_tokenizer, image_processor, is_train=True)\nval_dataset = MultimodalDataset(val_df, bert_tokenizer, image_processor, is_train=False)\ntest_dataset = MultimodalDataset(test_df, bert_tokenizer, image_processor, is_train=False)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# ================================================\n# ‚úÖ INITIALIZE MODEL\n# ================================================\nmodel = MultimodalFusionModel(bert_model, swin_backbone, num_classes=3, dropout_rate=0.3).to(device)\n\n# ================================================\n# ‚úÖ LOSS & OPTIMIZER WITH ADVANCED TECHNIQUES\n# ================================================\n# Focal Loss for handling class imbalance\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = (1 - pt) ** self.gamma * ce_loss\n        \n        if self.alpha is not None:\n            alpha_t = self.alpha[targets]\n            focal_loss = alpha_t * focal_loss\n\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\n# Calculate class weights\nclass_counts = train_df['label'].value_counts().sort_index().tolist()\ntotal_samples = sum(class_counts)\nclass_weights = [total_samples / count for count in class_counts]\nalpha = torch.FloatTensor(class_weights).to(device)\n\n# Use Focal Loss for better handling of class imbalance\ncriterion = FocalLoss(alpha=alpha, gamma=2.0)\n\n# Optimizer with different learning rates for different parts\ntext_params = list(model.bert.parameters())\nimage_params = list(model.swin_backbone.parameters())\nfusion_params = list(model.text_projector.parameters()) + \\\n                list(model.image_projector.parameters()) + \\\n                list(model.cross_attn.parameters()) + \\\n                list(model.fusion_layer1.parameters()) + \\\n                list(model.fusion_layer2.parameters()) + \\\n                list(model.classifier.parameters())\n\noptimizer = AdamW([\n    {'params': text_params, 'lr': 2e-5},\n    {'params': image_params, 'lr': 1e-5},\n    {'params': fusion_params, 'lr': 5e-4}\n], weight_decay=0.01)\n\n# Learning rate scheduler\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n\n# ================================================\n# ‚úÖ TRAINING LOOP WITH ADVANCED TECHNIQUES\n# ================================================\nnum_epochs = 25\npatience = 5\npatience_counter = 0\nbest_val_f1 = 0.0\n\nprint(\"üöÄ Starting Multimodal Fusion Training...\")\n\nfor epoch in range(num_epochs):\n    # ============================================================\n    # TRAINING PHASE\n    # ============================================================\n    model.train()\n    total_train_loss = 0\n    train_predictions = []\n    train_labels = []\n\n    for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        pixel_values = batch['pixel_values'].to(device)\n        labels = batch['label'].to(device)\n        \n        optimizer.zero_grad()\n        \n        logits = model(input_ids, attention_mask, pixel_values)\n        loss = criterion(logits, labels)\n        \n        loss.backward()\n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        total_train_loss += loss.item()\n        \n        predictions = torch.argmax(logits, dim=1)\n        train_predictions.extend(predictions.cpu().numpy())\n        train_labels.extend(labels.cpu().numpy())\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_accuracy = accuracy_score(train_labels, train_predictions)\n    train_f1 = precision_recall_fscore_support(train_labels, train_predictions, average='weighted')[2]\n\n    # ============================================================\n    # VALIDATION PHASE\n    # ============================================================\n    model.eval()\n    total_val_loss = 0\n    val_predictions = []\n    val_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            pixel_values = batch['pixel_values'].to(device)\n            labels = batch['label'].to(device)\n            \n            logits = model(input_ids, attention_mask, pixel_values)\n            loss = criterion(logits, labels)\n            \n            total_val_loss += loss.item()\n            \n            predictions = torch.argmax(logits, dim=1)\n            val_predictions.extend(predictions.cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = accuracy_score(val_labels, val_predictions)\n    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(val_labels, val_predictions, average='weighted')\n    \n    # Step scheduler\n    scheduler.step()\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f} | Train F1: {train_f1:.4f}\")\n    print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f} | Val F1: {val_f1:.4f}\")\n    print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n\n    # ============================================================\n    # EARLY STOPPING BASED ON F1 SCORE\n    # ============================================================\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        patience_counter = 0\n        torch.save(model.state_dict(), \"best_multimodal_model.pt\")\n        print(f\"‚úÖ Validation F1 improved to {val_f1:.4f} ‚Äî model saved.\")\n    else:\n        patience_counter += 1\n        print(f\"‚è∞ No improvement ‚Äî patience {patience_counter}/{patience}\")\n\n        if patience_counter >= patience:\n            print(f\"üõë Early stopping triggered at epoch {epoch+1}\")\n            break\n    print(\"-\" * 70)\n\n# ================================================\n# ‚úÖ FINAL TEST EVALUATION\n# ================================================\nprint(\"\\nüîç Loading best model for final evaluation...\")\nmodel.load_state_dict(torch.load(\"best_multimodal_model.pt\"))\nmodel.eval()\n\ntest_predictions = []\ntest_labels = []\ntotal_test_loss = 0\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Final Test Evaluation\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        pixel_values = batch['pixel_values'].to(device)\n        labels = batch['label'].to(device)\n        \n        logits = model(input_ids, attention_mask, pixel_values)\n        loss = criterion(logits, labels)\n        \n        total_test_loss += loss.item()\n        predictions = torch.argmax(logits, dim=1)\n        test_predictions.extend(predictions.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\n# Calculate comprehensive metrics\ntest_accuracy = accuracy_score(test_labels, test_predictions)\ntest_precision, test_recall, test_f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='weighted')\ntest_precision_macro, test_recall_macro, test_f1_macro, _ = precision_recall_fscore_support(test_labels, test_predictions, average='macro')\ncm = confusion_matrix(test_labels, test_predictions)\n\n# Per-class metrics\nprecision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n    test_labels, test_predictions, average=None\n)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üéØ FINAL MULTIMODAL FUSION TEST RESULTS\")\nprint(\"=\"*70)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test F1-Score (Weighted): {test_f1:.4f}\")\nprint(f\"Test F1-Score (Macro): {test_f1_macro:.4f}\")\nprint(f\"Test Precision (Weighted): {test_precision:.4f}\")\nprint(f\"Test Recall (Weighted): {test_recall:.4f}\")\nprint(f\"Test Loss: {total_test_loss/len(test_loader):.4f}\")\n\nprint(\"\\nüìà Per-Class Metrics:\")\nclass_names = ['Negative', 'Neutral', 'Positive']\nfor i, class_name in enumerate(class_names):\n    print(f\"{class_name:>8}: Precision={precision_per_class[i]:.4f}, Recall={recall_per_class[i]:.4f}, F1={f1_per_class[i]:.4f}, Support={support[i]}\")\n\nprint(f\"\\nüéØ Confusion Matrix:\")\nprint(f\"{'':>10} {'Neg':>6} {'Neu':>6} {'Pos':>6}\")\nfor i, class_name in enumerate(['Negative', 'Neutral', 'Positive']):\n    print(f\"{class_name:>10} {cm[i][0]:>6} {cm[i][1]:>6} {cm[i][2]:>6}\")\n\nprint(\"\\nüìã Detailed Classification Report:\")\nprint(classification_report(test_labels, test_predictions, target_names=class_names))\n\n# ================================================\n# ‚úÖ SAVE RESULTS\n# ================================================\nresults = {\n    'test_accuracy': test_accuracy,\n    'test_f1_weighted': test_f1,\n    'test_f1_macro': test_f1_macro,\n    'test_precision_weighted': test_precision,\n    'test_recall_weighted': test_recall,\n    'test_loss': total_test_loss/len(test_loader),\n    'confusion_matrix': cm.tolist(),\n    'per_class_metrics': {\n        'precision': precision_per_class.tolist(),\n        'recall': recall_per_class.tolist(),\n        'f1': f1_per_class.tolist(),\n        'support': support.tolist()\n    }\n}\n\nwith open('/kaggle/working/multimodal_fusion_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ MULTIMODAL FUSION MODEL TRAINING COMPLETE!\")\nprint(f\"üèÜ Best F1 Score Achieved: {test_f1:.4f}\")\nprint(\"üìÅ Results saved to 'multimodal_fusion_results.json'\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T06:00:41.159456Z","iopub.execute_input":"2025-07-09T06:00:41.160266Z","iopub.status.idle":"2025-07-09T07:05:06.601128Z","shell.execute_reply.started":"2025-07-09T06:00:41.160218Z","shell.execute_reply":"2025-07-09T07:05:06.600271Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTrain samples: 3156, Val samples: 451, Test samples: 902\nClass distribution: [1404, 1237, 515]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34c0e4411f484091aa4b80bafd097a0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7810401a42f44bd9610e963359e8393"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9efce263682947f68293ebb2a0153c9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5e5908b19f143918a024adc4b23e642"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bf06fdc59d94ee08c7772425443b958"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f916f63c617484aa38dd41fd1d15fae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85ef1716c377460295a0bc5a8714f2c7"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3a97462f71a4d82a1898729c0053415"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/352M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfdc9e9fc0b1480c941060f08237d864"}},"metadata":{}},{"name":"stderr","text":"Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([3, 1024]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"üöÄ Starting Multimodal Fusion Training...\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [07:57<00:00,  1.21s/it]\nValidation Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:25<00:00,  2.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/25]\n  Train Loss: 1.4712 | Train Acc: 0.4908 | Train F1: 0.5027\n  Val Loss: 1.1364 | Val Acc: 0.6231 | Val F1: 0.6152\n  LR: 0.000020\n‚úÖ Validation F1 improved to 0.6152 ‚Äî model saved.\n----------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [07:32<00:00,  1.14s/it]\nValidation Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:23<00:00,  2.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/25]\n  Train Loss: 1.2599 | Train Acc: 0.5662 | Train F1: 0.5729\n  Val Loss: 1.1817 | Val Acc: 0.6452 | Val F1: 0.6368\n  LR: 0.000020\n‚úÖ Validation F1 improved to 0.6368 ‚Äî model saved.\n----------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [07:24<00:00,  1.13s/it]\nValidation Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/25]\n  Train Loss: 1.1633 | Train Acc: 0.6261 | Train F1: 0.6320\n  Val Loss: 1.1420 | Val Acc: 0.6475 | Val F1: 0.6482\n  LR: 0.000019\n‚úÖ Validation F1 improved to 0.6482 ‚Äî model saved.\n----------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [07:27<00:00,  1.13s/it]\nValidation Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/25]\n  Train Loss: 1.0730 | Train Acc: 0.6759 | Train F1: 0.6790\n  Val Loss: 1.1224 | Val Acc: 0.6231 | Val F1: 0.6256\n  LR: 0.000018\n‚è∞ No improvement ‚Äî patience 1/5\n----------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [07:30<00:00,  1.14s/it]\nValidation Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:22<00:00,  2.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/25]\n  Train Loss: 0.9607 | Train Acc: 0.6854 | Train F1: 0.6880\n  Val Loss: 1.3530 | Val Acc: 0.6098 | Val F1: 0.6114\n  LR: 0.000017\n‚è∞ No improvement ‚Äî patience 2/5\n----------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [07:29<00:00,  1.14s/it]\nValidation Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/25]\n  Train Loss: 0.8708 | Train Acc: 0.7294 | Train F1: 0.7303\n  Val Loss: 1.4713 | Val Acc: 0.6120 | Val F1: 0.6076\n  LR: 0.000016\n‚è∞ No improvement ‚Äî patience 3/5\n----------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [07:25<00:00,  1.13s/it]\nValidation Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:22<00:00,  2.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/25]\n  Train Loss: 0.7706 | Train Acc: 0.7643 | Train F1: 0.7650\n  Val Loss: 1.4821 | Val Acc: 0.6341 | Val F1: 0.6331\n  LR: 0.000015\n‚è∞ No improvement ‚Äî patience 4/5\n----------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [07:25<00:00,  1.13s/it]\nValidation Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:22<00:00,  2.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/25]\n  Train Loss: 0.7046 | Train Acc: 0.7709 | Train F1: 0.7717\n  Val Loss: 1.4986 | Val Acc: 0.5987 | Val F1: 0.6024\n  LR: 0.000013\n‚è∞ No improvement ‚Äî patience 5/5\nüõë Early stopping triggered at epoch 8\n\nüîç Loading best model for final evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Final Test Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 113/113 [00:50<00:00,  2.22it/s]","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nüéØ FINAL MULTIMODAL FUSION TEST RESULTS\n======================================================================\nTest Accuracy: 0.6552\nTest F1-Score (Weighted): 0.6566\nTest F1-Score (Macro): 0.6327\nTest Precision (Weighted): 0.6620\nTest Recall (Weighted): 0.6552\nTest Loss: 1.0526\n\nüìà Per-Class Metrics:\nNegative: Precision=0.7528, Recall=0.6741, F1=0.7113, Support=402\n Neutral: Precision=0.6025, Recall=0.6827, F1=0.6401, Support=353\nPositive: Precision=0.5563, Recall=0.5374, F1=0.5467, Support=147\n\nüéØ Confusion Matrix:\n              Neg    Neu    Pos\n  Negative    271    107     24\n   Neutral     73    241     39\n  Positive     16     52     79\n\nüìã Detailed Classification Report:\n              precision    recall  f1-score   support\n\n    Negative       0.75      0.67      0.71       402\n     Neutral       0.60      0.68      0.64       353\n    Positive       0.56      0.54      0.55       147\n\n    accuracy                           0.66       902\n   macro avg       0.64      0.63      0.63       902\nweighted avg       0.66      0.66      0.66       902\n\n\n======================================================================\n‚úÖ MULTIMODAL FUSION MODEL TRAINING COMPLETE!\nüèÜ Best F1 Score Achieved: 0.6566\nüìÅ Results saved to 'multimodal_fusion_results.json'\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2}]}
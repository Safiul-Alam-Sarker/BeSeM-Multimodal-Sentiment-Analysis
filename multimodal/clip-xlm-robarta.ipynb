{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12355137,"sourceType":"datasetVersion","datasetId":7789382}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================\n# ‚úÖ MULTIMODAL BENGALI SENTIMENT ANALYSIS\n# CLIP (Vision) + XLM-RoBERTa (Text) \n# ================================================\n\nimport os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import CLIPProcessor, CLIPModel, AutoTokenizer, AutoModel\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nimport re\nimport string\nimport numpy as np\nimport json\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£ PATHS & SETUP\n# ================================================\nimage_dir = \"/kaggle/input/basem/images\"\ninput_csv = \"/kaggle/input/basem/dataset.csv\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# ================================================\n# ‚úÖ 2Ô∏è‚É£ LOAD & PREPROCESS DATA\n# ================================================\ndf = pd.read_csv(input_csv)\n\nexisting_data = []\nfor _, row in df.iterrows():\n    image_filename = row['image_path']\n    full_image_path = os.path.join(image_dir, image_filename)\n    if os.path.exists(full_image_path):\n        label_converted = row['label 2'] - 1  # Convert to 0-indexed\n        existing_data.append({\n            'Image_path': full_image_path,\n            'Captions': row['extracted_text'],\n            'Label_Sentiment': label_converted\n        })\n\nprocessed_df = pd.DataFrame(existing_data)\nprint(f\"Total samples with existing images: {len(processed_df)}\")\n\n# ================================================\n# ‚úÖ 3Ô∏è‚É£ TEXT CLEANING FUNCTION\n# ================================================\ndef clean_text(text):\n    if pd.isna(text): \n        return \"\"\n    text = str(text)\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n    text = \" \".join(text.split())  # Clean whitespace\n    return text\n\n# ================================================\n# ‚úÖ 4Ô∏è‚É£ DATA SPLIT\n# ================================================\ntrain_df, temp_df = train_test_split(\n    processed_df, \n    test_size=0.3, \n    stratify=processed_df['Label_Sentiment'], \n    random_state=42\n)\ntest_df, val_df = train_test_split(\n    temp_df, \n    test_size=1/3, \n    stratify=temp_df['Label_Sentiment'], \n    random_state=42\n)\n\n# Clean text and add label column\nfor df_name, df_ in [('train', train_df), ('test', test_df), ('val', val_df)]:\n    df_['Captions'] = df_['Captions'].astype(str).apply(clean_text)\n    df_['label'] = df_['Label_Sentiment']\n    df_.to_csv(f'/kaggle/working/{df_name}_multimodal.csv', index=False)\n    print(f\"{df_name.capitalize()} set: {len(df_)} samples\")\n\n# ================================================\n# ‚úÖ 5Ô∏è‚É£ LOAD MODELS\n# ================================================\nprint(\"Loading CLIP model...\")\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n\nprint(\"Loading XLM-RoBERTa model...\")\nxlm_model_name = \"xlm-roberta-base\"\nxlm_tokenizer = AutoTokenizer.from_pretrained(xlm_model_name)\nxlm_model = AutoModel.from_pretrained(xlm_model_name).to(device)\n\n# Get feature dimensions\ndummy_image = Image.new('RGB', (224, 224))\ndummy_img_input = clip_processor(images=dummy_image, return_tensors=\"pt\").to(device)\nimg_dim = clip_model.get_image_features(**dummy_img_input).shape[1]\ntext_dim = xlm_model.config.hidden_size\n\nprint(f\"Image feature dimension: {img_dim}\")\nprint(f\"Text feature dimension: {text_dim}\")\n\n# ================================================\n# ‚úÖ 6Ô∏è‚É£ MULTIMODAL DATASET\n# ================================================\nclass MultimodalDataset(Dataset):\n    def __init__(self, df, clip_processor, xlm_tokenizer, max_length=128):\n        self.df = df\n        self.clip_processor = clip_processor\n        self.xlm_tokenizer = xlm_tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        # Load image\n        image = Image.open(row['Image_path']).convert('RGB')\n        \n        # Process text\n        text = str(row['Captions'])\n        text_encoding = self.xlm_tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        label = int(row['label'])\n        \n        return {\n            'image': image,\n            'input_ids': text_encoding['input_ids'].flatten(),\n            'attention_mask': text_encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\ndef collate_fn(batch):\n    images = [item['image'] for item in batch]\n    input_ids = torch.stack([item['input_ids'] for item in batch])\n    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n    labels = torch.stack([item['label'] for item in batch])\n    \n    return {\n        'images': images,\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'labels': labels\n    }\n\n# ================================================\n# ‚úÖ 7Ô∏è‚É£ MULTIMODAL CLASSIFIER\n# ================================================\nclass MultimodalSentimentClassifier(torch.nn.Module):\n    def __init__(self, img_dim, text_dim, num_classes=3, dropout_rate=0.3):\n        super().__init__()\n        \n        # Feature projections\n        self.img_projection = torch.nn.Linear(img_dim, 256)\n        self.text_projection = torch.nn.Linear(text_dim, 256)\n        \n        # Fusion layers\n        self.fusion = torch.nn.Sequential(\n            torch.nn.Linear(512, 256),  # img_proj + text_proj\n            torch.nn.ReLU(),\n            torch.nn.Dropout(dropout_rate),\n            torch.nn.Linear(256, 128),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(dropout_rate)\n        )\n        \n        # Final classifier\n        self.classifier = torch.nn.Linear(128, num_classes)\n        \n        # Dropout\n        self.dropout = torch.nn.Dropout(dropout_rate)\n        \n    def forward(self, img_features, text_features):\n        # Project features to same dimension\n        img_proj = self.img_projection(img_features)\n        text_proj = self.text_projection(text_features)\n        \n        # Concatenate features\n        fused_features = torch.cat([img_proj, text_proj], dim=1)\n        \n        # Pass through fusion layers\n        fused_features = self.fusion(fused_features)\n        \n        # Final classification\n        logits = self.classifier(fused_features)\n        \n        return logits\n\n# ================================================\n# ‚úÖ 8Ô∏è‚É£ INITIALIZE MODEL & TRAINING SETUP\n# ================================================\nmodel = MultimodalSentimentClassifier(img_dim, text_dim).to(device)\n\n# Calculate class weights\nclass_counts = train_df['label'].value_counts().sort_index()\ntotal_samples = len(train_df)\nclass_weights = [total_samples / count for count in class_counts]\nprint(f\"Class distribution: {class_counts.to_dict()}\")\nprint(f\"Class weights: {class_weights}\")\n\ncriterion = torch.nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\noptimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n\n# ================================================\n# ‚úÖ 9Ô∏è‚É£ DATALOADERS\n# ================================================\nbatch_size = 8\n\ntrain_dataset = MultimodalDataset(train_df, clip_processor, xlm_tokenizer)\nval_dataset = MultimodalDataset(val_df, clip_processor, xlm_tokenizer)\ntest_dataset = MultimodalDataset(test_df, clip_processor, xlm_tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n\n# ================================================\n# ‚úÖ üîü TRAINING LOOP\n# ================================================\nnum_epochs = 15\npatience = 3  # As requested\npatience_counter = 0\nbest_val_loss = float('inf')\n\nprint(f\"Starting multimodal training for {num_epochs} epochs...\")\nprint(f\"Patience: {patience}\")\n\nfor epoch in range(num_epochs):\n    # ============================================================\n    # TRAINING PHASE\n    # ============================================================\n    model.train()\n    clip_model.eval()  # Keep CLIP frozen\n    xlm_model.eval()   # Keep XLM-RoBERTa frozen\n    \n    total_train_loss = 0\n    train_predictions = []\n    train_labels = []\n\n    for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n        images = batch['images']\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n        \n        # Extract image features\n        with torch.no_grad():\n            img_inputs = clip_processor(images=images, return_tensors=\"pt\").to(device)\n            img_features = clip_model.get_image_features(**img_inputs)\n        \n        # Extract text features\n        with torch.no_grad():\n            text_outputs = xlm_model(input_ids=input_ids, attention_mask=attention_mask)\n            text_features = text_outputs.last_hidden_state[:, 0, :]  # CLS token\n        \n        # Forward pass through multimodal classifier\n        logits = model(img_features, text_features)\n        loss = criterion(logits, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_train_loss += loss.item()\n        \n        # Store predictions for metrics\n        predictions = torch.argmax(logits, dim=1)\n        train_predictions.extend(predictions.cpu().numpy())\n        train_labels.extend(labels.cpu().numpy())\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_accuracy = accuracy_score(train_labels, train_predictions)\n\n    # ============================================================\n    # VALIDATION PHASE\n    # ============================================================\n    model.eval()\n    total_val_loss = 0\n    val_predictions = []\n    val_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n            images = batch['images']\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            # Extract features\n            img_inputs = clip_processor(images=images, return_tensors=\"pt\").to(device)\n            img_features = clip_model.get_image_features(**img_inputs)\n            \n            text_outputs = xlm_model(input_ids=input_ids, attention_mask=attention_mask)\n            text_features = text_outputs.last_hidden_state[:, 0, :]\n            \n            # Forward pass\n            logits = model(img_features, text_features)\n            loss = criterion(logits, labels)\n\n            total_val_loss += loss.item()\n            \n            # Store predictions for metrics\n            predictions = torch.argmax(logits, dim=1)\n            val_predictions.extend(predictions.cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = accuracy_score(val_labels, val_predictions)\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f}\")\n    print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n\n    # ============================================================\n    # EARLY STOPPING CHECK\n    # ============================================================\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), \"/kaggle/working/best_multimodal_model.pt\")\n        print(\"‚úÖ Validation loss improved ‚Äî model saved.\")\n    else:\n        patience_counter += 1\n        print(f\"‚è∞ No improvement ‚Äî patience {patience_counter}/{patience}\")\n\n        if patience_counter >= patience:\n            print(f\"üõë Early stopping triggered at epoch {epoch+1}\")\n            break\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£1Ô∏è‚É£ FINAL TEST EVALUATION\n# ================================================\nprint(\"\\nüîç Loading best model for final evaluation...\")\nmodel.load_state_dict(torch.load(\"/kaggle/working/best_multimodal_model.pt\"))\nmodel.eval()\n\ntest_predictions = []\ntest_labels = []\ntotal_test_loss = 0\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Final Test Evaluation\"):\n        images = batch['images']\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        # Extract features\n        img_inputs = clip_processor(images=images, return_tensors=\"pt\").to(device)\n        img_features = clip_model.get_image_features(**img_inputs)\n        \n        text_outputs = xlm_model(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = text_outputs.last_hidden_state[:, 0, :]\n        \n        # Forward pass\n        logits = model(img_features, text_features)\n        loss = criterion(logits, labels)\n        \n        total_test_loss += loss.item()\n        predictions = torch.argmax(logits, dim=1)\n        test_predictions.extend(predictions.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£2Ô∏è‚É£ CALCULATE METRICS\n# ================================================\ntest_accuracy = accuracy_score(test_labels, test_predictions)\nprecision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='weighted')\ncm = confusion_matrix(test_labels, test_predictions)\n\n# Per-class metrics\nprecision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n    test_labels, test_predictions, average=None\n)\n\nprint(\"\\nüìä FINAL MULTIMODAL TEST RESULTS:\")\nprint(\"=\" * 60)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Precision (Weighted): {precision:.4f}\")\nprint(f\"Test Recall (Weighted): {recall:.4f}\")\nprint(f\"Test F1-Score (Weighted): {f1:.4f}\")\nprint(f\"Test Loss: {total_test_loss/len(test_loader):.4f}\")\n\nprint(f\"\\nConfusion Matrix:\")\nprint(cm)\n\nprint(f\"\\nPer-Class Metrics:\")\nprint(\"-\" * 30)\nfor i in range(len(precision_per_class)):\n    print(f\"Class {i} (Sentiment):\")\n    print(f\"  Precision: {precision_per_class[i]:.4f}\")\n    print(f\"  Recall: {recall_per_class[i]:.4f}\")\n    print(f\"  F1-Score: {f1_per_class[i]:.4f}\")\n    print(f\"  Support: {support_per_class[i]}\")\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£3Ô∏è‚É£ SAVE RESULTS\n# ================================================\nresults = {\n    'model': 'Multimodal (CLIP + XLM-RoBERTa)',\n    'test_accuracy': float(test_accuracy),\n    'test_precision_weighted': float(precision),\n    'test_recall_weighted': float(recall),\n    'test_f1_weighted': float(f1),\n    'test_loss': float(total_test_loss/len(test_loader)),\n    'confusion_matrix': cm.tolist(),\n    'per_class_precision': precision_per_class.tolist(),\n    'per_class_recall': recall_per_class.tolist(),\n    'per_class_f1': f1_per_class.tolist(),\n    'per_class_support': support_per_class.tolist(),\n    'training_params': {\n        'epochs': epoch + 1,\n        'patience': patience,\n        'batch_size': batch_size,\n        'learning_rate': 1e-4,\n        'early_stopped': patience_counter >= patience\n    }\n}\n\nwith open('/kaggle/working/multimodal_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\"\\n‚úÖ Results saved to 'multimodal_results.json'\")\nprint(\"üéØ Multimodal Bengali sentiment analysis completed!\")\nprint(f\"üî• Final Test Accuracy: {test_accuracy:.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T12:08:11.529210Z","iopub.execute_input":"2025-07-07T12:08:11.529826Z","iopub.status.idle":"2025-07-07T12:32:52.712874Z","shell.execute_reply.started":"2025-07-07T12:08:11.529799Z","shell.execute_reply":"2025-07-07T12:32:52.712060Z"}},"outputs":[{"name":"stderr","text":"2025-07-07 12:08:24.474206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751890104.657534      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751890104.707116      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nTotal samples with existing images: 4509\nTrain set: 3156 samples\nTest set: 902 samples\nVal set: 451 samples\nLoading CLIP model...\n","output_type":"stream"},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"376281d041b542198da95e997799fd9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a152ec168cd74a9dbd8c756533904581"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3771951f639b4f2ba5ba9585f241787c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbe1dcdd0bd54518b4d919efe7cbe0df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"880a14865f984b7bb131600fb3e74e36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccbdb0ceea594ef2bd5d5cd5ad65e775"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6df9d0635c584e46b798291c8440a8b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f5fba203841488fbef443ebde381b8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43271d1af1454f619c46e8eeb0bcdab5"}},"metadata":{}},{"name":"stdout","text":"Loading XLM-RoBERTa model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71b68aa5291e4b039e2537949108e551"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"747d14e9adc5441697077bc65577aa03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e629e484c2134726bf18250de5b5ba58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9299190a49a4ea6bde81f866ef8e928"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7fa773c7c33497eb489837d28a62cc2"}},"metadata":{}},{"name":"stdout","text":"Image feature dimension: 512\nText feature dimension: 768\nClass distribution: {0: 1404, 1: 1237, 2: 515}\nClass weights: [2.247863247863248, 2.551333872271625, 6.128155339805825]\nStarting multimodal training for 15 epochs...\nPatience: 3\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [02:30<00:00,  2.62it/s]\nValidation Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:22<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/15]\n  Train Loss: 0.9948 | Train Acc: 0.5361\n  Val Loss: 0.8751 | Val Acc: 0.6475\n‚úÖ Validation loss improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [02:09<00:00,  3.04it/s]\nValidation Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/15]\n  Train Loss: 0.8555 | Train Acc: 0.6258\n  Val Loss: 0.8442 | Val Acc: 0.6541\n‚úÖ Validation loss improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [02:10<00:00,  3.04it/s]\nValidation Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/15]\n  Train Loss: 0.8135 | Train Acc: 0.6420\n  Val Loss: 0.8423 | Val Acc: 0.6696\n‚úÖ Validation loss improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [02:11<00:00,  3.01it/s]\nValidation Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/15]\n  Train Loss: 0.7879 | Train Acc: 0.6502\n  Val Loss: 0.8360 | Val Acc: 0.6585\n‚úÖ Validation loss improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [02:10<00:00,  3.02it/s]\nValidation Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:20<00:00,  2.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/15]\n  Train Loss: 0.7610 | Train Acc: 0.6800\n  Val Loss: 0.8407 | Val Acc: 0.6630\n‚è∞ No improvement ‚Äî patience 1/3\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [02:10<00:00,  3.02it/s]\nValidation Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:20<00:00,  2.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/15]\n  Train Loss: 0.7431 | Train Acc: 0.6841\n  Val Loss: 0.8276 | Val Acc: 0.6674\n‚úÖ Validation loss improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [02:11<00:00,  3.00it/s]\nValidation Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/15]\n  Train Loss: 0.7260 | Train Acc: 0.6993\n  Val Loss: 0.8483 | Val Acc: 0.6696\n‚è∞ No improvement ‚Äî patience 1/3\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [02:10<00:00,  3.02it/s]\nValidation Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/15]\n  Train Loss: 0.7032 | Train Acc: 0.7025\n  Val Loss: 0.8548 | Val Acc: 0.6608\n‚è∞ No improvement ‚Äî patience 2/3\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [02:10<00:00,  3.02it/s]\nValidation Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/15]\n  Train Loss: 0.6848 | Train Acc: 0.7018\n  Val Loss: 0.8999 | Val Acc: 0.6386\n‚è∞ No improvement ‚Äî patience 3/3\nüõë Early stopping triggered at epoch 9\n\nüîç Loading best model for final evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Final Test Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 113/113 [00:43<00:00,  2.59it/s]","output_type":"stream"},{"name":"stdout","text":"\nüìä FINAL MULTIMODAL TEST RESULTS:\n============================================================\nTest Accuracy: 0.6940\nTest Precision (Weighted): 0.7220\nTest Recall (Weighted): 0.6940\nTest F1-Score (Weighted): 0.6925\nTest Loss: 0.7591\n\nConfusion Matrix:\n[[258 125  19]\n [ 27 298  28]\n [ 14  63  70]]\n\nPer-Class Metrics:\n------------------------------\nClass 0 (Sentiment):\n  Precision: 0.8629\n  Recall: 0.6418\n  F1-Score: 0.7361\n  Support: 402\nClass 1 (Sentiment):\n  Precision: 0.6132\n  Recall: 0.8442\n  F1-Score: 0.7104\n  Support: 353\nClass 2 (Sentiment):\n  Precision: 0.5983\n  Recall: 0.4762\n  F1-Score: 0.5303\n  Support: 147\n\n‚úÖ Results saved to 'multimodal_results.json'\nüéØ Multimodal Bengali sentiment analysis completed!\nüî• Final Test Accuracy: 0.6940\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1}]}
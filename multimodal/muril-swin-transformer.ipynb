{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12355137,"sourceType":"datasetVersion","datasetId":7789382}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================\n# ‚úÖ MULTIMODAL SENTIMENT ANALYSIS\n# MuRIL (Text) + Swin Transformer (Vision) Fusion\n# ================================================\n\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import AutoTokenizer, AutoModel, AutoImageProcessor, SwinForImageClassification\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\nimport re\nimport string\nimport json\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£ SETUP & PATHS\n# ================================================\nimage_dir = \"/kaggle/input/basem/images\"\ninput_csv = \"/kaggle/input/basem/dataset.csv\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# ================================================\n# ‚úÖ 2Ô∏è‚É£ LOAD & PREPROCESS DATA\n# ================================================\ndf = pd.read_csv(input_csv)\n\ndef clean_text(text):\n    if pd.isna(text): return \"\"\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub(r'<.*?>', '', text)\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = \" \".join(text.split())\n    return text\n\n# Create multimodal dataset\nmultimodal_data = []\nfor _, row in df.iterrows():\n    image_filename = row['image_path']\n    full_image_path = os.path.join(image_dir, image_filename)\n    \n    # Check if both image and text exist\n    if (os.path.exists(full_image_path) and \n        pd.notna(row['extracted_text']) and \n        row['extracted_text'].strip()):\n        \n        label_converted = row['label 2'] - 1  # Convert to 0-indexed\n        multimodal_data.append({\n            'Image_path': full_image_path,\n            'text': clean_text(row['extracted_text']),\n            'label': label_converted\n        })\n\nprocessed_df = pd.DataFrame(multimodal_data)\nprint(f\"Total multimodal samples: {len(processed_df)}\")\n\n# ================================================\n# ‚úÖ 3Ô∏è‚É£ DATA SPLITS\n# ================================================\ntrain_df, temp_df = train_test_split(processed_df, test_size=0.3, stratify=processed_df['label'], random_state=42)\ntest_df, val_df = train_test_split(temp_df, test_size=1/3, stratify=temp_df['label'], random_state=42)\n\nprint(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n\n# ================================================\n# ‚úÖ 4Ô∏è‚É£ LOAD MODELS & PROCESSORS\n# ================================================\n# Text Model (MuRIL)\ntext_tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\nmuril_model = AutoModel.from_pretrained(\"google/muril-base-cased\")\n\n# Vision Model (Swin Transformer)\nvision_processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-base-patch4-window7-224\")\nswin_model = SwinForImageClassification.from_pretrained(\n    \"microsoft/swin-base-patch4-window7-224\",\n    num_labels=3,\n    ignore_mismatched_sizes=True\n)\n\n# ================================================\n# ‚úÖ 5Ô∏è‚É£ MULTIMODAL DATASET\n# ================================================\nclass MultiModalDataset(Dataset):\n    def __init__(self, df, text_tokenizer, vision_processor, max_length=128):\n        self.df = df\n        self.text_tokenizer = text_tokenizer\n        self.vision_processor = vision_processor\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        # Process text\n        text = row['text']\n        text_encoded = self.text_tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        # Process image\n        image = Image.open(row['Image_path']).convert('RGB')\n        image_inputs = self.vision_processor(image, return_tensors=\"pt\")\n        \n        # Label\n        label = row['label']\n        \n        return {\n            'input_ids': text_encoded['input_ids'].flatten(),\n            'attention_mask': text_encoded['attention_mask'].flatten(),\n            'pixel_values': image_inputs['pixel_values'].squeeze(0),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# ================================================\n# ‚úÖ 6Ô∏è‚É£ MULTIMODAL FUSION MODEL\n# ================================================\nclass MultiModalSentimentModel(nn.Module):\n    def __init__(self, muril_model, swin_model, num_classes=3, dropout=0.3, fusion_dim=512):\n        super().__init__()\n        \n        # Text branch (MuRIL)\n        self.text_encoder = muril_model\n        self.text_projection = nn.Linear(muril_model.config.hidden_size, fusion_dim)\n        \n        # Vision branch (Swin)\n        self.vision_encoder = swin_model.swin\n        self.vision_projection = nn.Linear(swin_model.config.hidden_size, fusion_dim)\n        \n        # Fusion layers\n        self.fusion_dropout = nn.Dropout(dropout)\n        self.fusion_layer = nn.Linear(fusion_dim * 2, fusion_dim)\n        self.fusion_activation = nn.ReLU()\n        \n        # Classification head\n        self.classifier = nn.Sequential(\n            nn.Linear(fusion_dim, fusion_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(fusion_dim // 2, num_classes)\n        )\n        \n        # Attention mechanism for fusion\n        self.attention = nn.MultiheadAttention(fusion_dim, num_heads=8, batch_first=True)\n        \n    def forward(self, input_ids, attention_mask, pixel_values):\n        # Text encoding\n        text_outputs = self.text_encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        text_features = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n        text_projected = self.text_projection(text_features)\n        \n        # Vision encoding\n        vision_outputs = self.vision_encoder(pixel_values)\n        vision_features = vision_outputs.last_hidden_state.mean(dim=1)  # Global average pooling\n        vision_projected = self.vision_projection(vision_features)\n        \n        # Cross-modal attention\n        # Stack text and vision features\n        multimodal_features = torch.stack([text_projected, vision_projected], dim=1)\n        attended_features, _ = self.attention(multimodal_features, multimodal_features, multimodal_features)\n        \n        # Fusion\n        fused_features = torch.cat([attended_features[:, 0, :], attended_features[:, 1, :]], dim=1)\n        fused_features = self.fusion_dropout(fused_features)\n        fused_features = self.fusion_activation(self.fusion_layer(fused_features))\n        \n        # Classification\n        logits = self.classifier(fused_features)\n        \n        return logits\n\n# ================================================\n# ‚úÖ 7Ô∏è‚É£ DATALOADERS\n# ================================================\nbatch_size = 8\n\ntrain_dataset = MultiModalDataset(train_df, text_tokenizer, vision_processor)\nval_dataset = MultiModalDataset(val_df, text_tokenizer, vision_processor)\ntest_dataset = MultiModalDataset(test_df, text_tokenizer, vision_processor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# ================================================\n# ‚úÖ 8Ô∏è‚É£ MODEL INITIALIZATION\n# ================================================\nmodel = MultiModalSentimentModel(muril_model, swin_model, num_classes=3).to(device)\n\n# ================================================\n# ‚úÖ 9Ô∏è‚É£ LOSS & OPTIMIZER\n# ================================================\n# Calculate class weights\nclass_weights = train_df['label'].value_counts().sort_index().tolist()\ntotal = sum(class_weights)\nweights = [total / c for c in class_weights]\nprint(f\"Class distribution: {class_weights}\")\nprint(f\"Class weights: {weights}\")\n\ncriterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(weights).to(device))\n\n# Different learning rates for different components\noptimizer = AdamW([\n    {'params': model.text_encoder.parameters(), 'lr': 1e-5},\n    {'params': model.vision_encoder.parameters(), 'lr': 1e-5},\n    {'params': model.text_projection.parameters(), 'lr': 2e-4},\n    {'params': model.vision_projection.parameters(), 'lr': 2e-4},\n    {'params': model.fusion_layer.parameters(), 'lr': 2e-4},\n    {'params': model.attention.parameters(), 'lr': 2e-4},\n    {'params': model.classifier.parameters(), 'lr': 2e-4},\n], weight_decay=0.01)\n\n# ================================================\n# ‚úÖ üîü TRAINING LOOP\n# ================================================\nnum_epochs = 15\npatience = 5\npatience_counter = 0\nbest_val_f1 = 0.0\n\nprint(\"üöÄ Starting multimodal training...\")\n\nfor epoch in range(num_epochs):\n    # ============================================================\n    # TRAINING PHASE\n    # ============================================================\n    model.train()\n    total_train_loss = 0\n    train_predictions = []\n    train_labels = []\n\n    for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        pixel_values = batch['pixel_values'].to(device)\n        labels = batch['label'].to(device)\n\n        optimizer.zero_grad()\n        \n        logits = model(input_ids, attention_mask, pixel_values)\n        loss = criterion(logits, labels)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n        optimizer.step()\n        \n        total_train_loss += loss.item()\n        \n        predictions = torch.argmax(logits, dim=1)\n        train_predictions.extend(predictions.cpu().numpy())\n        train_labels.extend(labels.cpu().numpy())\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_accuracy = accuracy_score(train_labels, train_predictions)\n    train_f1 = precision_recall_fscore_support(train_labels, train_predictions, average='weighted')[2]\n\n    # ============================================================\n    # VALIDATION PHASE\n    # ============================================================\n    model.eval()\n    total_val_loss = 0\n    val_predictions = []\n    val_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            pixel_values = batch['pixel_values'].to(device)\n            labels = batch['label'].to(device)\n\n            logits = model(input_ids, attention_mask, pixel_values)\n            loss = criterion(logits, labels)\n\n            total_val_loss += loss.item()\n            \n            predictions = torch.argmax(logits, dim=1)\n            val_predictions.extend(predictions.cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = accuracy_score(val_labels, val_predictions)\n    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(val_labels, val_predictions, average='weighted')\n    \n    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n    print(f\"Train - Loss: {avg_train_loss:.4f}, Acc: {train_accuracy:.4f}, F1: {train_f1:.4f}\")\n    print(f\"Val   - Loss: {avg_val_loss:.4f}, Acc: {val_accuracy:.4f}, F1: {val_f1:.4f}\")\n\n    # ============================================================\n    # EARLY STOPPING (based on F1 score)\n    # ============================================================\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        patience_counter = 0\n        torch.save(model.state_dict(), \"best_multimodal_model.pt\")\n        print(\"‚úÖ Validation F1 improved ‚Äî model saved.\")\n    else:\n        patience_counter += 1\n        print(f\"‚è∞ No improvement ‚Äî patience {patience_counter}/{patience}\")\n\n        if patience_counter >= patience:\n            print(f\"üõë Early stopping triggered at epoch {epoch+1}\")\n            break\n    \n    print(\"-\" * 60)\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£1Ô∏è‚É£ FINAL TEST EVALUATION\n# ================================================\nprint(\"\\nüîç Loading best model for final evaluation...\")\nmodel.load_state_dict(torch.load(\"best_multimodal_model.pt\"))\nmodel.eval()\n\ntest_predictions = []\ntest_labels = []\ntotal_test_loss = 0\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Final Test Evaluation\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        pixel_values = batch['pixel_values'].to(device)\n        labels = batch['label'].to(device)\n\n        logits = model(input_ids, attention_mask, pixel_values)\n        loss = criterion(logits, labels)\n        \n        total_test_loss += loss.item()\n        predictions = torch.argmax(logits, dim=1)\n        test_predictions.extend(predictions.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£2Ô∏è‚É£ COMPREHENSIVE RESULTS\n# ================================================\ntest_accuracy = accuracy_score(test_labels, test_predictions)\nprecision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='weighted')\ncm = confusion_matrix(test_labels, test_predictions)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"üéØ FINAL MULTIMODAL SENTIMENT ANALYSIS RESULTS\")\nprint(\"   MuRIL (Text) + Swin Transformer (Vision) Fusion\")\nprint(\"=\"*80)\nprint(f\"Test Accuracy:  {test_accuracy:.4f}\")\nprint(f\"Test Precision: {precision:.4f}\")\nprint(f\"Test Recall:    {recall:.4f}\")\nprint(f\"Test F1-Score:  {f1:.4f}\")\nprint(f\"Test Loss:      {total_test_loss/len(test_loader):.4f}\")\nprint(f\"\\nConfusion Matrix:\\n{cm}\")\n\n# Per-class metrics\nprecision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n    test_labels, test_predictions, average=None\n)\n\nprint(\"\\nüìä PER-CLASS DETAILED METRICS:\")\nprint(\"-\" * 50)\nclass_names = ['Negative', 'Neutral', 'Positive']\nfor i, class_name in enumerate(class_names):\n    print(f\"{class_name:>8}: Precision={precision_per_class[i]:.4f}, Recall={recall_per_class[i]:.4f}, F1={f1_per_class[i]:.4f}, Support={support[i]}\")\n\nprint(f\"\\nüìà Classification Report:\")\nprint(classification_report(test_labels, test_predictions, target_names=class_names))\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£3Ô∏è‚É£ SAVE RESULTS\n# ================================================\nresults = {\n    'model_type': 'multimodal_muril_swin',\n    'test_accuracy': float(test_accuracy),\n    'test_precision': float(precision),\n    'test_recall': float(recall),\n    'test_f1': float(f1),\n    'test_loss': float(total_test_loss/len(test_loader)),\n    'confusion_matrix': cm.tolist(),\n    'per_class_metrics': {\n        'precision': precision_per_class.tolist(),\n        'recall': recall_per_class.tolist(),\n        'f1': f1_per_class.tolist(),\n        'support': support.tolist()\n    },\n    'class_names': class_names,\n    'dataset_info': {\n        'train_samples': len(train_df),\n        'val_samples': len(val_df),\n        'test_samples': len(test_df),\n        'total_samples': len(processed_df)\n    }\n}\n\nwith open('/kaggle/working/multimodal_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(f\"\\n‚úÖ Results saved to 'multimodal_results.json'\")\nprint(f\"üéâ Multimodal sentiment analysis complete!\")\nprint(f\"üèÜ Best F1-Score achieved: {f1:.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T10:16:37.363448Z","iopub.execute_input":"2025-07-07T10:16:37.363715Z","iopub.status.idle":"2025-07-07T11:17:51.612072Z","shell.execute_reply.started":"2025-07-07T10:16:37.363691Z","shell.execute_reply":"2025-07-07T11:17:51.611418Z"}},"outputs":[{"name":"stderr","text":"2025-07-07 10:16:52.250143: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751883412.463166      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751883412.524248      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nTotal multimodal samples: 4509\nTrain: 3156, Val: 451, Test: 902\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e15c6ef625a74f3f823b95cfe1a19b70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a78ddef9bc6d43ea93c0b77a47c02d80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3954c4aaeb14600a269d0cc19a3d02b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/113 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80364eeb95d34ff28e5eda3fc9ea0679"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/953M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9c7d9cc60ad471f919f94712b23f2eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/953M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ce696e4f7fc470ba4b5f05f218f1788"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79c0ea1c968242c4a37bf07f4dddcb32"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac889762262949b5a167ebf2bb68f0b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/352M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fb9987459c7401cad7711368d5e217c"}},"metadata":{}},{"name":"stderr","text":"Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([3, 1024]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Class distribution: [1404, 1237, 515]\nClass weights: [2.247863247863248, 2.551333872271625, 6.128155339805825]\nüöÄ Starting multimodal training...\n","output_type":"stream"},{"name":"stderr","text":"\nTrain Epoch 1:   0%|          | 0/395 [00:00<?, ?it/s]\u001b[A\nTrain Epoch 1:   0%|          | 1/395 [00:04<31:29,  4.79s/it]\u001b[A\nTrain Epoch 1:   1%|          | 2/395 [00:06<18:12,  2.78s/it]\u001b[A\nTrain Epoch 1:   1%|          | 3/395 [00:07<14:14,  2.18s/it]\u001b[A\nTrain Epoch 1:   1%|          | 4/395 [00:09<13:12,  2.03s/it]\u001b[A\nTrain Epoch 1:   1%|‚ñè         | 5/395 [00:10<10:48,  1.66s/it]\u001b[A\nTrain Epoch 1:   2%|‚ñè         | 6/395 [00:11<09:45,  1.51s/it]\u001b[A\nTrain Epoch 1:   2%|‚ñè         | 7/395 [00:12<08:33,  1.32s/it]\u001b[A\nTrain Epoch 1:   2%|‚ñè         | 8/395 [00:13<08:05,  1.26s/it]\u001b[A\nTrain Epoch 1:   2%|‚ñè         | 9/395 [00:15<09:52,  1.54s/it]\u001b[A\nTrain Epoch 1:   3%|‚ñé         | 10/395 [00:17<09:40,  1.51s/it]\u001b[A\nTrain Epoch 1:   3%|‚ñé         | 11/395 [00:18<09:50,  1.54s/it]\u001b[A\nTrain Epoch 1:   3%|‚ñé         | 12/395 [00:20<09:10,  1.44s/it]\u001b[A\nTrain Epoch 1:   3%|‚ñé         | 13/395 [00:21<08:35,  1.35s/it]\u001b[A\nTrain Epoch 1:   4%|‚ñé         | 14/395 [00:22<08:02,  1.27s/it]\u001b[A\nTrain Epoch 1:   4%|‚ñç         | 15/395 [00:23<07:55,  1.25s/it]\u001b[A\nTrain Epoch 1:   4%|‚ñç         | 16/395 [00:24<07:49,  1.24s/it]\u001b[A\nTrain Epoch 1:   4%|‚ñç         | 17/395 [00:25<07:13,  1.15s/it]\u001b[A\nTrain Epoch 1:   5%|‚ñç         | 18/395 [00:26<07:25,  1.18s/it]\u001b[A\nTrain Epoch 1:   5%|‚ñç         | 19/395 [00:28<07:09,  1.14s/it]\u001b[A\nTrain Epoch 1:   5%|‚ñå         | 20/395 [00:29<07:35,  1.21s/it]\u001b[A\nTrain Epoch 1:   5%|‚ñå         | 21/395 [00:30<07:17,  1.17s/it]\u001b[A\nTrain Epoch 1:   6%|‚ñå         | 22/395 [00:31<07:41,  1.24s/it]\u001b[A\nTrain Epoch 1:   6%|‚ñå         | 23/395 [00:33<07:40,  1.24s/it]\u001b[A\nTrain Epoch 1:   6%|‚ñå         | 24/395 [00:35<09:35,  1.55s/it]\u001b[A\nTrain Epoch 1:   6%|‚ñã         | 25/395 [00:36<09:00,  1.46s/it]\u001b[A\nTrain Epoch 1:   7%|‚ñã         | 26/395 [00:38<09:12,  1.50s/it]\u001b[A\nTrain Epoch 1:   7%|‚ñã         | 27/395 [00:40<11:00,  1.80s/it]\u001b[A\nTrain Epoch 1:   7%|‚ñã         | 28/395 [00:41<09:51,  1.61s/it]\u001b[A\nTrain Epoch 1:   7%|‚ñã         | 29/395 [00:42<08:46,  1.44s/it]\u001b[A\nTrain Epoch 1:   8%|‚ñä         | 30/395 [00:43<08:06,  1.33s/it]\u001b[A\nTrain Epoch 1:   8%|‚ñä         | 31/395 [00:45<07:41,  1.27s/it]\u001b[A\nTrain Epoch 1:   8%|‚ñä         | 32/395 [00:46<07:11,  1.19s/it]\u001b[A\nTrain Epoch 1:   8%|‚ñä         | 33/395 [00:47<06:58,  1.16s/it]\u001b[A\nTrain Epoch 1:   9%|‚ñä         | 34/395 [00:48<07:44,  1.29s/it]\u001b[A\nTrain Epoch 1:   9%|‚ñâ         | 35/395 [00:50<08:28,  1.41s/it]\u001b[A\nTrain Epoch 1:   9%|‚ñâ         | 36/395 [00:51<08:01,  1.34s/it]\u001b[A\nTrain Epoch 1:   9%|‚ñâ         | 37/395 [00:53<08:02,  1.35s/it]\u001b[A\nTrain Epoch 1:  10%|‚ñâ         | 38/395 [00:54<07:27,  1.25s/it]\u001b[A\nTrain Epoch 1:  10%|‚ñâ         | 39/395 [00:55<07:06,  1.20s/it]\u001b[A\nTrain Epoch 1:  10%|‚ñà         | 40/395 [00:56<06:29,  1.10s/it]\u001b[A\nTrain Epoch 1:  10%|‚ñà         | 41/395 [00:57<06:47,  1.15s/it]\u001b[A\nTrain Epoch 1:  11%|‚ñà         | 42/395 [00:58<06:53,  1.17s/it]\u001b[A\nTrain Epoch 1:  11%|‚ñà         | 43/395 [00:59<06:22,  1.09s/it]\u001b[A\nTrain Epoch 1:  11%|‚ñà         | 44/395 [01:00<06:23,  1.09s/it]\u001b[A\nTrain Epoch 1:  11%|‚ñà‚ñè        | 45/395 [01:01<06:16,  1.08s/it]\u001b[A\nTrain Epoch 1:  12%|‚ñà‚ñè        | 46/395 [01:02<06:07,  1.05s/it]\u001b[A\nTrain Epoch 1:  12%|‚ñà‚ñè        | 47/395 [01:03<06:00,  1.04s/it]\u001b[A\nTrain Epoch 1:  12%|‚ñà‚ñè        | 48/395 [01:05<06:47,  1.17s/it]\u001b[A\nTrain Epoch 1:  12%|‚ñà‚ñè        | 49/395 [01:06<06:35,  1.14s/it]\u001b[A\nTrain Epoch 1:  13%|‚ñà‚ñé        | 50/395 [01:07<07:00,  1.22s/it]\u001b[A\nTrain Epoch 1:  13%|‚ñà‚ñé        | 51/395 [01:08<06:55,  1.21s/it]\u001b[A\nTrain Epoch 1:  13%|‚ñà‚ñé        | 52/395 [01:09<06:42,  1.17s/it]\u001b[A\nTrain Epoch 1:  13%|‚ñà‚ñé        | 53/395 [01:10<06:37,  1.16s/it]\u001b[A\nTrain Epoch 1:  14%|‚ñà‚ñé        | 54/395 [01:11<06:04,  1.07s/it]\u001b[A\nTrain Epoch 1:  14%|‚ñà‚ñç        | 55/395 [01:12<06:21,  1.12s/it]\u001b[A\nTrain Epoch 1:  14%|‚ñà‚ñç        | 56/395 [01:14<06:51,  1.21s/it]\u001b[A\nTrain Epoch 1:  14%|‚ñà‚ñç        | 57/395 [01:15<06:39,  1.18s/it]\u001b[A\nTrain Epoch 1:  15%|‚ñà‚ñç        | 58/395 [01:16<05:40,  1.01s/it]\u001b[A\nTrain Epoch 1:  15%|‚ñà‚ñç        | 59/395 [01:16<05:01,  1.11it/s]\u001b[A\nTrain Epoch 1:  15%|‚ñà‚ñå        | 60/395 [01:17<04:32,  1.23it/s]\u001b[A\nTrain Epoch 1:  15%|‚ñà‚ñå        | 61/395 [01:18<04:12,  1.32it/s]\u001b[A\nTrain Epoch 1:  16%|‚ñà‚ñå        | 62/395 [01:18<03:51,  1.44it/s]\u001b[A\nTrain Epoch 1:  16%|‚ñà‚ñå        | 63/395 [01:19<03:47,  1.46it/s]\u001b[A\nTrain Epoch 1:  16%|‚ñà‚ñå        | 64/395 [01:19<03:40,  1.50it/s]\u001b[A\nTrain Epoch 1:  16%|‚ñà‚ñã        | 65/395 [01:20<03:39,  1.51it/s]\u001b[A\nTrain Epoch 1:  17%|‚ñà‚ñã        | 66/395 [01:21<03:36,  1.52it/s]\u001b[A\nTrain Epoch 1:  17%|‚ñà‚ñã        | 67/395 [01:21<03:32,  1.54it/s]\u001b[A\nTrain Epoch 1:  17%|‚ñà‚ñã        | 68/395 [01:22<03:31,  1.54it/s]\u001b[A\nTrain Epoch 1:  17%|‚ñà‚ñã        | 69/395 [01:23<03:31,  1.54it/s]\u001b[A\nTrain Epoch 1:  18%|‚ñà‚ñä        | 70/395 [01:23<03:46,  1.43it/s]\u001b[A\nTrain Epoch 1:  18%|‚ñà‚ñä        | 71/395 [01:24<03:44,  1.45it/s]\u001b[A\nTrain Epoch 1:  18%|‚ñà‚ñä        | 72/395 [01:25<03:58,  1.35it/s]\u001b[A\nTrain Epoch 1:  18%|‚ñà‚ñä        | 73/395 [01:26<03:51,  1.39it/s]\u001b[A\nTrain Epoch 1:  19%|‚ñà‚ñä        | 74/395 [01:26<03:35,  1.49it/s]\u001b[A\nTrain Epoch 1:  19%|‚ñà‚ñâ        | 75/395 [01:27<03:33,  1.50it/s]\u001b[A\nTrain Epoch 1:  19%|‚ñà‚ñâ        | 76/395 [01:27<03:33,  1.49it/s]\u001b[A\nTrain Epoch 1:  19%|‚ñà‚ñâ        | 77/395 [01:28<03:36,  1.47it/s]\u001b[A\nTrain Epoch 1:  20%|‚ñà‚ñâ        | 78/395 [01:29<03:34,  1.48it/s]\u001b[A\nTrain Epoch 1:  20%|‚ñà‚ñà        | 79/395 [01:30<03:34,  1.47it/s]\u001b[A\nTrain Epoch 1:  20%|‚ñà‚ñà        | 80/395 [01:30<03:26,  1.52it/s]\u001b[A\nTrain Epoch 1:  21%|‚ñà‚ñà        | 81/395 [01:31<03:48,  1.37it/s]\u001b[A\nTrain Epoch 1:  21%|‚ñà‚ñà        | 82/395 [01:32<03:36,  1.44it/s]\u001b[A\nTrain Epoch 1:  21%|‚ñà‚ñà        | 83/395 [01:32<03:33,  1.46it/s]\u001b[A\nTrain Epoch 1:  21%|‚ñà‚ñà‚ñè       | 84/395 [01:33<03:33,  1.46it/s]\u001b[A\nTrain Epoch 1:  22%|‚ñà‚ñà‚ñè       | 85/395 [01:34<03:32,  1.46it/s]\u001b[A\nTrain Epoch 1:  22%|‚ñà‚ñà‚ñè       | 86/395 [01:34<03:26,  1.49it/s]\u001b[A\nTrain Epoch 1:  22%|‚ñà‚ñà‚ñè       | 87/395 [01:35<03:23,  1.51it/s]\u001b[A\nTrain Epoch 1:  22%|‚ñà‚ñà‚ñè       | 88/395 [01:36<03:24,  1.50it/s]\u001b[A\nTrain Epoch 1:  23%|‚ñà‚ñà‚ñé       | 89/395 [01:36<03:22,  1.51it/s]\u001b[A\nTrain Epoch 1:  23%|‚ñà‚ñà‚ñé       | 90/395 [01:37<03:21,  1.51it/s]\u001b[A\nTrain Epoch 1:  23%|‚ñà‚ñà‚ñé       | 91/395 [01:38<03:15,  1.56it/s]\u001b[A\nTrain Epoch 1:  23%|‚ñà‚ñà‚ñé       | 92/395 [01:38<03:11,  1.58it/s]\u001b[A\nTrain Epoch 1:  24%|‚ñà‚ñà‚ñé       | 93/395 [01:39<03:15,  1.54it/s]\u001b[A\nTrain Epoch 1:  24%|‚ñà‚ñà‚ñç       | 94/395 [01:39<03:14,  1.54it/s]\u001b[A\nTrain Epoch 1:  24%|‚ñà‚ñà‚ñç       | 95/395 [01:40<03:19,  1.51it/s]\u001b[A\nTrain Epoch 1:  24%|‚ñà‚ñà‚ñç       | 96/395 [01:41<03:25,  1.45it/s]\u001b[A\nTrain Epoch 1:  25%|‚ñà‚ñà‚ñç       | 97/395 [01:42<03:16,  1.52it/s]\u001b[A\nTrain Epoch 1:  25%|‚ñà‚ñà‚ñç       | 98/395 [01:42<03:04,  1.61it/s]\u001b[A\nTrain Epoch 1:  25%|‚ñà‚ñà‚ñå       | 99/395 [01:43<03:16,  1.50it/s]\u001b[A\nTrain Epoch 1:  25%|‚ñà‚ñà‚ñå       | 100/395 [01:44<03:20,  1.47it/s]\u001b[A\nTrain Epoch 1:  26%|‚ñà‚ñà‚ñå       | 101/395 [01:44<03:15,  1.50it/s]\u001b[A\nTrain Epoch 1:  26%|‚ñà‚ñà‚ñå       | 102/395 [01:45<03:14,  1.51it/s]\u001b[A\nTrain Epoch 1:  26%|‚ñà‚ñà‚ñå       | 103/395 [01:46<03:25,  1.42it/s]\u001b[A\nTrain Epoch 1:  26%|‚ñà‚ñà‚ñã       | 104/395 [01:46<03:15,  1.49it/s]\u001b[A\nTrain Epoch 1:  27%|‚ñà‚ñà‚ñã       | 105/395 [01:47<03:16,  1.48it/s]\u001b[A\nTrain Epoch 1:  27%|‚ñà‚ñà‚ñã       | 106/395 [01:48<03:08,  1.53it/s]\u001b[A\nTrain Epoch 1:  27%|‚ñà‚ñà‚ñã       | 107/395 [01:48<03:20,  1.43it/s]\u001b[A\nTrain Epoch 1:  27%|‚ñà‚ñà‚ñã       | 108/395 [01:49<03:38,  1.32it/s]\u001b[A\nTrain Epoch 1:  28%|‚ñà‚ñà‚ñä       | 109/395 [01:50<03:22,  1.41it/s]\u001b[A\nTrain Epoch 1:  28%|‚ñà‚ñà‚ñä       | 110/395 [01:50<03:16,  1.45it/s]\u001b[A\nTrain Epoch 1:  28%|‚ñà‚ñà‚ñä       | 111/395 [01:51<03:08,  1.50it/s]\u001b[A\nTrain Epoch 1:  28%|‚ñà‚ñà‚ñä       | 112/395 [01:52<03:05,  1.53it/s]\u001b[A\nTrain Epoch 1:  29%|‚ñà‚ñà‚ñä       | 113/395 [01:52<02:59,  1.57it/s]\u001b[A\nTrain Epoch 1:  29%|‚ñà‚ñà‚ñâ       | 114/395 [01:53<02:59,  1.56it/s]\u001b[A\nTrain Epoch 1:  29%|‚ñà‚ñà‚ñâ       | 115/395 [01:54<03:06,  1.50it/s]\u001b[A\nTrain Epoch 1:  29%|‚ñà‚ñà‚ñâ       | 116/395 [01:54<03:06,  1.50it/s]\u001b[A\nTrain Epoch 1:  30%|‚ñà‚ñà‚ñâ       | 117/395 [01:55<03:00,  1.54it/s]\u001b[A\nTrain Epoch 1:  30%|‚ñà‚ñà‚ñâ       | 118/395 [01:56<02:54,  1.59it/s]\u001b[A\nTrain Epoch 1:  30%|‚ñà‚ñà‚ñà       | 119/395 [01:56<03:01,  1.52it/s]\u001b[A\nTrain Epoch 1:  30%|‚ñà‚ñà‚ñà       | 120/395 [01:57<02:59,  1.53it/s]\u001b[A\nTrain Epoch 1:  31%|‚ñà‚ñà‚ñà       | 121/395 [01:57<02:52,  1.59it/s]\u001b[A\nTrain Epoch 1:  31%|‚ñà‚ñà‚ñà       | 122/395 [01:58<03:00,  1.51it/s]\u001b[A\nTrain Epoch 1:  31%|‚ñà‚ñà‚ñà       | 123/395 [01:59<03:02,  1.49it/s]\u001b[A\nTrain Epoch 1:  31%|‚ñà‚ñà‚ñà‚ñè      | 124/395 [01:59<02:53,  1.56it/s]\u001b[A\nTrain Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 125/395 [02:00<02:46,  1.62it/s]\u001b[A\nTrain Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 126/395 [02:01<02:48,  1.59it/s]\u001b[A\nTrain Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 127/395 [02:01<02:48,  1.59it/s]\u001b[A\nTrain Epoch 1:  32%|‚ñà‚ñà‚ñà‚ñè      | 128/395 [02:02<02:49,  1.58it/s]\u001b[A\nTrain Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 129/395 [02:03<03:01,  1.47it/s]\u001b[A\nTrain Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 130/395 [02:03<03:01,  1.46it/s]\u001b[A\nTrain Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 131/395 [02:04<03:04,  1.43it/s]\u001b[A\nTrain Epoch 1:  33%|‚ñà‚ñà‚ñà‚ñé      | 132/395 [02:05<02:53,  1.52it/s]\u001b[A\nTrain Epoch 1:  34%|‚ñà‚ñà‚ñà‚ñé      | 133/395 [02:06<03:07,  1.40it/s]\u001b[A\nTrain Epoch 1:  34%|‚ñà‚ñà‚ñà‚ñç      | 134/395 [02:06<02:59,  1.45it/s]\u001b[A\nTrain Epoch 1:  34%|‚ñà‚ñà‚ñà‚ñç      | 135/395 [02:07<03:04,  1.41it/s]\u001b[A\nTrain Epoch 1:  34%|‚ñà‚ñà‚ñà‚ñç      | 136/395 [02:08<02:59,  1.45it/s]\u001b[A\nTrain Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñç      | 137/395 [02:08<02:51,  1.51it/s]\u001b[A\nTrain Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñç      | 138/395 [02:09<02:47,  1.53it/s]\u001b[A\nTrain Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñå      | 139/395 [02:10<02:56,  1.45it/s]\u001b[A\nTrain Epoch 1:  35%|‚ñà‚ñà‚ñà‚ñå      | 140/395 [02:10<02:59,  1.42it/s]\u001b[A\nTrain Epoch 1:  36%|‚ñà‚ñà‚ñà‚ñå      | 141/395 [02:11<02:54,  1.45it/s]\u001b[A\nTrain Epoch 1:  36%|‚ñà‚ñà‚ñà‚ñå      | 142/395 [02:12<02:49,  1.49it/s]\u001b[A\nTrain Epoch 1:  36%|‚ñà‚ñà‚ñà‚ñå      | 143/395 [02:12<02:49,  1.49it/s]\u001b[A\nTrain Epoch 1:  36%|‚ñà‚ñà‚ñà‚ñã      | 144/395 [02:13<02:48,  1.49it/s]\u001b[A\nTrain Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 145/395 [02:14<02:46,  1.50it/s]\u001b[A\nTrain Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 146/395 [02:15<03:32,  1.17it/s]\u001b[A\nTrain Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 147/395 [02:16<03:14,  1.27it/s]\u001b[A\nTrain Epoch 1:  37%|‚ñà‚ñà‚ñà‚ñã      | 148/395 [02:16<03:05,  1.33it/s]\u001b[A\nTrain Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 149/395 [02:17<02:57,  1.38it/s]\u001b[A\nTrain Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 150/395 [02:17<02:45,  1.48it/s]\u001b[A\nTrain Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 151/395 [02:18<02:42,  1.50it/s]\u001b[A\nTrain Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 152/395 [02:19<02:44,  1.48it/s]\u001b[A\nTrain Epoch 1:  39%|‚ñà‚ñà‚ñà‚ñä      | 153/395 [02:19<02:37,  1.54it/s]\u001b[A\nTrain Epoch 1:  39%|‚ñà‚ñà‚ñà‚ñâ      | 154/395 [02:20<02:37,  1.53it/s]\u001b[A\nTrain Epoch 1:  39%|‚ñà‚ñà‚ñà‚ñâ      | 155/395 [02:21<02:37,  1.52it/s]\u001b[A\nTrain Epoch 1:  39%|‚ñà‚ñà‚ñà‚ñâ      | 156/395 [02:21<02:36,  1.53it/s]\u001b[A\nTrain Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñâ      | 157/395 [02:22<02:28,  1.60it/s]\u001b[A\nTrain Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñà      | 158/395 [02:23<02:37,  1.51it/s]\u001b[A\nTrain Epoch 1:  40%|‚ñà‚ñà‚ñà‚ñà      | 159/395 [02:23<02:40,  1.47it/s]\u001b[A\nTrain Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà      | 160/395 [02:24<02:41,  1.46it/s]\u001b[A\nTrain Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà      | 161/395 [02:25<02:40,  1.46it/s]\u001b[A\nTrain Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà      | 162/395 [02:25<02:33,  1.51it/s]\u001b[A\nTrain Epoch 1:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 163/395 [02:26<02:30,  1.54it/s]\u001b[A\nTrain Epoch 1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 164/395 [02:27<02:33,  1.51it/s]\u001b[A\nTrain Epoch 1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 165/395 [02:27<02:32,  1.51it/s]\u001b[A\nTrain Epoch 1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 166/395 [02:28<02:28,  1.54it/s]\u001b[A\nTrain Epoch 1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 167/395 [02:29<02:37,  1.45it/s]\u001b[A\nTrain Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 168/395 [02:29<02:30,  1.51it/s]\u001b[A\nTrain Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 169/395 [02:30<02:20,  1.61it/s]\u001b[A\nTrain Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 170/395 [02:31<02:25,  1.55it/s]\u001b[A\nTrain Epoch 1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 171/395 [02:31<02:21,  1.59it/s]\u001b[A\nTrain Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 172/395 [02:32<02:23,  1.56it/s]\u001b[A\nTrain Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 173/395 [02:32<02:20,  1.58it/s]\u001b[A\nTrain Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 174/395 [02:33<02:17,  1.61it/s]\u001b[A\nTrain Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 175/395 [02:34<02:21,  1.55it/s]\u001b[A\nTrain Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 176/395 [02:34<02:15,  1.62it/s]\u001b[A\nTrain Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 177/395 [02:35<02:10,  1.66it/s]\u001b[A\nTrain Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 178/395 [02:36<02:12,  1.63it/s]\u001b[A\nTrain Epoch 1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 179/395 [02:36<02:12,  1.63it/s]\u001b[A\nTrain Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 180/395 [02:37<02:35,  1.39it/s]\u001b[A\nTrain Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 181/395 [02:38<02:34,  1.38it/s]\u001b[A\nTrain Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 182/395 [02:39<02:34,  1.38it/s]\u001b[A\nTrain Epoch 1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 183/395 [02:39<02:23,  1.48it/s]\u001b[A\nTrain Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 184/395 [02:40<02:22,  1.48it/s]\u001b[A\nTrain Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 185/395 [02:40<02:21,  1.49it/s]\u001b[A\nTrain Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 186/395 [02:41<02:21,  1.48it/s]\u001b[A\nTrain Epoch 1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 187/395 [02:42<02:19,  1.49it/s]\u001b[A\nTrain Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 188/395 [02:42<02:12,  1.56it/s]\u001b[A\nTrain Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 189/395 [02:43<02:13,  1.54it/s]\u001b[A\nTrain Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 190/395 [02:44<02:09,  1.58it/s]\u001b[A\nTrain Epoch 1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 191/395 [02:44<02:15,  1.50it/s]\u001b[A\nTrain Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 192/395 [02:45<02:14,  1.51it/s]\u001b[A\nTrain Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 193/395 [02:46<02:17,  1.47it/s]\u001b[A\nTrain Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 194/395 [02:46<02:13,  1.51it/s]\u001b[A\nTrain Epoch 1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 195/395 [02:47<02:12,  1.51it/s]\u001b[A\nTrain Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 196/395 [02:48<02:11,  1.51it/s]\u001b[A\nTrain Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 197/395 [02:48<02:11,  1.51it/s]\u001b[A\nTrain Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 198/395 [02:49<02:09,  1.52it/s]\u001b[A\nTrain Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 199/395 [02:50<02:24,  1.35it/s]\u001b[A\nTrain Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 200/395 [02:51<02:22,  1.37it/s]\u001b[A\nTrain Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 201/395 [02:51<02:28,  1.31it/s]\u001b[A\nTrain Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 202/395 [02:52<02:27,  1.31it/s]\u001b[A\nTrain Epoch 1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 203/395 [02:53<02:18,  1.38it/s]\u001b[A\nTrain Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 204/395 [02:54<02:12,  1.44it/s]\u001b[A\nTrain Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 205/395 [02:54<02:09,  1.46it/s]\u001b[A\nTrain Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 206/395 [02:55<02:03,  1.53it/s]\u001b[A\nTrain Epoch 1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 207/395 [02:56<02:26,  1.28it/s]\u001b[A\nTrain Epoch 1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 208/395 [02:57<02:20,  1.33it/s]\u001b[A\nTrain Epoch 1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 209/395 [02:57<02:21,  1.31it/s]\u001b[A\nTrain Epoch 1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 210/395 [02:58<02:24,  1.28it/s]\u001b[A\nTrain Epoch 1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 211/395 [02:59<02:19,  1.32it/s]\u001b[A\nTrain Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 212/395 [02:59<02:06,  1.44it/s]\u001b[A\nTrain Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 213/395 [03:00<02:05,  1.45it/s]\u001b[A\nTrain Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 214/395 [03:01<01:57,  1.54it/s]\u001b[A\nTrain Epoch 1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 215/395 [03:01<01:53,  1.58it/s]\u001b[A\nTrain Epoch 1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 216/395 [03:02<01:56,  1.54it/s]\u001b[A\nTrain Epoch 1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 217/395 [03:02<01:50,  1.61it/s]\u001b[A\nTrain Epoch 1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 218/395 [03:03<01:47,  1.65it/s]\u001b[A\nTrain Epoch 1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 219/395 [03:04<01:51,  1.58it/s]\u001b[A\nTrain Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 220/395 [03:04<01:52,  1.56it/s]\u001b[A\nTrain Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 221/395 [03:05<01:51,  1.56it/s]\u001b[A\nTrain Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 222/395 [03:06<01:59,  1.44it/s]\u001b[A\nTrain Epoch 1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 223/395 [03:07<01:59,  1.44it/s]\u001b[A\nTrain Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 224/395 [03:07<02:01,  1.41it/s]\u001b[A\nTrain Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 225/395 [03:08<01:52,  1.51it/s]\u001b[A\nTrain Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 226/395 [03:09<02:09,  1.31it/s]\u001b[A\nTrain Epoch 1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 227/395 [03:09<02:03,  1.36it/s]\u001b[A\nTrain Epoch 1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 228/395 [03:10<01:57,  1.42it/s]\u001b[A\nTrain Epoch 1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 229/395 [03:11<01:54,  1.44it/s]\u001b[A\nTrain Epoch 1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 230/395 [03:11<01:52,  1.47it/s]\u001b[A\nTrain Epoch 1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 231/395 [03:12<01:53,  1.44it/s]\u001b[A\nTrain Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 232/395 [03:13<01:54,  1.42it/s]\u001b[A\nTrain Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 233/395 [03:13<01:48,  1.49it/s]\u001b[A\nTrain Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 234/395 [03:14<01:45,  1.53it/s]\u001b[A\nTrain Epoch 1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 235/395 [03:15<01:39,  1.61it/s]\u001b[A\nTrain Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 236/395 [03:15<01:44,  1.52it/s]\u001b[A\nTrain Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 237/395 [03:16<01:38,  1.61it/s]\u001b[A\nTrain Epoch 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 238/395 [03:17<01:40,  1.57it/s]\u001b[A\nTrain Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 239/395 [03:17<01:39,  1.56it/s]\u001b[A\nTrain Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 240/395 [03:18<01:39,  1.55it/s]\u001b[A\nTrain Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 241/395 [03:19<01:39,  1.55it/s]\u001b[A\nTrain Epoch 1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 242/395 [03:19<01:41,  1.51it/s]\u001b[A\nTrain Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 243/395 [03:20<01:46,  1.43it/s]\u001b[A\nTrain Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 244/395 [03:21<01:49,  1.38it/s]\u001b[A\nTrain Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 245/395 [03:21<01:43,  1.45it/s]\u001b[A\nTrain Epoch 1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 246/395 [03:22<01:41,  1.47it/s]\u001b[A\nTrain Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 247/395 [03:23<01:41,  1.46it/s]\u001b[A\nTrain Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 248/395 [03:23<01:32,  1.59it/s]\u001b[A\nTrain Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 249/395 [03:24<01:33,  1.56it/s]\u001b[A\nTrain Epoch 1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 250/395 [03:25<01:38,  1.47it/s]\u001b[A\nTrain Epoch 1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 251/395 [03:25<01:41,  1.41it/s]\u001b[A\nTrain Epoch 1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 252/395 [03:26<01:36,  1.49it/s]\u001b[A\nTrain Epoch 1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 253/395 [03:27<01:36,  1.47it/s]\u001b[A\nTrain Epoch 1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 254/395 [03:28<01:38,  1.43it/s]\u001b[A\nTrain Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 255/395 [03:28<01:31,  1.53it/s]\u001b[A\nTrain Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 256/395 [03:29<01:31,  1.51it/s]\u001b[A\nTrain Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 257/395 [03:29<01:28,  1.56it/s]\u001b[A\nTrain Epoch 1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 258/395 [03:30<01:27,  1.57it/s]\u001b[A\nTrain Epoch 1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 259/395 [03:31<01:30,  1.50it/s]\u001b[A\nTrain Epoch 1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 260/395 [03:31<01:30,  1.49it/s]\u001b[A\nTrain Epoch 1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 261/395 [03:32<01:35,  1.40it/s]\u001b[A\nTrain Epoch 1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 262/395 [03:33<01:32,  1.44it/s]\u001b[A\nTrain Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 263/395 [03:34<01:30,  1.45it/s]\u001b[A\nTrain Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 264/395 [03:34<01:38,  1.33it/s]\u001b[A\nTrain Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 265/395 [03:35<01:36,  1.34it/s]\u001b[A\nTrain Epoch 1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 266/395 [03:36<01:33,  1.38it/s]\u001b[A\nTrain Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 267/395 [03:36<01:27,  1.46it/s]\u001b[A\nTrain Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 268/395 [03:37<01:27,  1.45it/s]\u001b[A\nTrain Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 269/395 [03:38<01:24,  1.49it/s]\u001b[A\nTrain Epoch 1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 270/395 [03:38<01:22,  1.51it/s]\u001b[A\nTrain Epoch 1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 271/395 [03:39<01:20,  1.55it/s]\u001b[A\nTrain Epoch 1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 272/395 [03:40<01:21,  1.52it/s]\u001b[A\nTrain Epoch 1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 273/395 [03:40<01:17,  1.56it/s]\u001b[A\nTrain Epoch 1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 274/395 [03:41<01:14,  1.63it/s]\u001b[A\nTrain Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 275/395 [03:42<01:15,  1.60it/s]\u001b[A\nTrain Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 276/395 [03:42<01:13,  1.62it/s]\u001b[A\nTrain Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 277/395 [03:43<01:21,  1.46it/s]\u001b[A\nTrain Epoch 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 278/395 [03:44<01:19,  1.48it/s]\u001b[A\nTrain Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 279/395 [03:44<01:20,  1.44it/s]\u001b[A\nTrain Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 280/395 [03:45<01:27,  1.31it/s]\u001b[A\nTrain Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 281/395 [03:46<01:21,  1.40it/s]\u001b[A\nTrain Epoch 1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 282/395 [03:47<01:19,  1.41it/s]\u001b[A\nTrain Epoch 1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 283/395 [03:47<01:15,  1.47it/s]\u001b[A\nTrain Epoch 1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 284/395 [03:48<01:11,  1.54it/s]\u001b[A\nTrain Epoch 1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 285/395 [03:49<01:17,  1.41it/s]\u001b[A\nTrain Epoch 1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 286/395 [03:49<01:17,  1.41it/s]\u001b[A\nTrain Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 287/395 [03:50<01:14,  1.45it/s]\u001b[A\nTrain Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 288/395 [03:51<01:12,  1.47it/s]\u001b[A\nTrain Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 289/395 [03:51<01:11,  1.49it/s]\u001b[A\nTrain Epoch 1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 290/395 [03:52<01:10,  1.48it/s]\u001b[A\nTrain Epoch 1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 291/395 [03:53<01:11,  1.46it/s]\u001b[A\nTrain Epoch 1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 292/395 [03:54<01:16,  1.35it/s]\u001b[A\nTrain Epoch 1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 293/395 [03:54<01:14,  1.36it/s]\u001b[A\nTrain Epoch 1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 294/395 [03:55<01:12,  1.38it/s]\u001b[A\nTrain Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 295/395 [03:56<01:11,  1.40it/s]\u001b[A\nTrain Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 296/395 [03:56<01:10,  1.41it/s]\u001b[A\nTrain Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 297/395 [03:57<01:07,  1.46it/s]\u001b[A\nTrain Epoch 1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 298/395 [03:58<01:11,  1.36it/s]\u001b[A\nTrain Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 299/395 [03:58<01:08,  1.40it/s]\u001b[A\nTrain Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 300/395 [03:59<01:04,  1.46it/s]\u001b[A\nTrain Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 301/395 [04:00<01:04,  1.45it/s]\u001b[A\nTrain Epoch 1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 302/395 [04:00<01:01,  1.50it/s]\u001b[A\nTrain Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 303/395 [04:01<00:59,  1.53it/s]\u001b[A\nTrain Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 304/395 [04:02<01:01,  1.47it/s]\u001b[A\nTrain Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 305/395 [04:03<01:13,  1.23it/s]\u001b[A\nTrain Epoch 1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 306/395 [04:04<01:08,  1.31it/s]\u001b[A\nTrain Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 307/395 [04:04<01:06,  1.32it/s]\u001b[A\nTrain Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 308/395 [04:05<01:01,  1.42it/s]\u001b[A\nTrain Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 309/395 [04:05<00:58,  1.47it/s]\u001b[A\nTrain Epoch 1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 310/395 [04:06<00:57,  1.47it/s]\u001b[A\nTrain Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 311/395 [04:07<00:58,  1.44it/s]\u001b[A\nTrain Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 312/395 [04:08<00:55,  1.49it/s]\u001b[A\nTrain Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 313/395 [04:08<00:52,  1.56it/s]\u001b[A\nTrain Epoch 1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 314/395 [04:09<00:53,  1.52it/s]\u001b[A\nTrain Epoch 1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 315/395 [04:10<00:57,  1.40it/s]\u001b[A\nTrain Epoch 1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 316/395 [04:10<00:54,  1.45it/s]\u001b[A\nTrain Epoch 1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 317/395 [04:11<00:53,  1.45it/s]\u001b[A\nTrain Epoch 1:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 318/395 [04:11<00:49,  1.55it/s]\u001b[A\nTrain Epoch 1:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 319/395 [04:12<00:50,  1.49it/s]\u001b[A\nTrain Epoch 1:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 320/395 [04:13<00:48,  1.53it/s]\u001b[A\nTrain Epoch 1:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 321/395 [04:13<00:46,  1.58it/s]\u001b[A\nTrain Epoch 1:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 322/395 [04:14<00:48,  1.49it/s]\u001b[A\nTrain Epoch 1:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 323/395 [04:15<00:45,  1.57it/s]\u001b[A\nTrain Epoch 1:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 324/395 [04:15<00:46,  1.53it/s]\u001b[A\nTrain Epoch 1:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 325/395 [04:16<00:45,  1.55it/s]\u001b[A\nTrain Epoch 1:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 326/395 [04:17<00:42,  1.61it/s]\u001b[A\nTrain Epoch 1:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 327/395 [04:17<00:41,  1.65it/s]\u001b[A\nTrain Epoch 1:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 328/395 [04:18<00:40,  1.65it/s]\u001b[A\nTrain Epoch 1:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 329/395 [04:19<00:43,  1.53it/s]\u001b[A\nTrain Epoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 330/395 [04:19<00:43,  1.50it/s]\u001b[A\nTrain Epoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 331/395 [04:20<00:43,  1.46it/s]\u001b[A\nTrain Epoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 332/395 [04:21<00:41,  1.53it/s]\u001b[A\nTrain Epoch 1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 333/395 [04:21<00:41,  1.49it/s]\u001b[A\nTrain Epoch 1:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 334/395 [04:22<00:39,  1.53it/s]\u001b[A\nTrain Epoch 1:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 335/395 [04:22<00:38,  1.57it/s]\u001b[A\nTrain Epoch 1:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 336/395 [04:23<00:37,  1.57it/s]\u001b[A\nTrain Epoch 1:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 337/395 [04:24<00:35,  1.64it/s]\u001b[A\nTrain Epoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 338/395 [04:24<00:35,  1.60it/s]\u001b[A\nTrain Epoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 339/395 [04:25<00:35,  1.59it/s]\u001b[A\nTrain Epoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 340/395 [04:26<00:34,  1.60it/s]\u001b[A\nTrain Epoch 1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 341/395 [04:26<00:35,  1.53it/s]\u001b[A\nTrain Epoch 1:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 342/395 [04:27<00:36,  1.43it/s]\u001b[A\nTrain Epoch 1:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 343/395 [04:28<00:34,  1.50it/s]\u001b[A\nTrain Epoch 1:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 344/395 [04:28<00:34,  1.48it/s]\u001b[A\nTrain Epoch 1:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 345/395 [04:29<00:34,  1.46it/s]\u001b[A\nTrain Epoch 1:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 346/395 [04:30<00:33,  1.48it/s]\u001b[A\nTrain Epoch 1:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 347/395 [04:30<00:32,  1.49it/s]\u001b[A\nTrain Epoch 1:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 348/395 [04:31<00:34,  1.37it/s]\u001b[A\nTrain Epoch 1:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 349/395 [04:32<00:32,  1.41it/s]\u001b[A\nTrain Epoch 1:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 350/395 [04:33<00:32,  1.40it/s]\u001b[A\nTrain Epoch 1:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 351/395 [04:33<00:29,  1.49it/s]\u001b[A\nTrain Epoch 1:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 352/395 [04:34<00:29,  1.48it/s]\u001b[A\nTrain Epoch 1:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 353/395 [04:35<00:29,  1.42it/s]\u001b[A\nTrain Epoch 1:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 354/395 [04:35<00:27,  1.51it/s]\u001b[A\nTrain Epoch 1:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 355/395 [04:36<00:25,  1.58it/s]\u001b[A\nTrain Epoch 1:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 356/395 [04:36<00:23,  1.64it/s]\u001b[A\nTrain Epoch 1:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 357/395 [04:37<00:23,  1.61it/s]\u001b[A\nTrain Epoch 1:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 358/395 [04:38<00:24,  1.54it/s]\u001b[A\nTrain Epoch 1:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 359/395 [04:39<00:25,  1.42it/s]\u001b[A\nTrain Epoch 1:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 360/395 [04:39<00:25,  1.38it/s]\u001b[A\nTrain Epoch 1:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 361/395 [04:40<00:23,  1.45it/s]\u001b[A\nTrain Epoch 1:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 362/395 [04:41<00:22,  1.45it/s]\u001b[A\nTrain Epoch 1:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 363/395 [04:41<00:23,  1.36it/s]\u001b[A\nTrain Epoch 1:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 364/395 [04:42<00:21,  1.42it/s]\u001b[A\nTrain Epoch 1:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 365/395 [04:43<00:24,  1.24it/s]\u001b[A\nTrain Epoch 1:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 366/395 [04:44<00:22,  1.30it/s]\u001b[A\nTrain Epoch 1:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 367/395 [04:45<00:22,  1.25it/s]\u001b[A\nTrain Epoch 1:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 368/395 [04:45<00:20,  1.29it/s]\u001b[A\nTrain Epoch 1:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 369/395 [04:46<00:19,  1.35it/s]\u001b[A\nTrain Epoch 1:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 370/395 [04:47<00:18,  1.37it/s]\u001b[A\nTrain Epoch 1:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 371/395 [04:47<00:16,  1.47it/s]\u001b[A\nTrain Epoch 1:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 372/395 [04:48<00:14,  1.55it/s]\u001b[A\nTrain Epoch 1:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 373/395 [04:49<00:14,  1.50it/s]\u001b[A\nTrain Epoch 1:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 374/395 [04:49<00:13,  1.53it/s]\u001b[A\nTrain Epoch 1:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 375/395 [04:50<00:12,  1.57it/s]\u001b[A\nTrain Epoch 1:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 376/395 [04:51<00:12,  1.51it/s]\u001b[A\nTrain Epoch 1:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 377/395 [04:51<00:12,  1.45it/s]\u001b[A\nTrain Epoch 1:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 378/395 [04:52<00:11,  1.46it/s]\u001b[A\nTrain Epoch 1:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 379/395 [04:53<00:11,  1.44it/s]\u001b[A\nTrain Epoch 1:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 380/395 [04:53<00:10,  1.43it/s]\u001b[A\nTrain Epoch 1:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 381/395 [04:54<00:09,  1.45it/s]\u001b[A\nTrain Epoch 1:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 382/395 [04:55<00:09,  1.41it/s]\u001b[A\nTrain Epoch 1:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 383/395 [04:55<00:07,  1.52it/s]\u001b[A\nTrain Epoch 1:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 384/395 [04:56<00:07,  1.42it/s]\u001b[A\nTrain Epoch 1:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 385/395 [04:57<00:06,  1.46it/s]\u001b[A\nTrain Epoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 386/395 [04:58<00:06,  1.49it/s]\u001b[A\nTrain Epoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 387/395 [04:58<00:05,  1.54it/s]\u001b[A\nTrain Epoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 388/395 [04:59<00:04,  1.47it/s]\u001b[A\nTrain Epoch 1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 389/395 [05:00<00:04,  1.48it/s]\u001b[A\nTrain Epoch 1:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 390/395 [05:00<00:03,  1.47it/s]\u001b[A\nTrain Epoch 1:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 391/395 [05:01<00:02,  1.47it/s]\u001b[A\nTrain Epoch 1:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 392/395 [05:02<00:02,  1.49it/s]\u001b[A\nTrain Epoch 1:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 393/395 [05:02<00:01,  1.49it/s]\u001b[A\nTrain Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 394/395 [05:03<00:00,  1.44it/s]\u001b[A\nTrain Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [05:03<00:00,  1.30it/s]\u001b[A\nValidation Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:25<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [1/15]\nTrain - Loss: 0.9232, Acc: 0.6077, F1: 0.6102\nVal   - Loss: 0.8163, Acc: 0.6541, F1: 0.6721\n‚úÖ Validation F1 improved ‚Äî model saved.\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:59<00:00,  1.65it/s]\nValidation Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [2/15]\nTrain - Loss: 0.6524, Acc: 0.7769, F1: 0.7791\nVal   - Loss: 0.7273, Acc: 0.7361, F1: 0.7354\n‚úÖ Validation F1 improved ‚Äî model saved.\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [04:00<00:00,  1.65it/s]\nValidation Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [3/15]\nTrain - Loss: 0.4786, Acc: 0.8685, F1: 0.8681\nVal   - Loss: 1.4125, Acc: 0.7472, F1: 0.7456\n‚úÖ Validation F1 improved ‚Äî model saved.\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [04:00<00:00,  1.64it/s]\nValidation Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [4/15]\nTrain - Loss: 0.3371, Acc: 0.9265, F1: 0.9264\nVal   - Loss: 1.9627, Acc: 0.7494, F1: 0.7531\n‚úÖ Validation F1 improved ‚Äî model saved.\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [04:00<00:00,  1.64it/s]\nValidation Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [5/15]\nTrain - Loss: 0.2753, Acc: 0.9534, F1: 0.9532\nVal   - Loss: 2.0989, Acc: 0.7472, F1: 0.7458\n‚è∞ No improvement ‚Äî patience 1/5\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:59<00:00,  1.65it/s]\nValidation Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [6/15]\nTrain - Loss: 0.1870, Acc: 0.9658, F1: 0.9658\nVal   - Loss: 2.3513, Acc: 0.7361, F1: 0.7397\n‚è∞ No improvement ‚Äî patience 2/5\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:59<00:00,  1.65it/s]\nValidation Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [7/15]\nTrain - Loss: 0.1524, Acc: 0.9785, F1: 0.9784\nVal   - Loss: 3.4326, Acc: 0.7517, F1: 0.7467\n‚è∞ No improvement ‚Äî patience 3/5\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [04:01<00:00,  1.63it/s]\nValidation Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [8/15]\nTrain - Loss: 0.1294, Acc: 0.9794, F1: 0.9794\nVal   - Loss: 3.1050, Acc: 0.7605, F1: 0.7562\n‚úÖ Validation F1 improved ‚Äî model saved.\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [04:01<00:00,  1.63it/s]\nValidation Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [9/15]\nTrain - Loss: 0.1077, Acc: 0.9845, F1: 0.9845\nVal   - Loss: 3.2978, Acc: 0.7472, F1: 0.7481\n‚è∞ No improvement ‚Äî patience 1/5\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [04:03<00:00,  1.62it/s]\nValidation Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [10/15]\nTrain - Loss: 0.1069, Acc: 0.9816, F1: 0.9816\nVal   - Loss: 3.2908, Acc: 0.7428, F1: 0.7437\n‚è∞ No improvement ‚Äî patience 2/5\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [04:02<00:00,  1.63it/s]\nValidation Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [11/15]\nTrain - Loss: 0.1310, Acc: 0.9823, F1: 0.9822\nVal   - Loss: 3.9457, Acc: 0.7450, F1: 0.7462\n‚è∞ No improvement ‚Äî patience 3/5\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [04:02<00:00,  1.63it/s]\nValidation Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:22<00:00,  2.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [12/15]\nTrain - Loss: 0.1004, Acc: 0.9883, F1: 0.9883\nVal   - Loss: 5.1145, Acc: 0.7428, F1: 0.7417\n‚è∞ No improvement ‚Äî patience 4/5\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [04:01<00:00,  1.64it/s]\nValidation Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch [13/15]\nTrain - Loss: 0.0620, Acc: 0.9902, F1: 0.9902\nVal   - Loss: 4.4622, Acc: 0.7517, F1: 0.7467\n‚è∞ No improvement ‚Äî patience 5/5\nüõë Early stopping triggered at epoch 13\n\nüîç Loading best model for final evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Final Test Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 113/113 [00:49<00:00,  2.27it/s]","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nüéØ FINAL MULTIMODAL SENTIMENT ANALYSIS RESULTS\n   MuRIL (Text) + Swin Transformer (Vision) Fusion\n================================================================================\nTest Accuracy:  0.7849\nTest Precision: 0.7843\nTest Recall:    0.7849\nTest F1-Score:  0.7792\nTest Loss:      2.4547\n\nConfusion Matrix:\n[[357  36   9]\n [ 62 275  16]\n [ 42  29  76]]\n\nüìä PER-CLASS DETAILED METRICS:\n--------------------------------------------------\nNegative: Precision=0.7744, Recall=0.8881, F1=0.8273, Support=402\n Neutral: Precision=0.8088, Recall=0.7790, F1=0.7937, Support=353\nPositive: Precision=0.7525, Recall=0.5170, F1=0.6129, Support=147\n\nüìà Classification Report:\n              precision    recall  f1-score   support\n\n    Negative       0.77      0.89      0.83       402\n     Neutral       0.81      0.78      0.79       353\n    Positive       0.75      0.52      0.61       147\n\n    accuracy                           0.78       902\n   macro avg       0.78      0.73      0.74       902\nweighted avg       0.78      0.78      0.78       902\n\n\n‚úÖ Results saved to 'multimodal_results.json'\nüéâ Multimodal sentiment analysis complete!\nüèÜ Best F1-Score achieved: 0.7792\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12397007,"sourceType":"datasetVersion","datasetId":7817576}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ‚úÖ 1Ô∏è‚É£ LIBRARIES & SETUP\nimport os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import AutoTokenizer, AutoModel, ViTImageProcessor, ViTModel\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nimport torch.nn as nn\nimport re\nimport string\n\n# ‚úÖ 2Ô∏è‚É£ PATHS\nimage_dir = \"/kaggle/input/basem/images\"\ninput_csv = \"/kaggle/input/basem/dataset.csv\"\n\n# ‚úÖ 3Ô∏è‚É£ LOAD & PREPROCESS CSV\ndf = pd.read_csv(input_csv)\n\nexisting_data = []\nfor _, row in df.iterrows():\n    image_filename = row['image_path']\n    full_image_path = os.path.join(image_dir, image_filename)\n    if os.path.exists(full_image_path) and pd.notna(row['extracted_text']) and row['extracted_text'].strip():\n        label_converted = row['label 2'] - 1\n        existing_data.append({\n            'Image_path': full_image_path,\n            'Captions': row['extracted_text'],\n            'Label_Sentiment': label_converted\n        })\n\nprocessed_df = pd.DataFrame(existing_data)\n\n# ‚úÖ 4Ô∏è‚É£ DATA SPLITTING\ntrain_df, temp_df = train_test_split(processed_df, test_size=0.3, stratify=processed_df['Label_Sentiment'], random_state=42)\ntest_df, val_df = train_test_split(temp_df, test_size=1/3, stratify=temp_df['Label_Sentiment'], random_state=42)\n\n# ‚úÖ 5Ô∏è‚É£ TOKENIZER & PROCESSOR\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\nvit_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n\n# ‚úÖ 6Ô∏è‚É£ DATASET CLASS\nclass MultimodalDataset(Dataset):\n    def __init__(self, df, tokenizer, processor, max_length=128):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.processor = processor\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(row['Image_path']).convert('RGB')\n        text = row['Captions']\n        label = row['Label_Sentiment']\n\n        encoded = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n\n        image_inputs = self.processor(image, return_tensors=\"pt\")\n\n        return {\n            'input_ids': encoded['input_ids'].squeeze(0),\n            'attention_mask': encoded['attention_mask'].squeeze(0),\n            'pixel_values': image_inputs['pixel_values'].squeeze(0),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# ‚úÖ 7Ô∏è‚É£ DATALOADERS\nbatch_size = 8\n\ntrain_dataset = MultimodalDataset(train_df, tokenizer, vit_processor)\nval_dataset = MultimodalDataset(val_df, tokenizer, vit_processor)\ntest_dataset = MultimodalDataset(test_df, tokenizer, vit_processor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# ‚úÖ 8Ô∏è‚É£ MULTIMODAL MODEL\nclass MultimodalClassifier(nn.Module):\n    def __init__(self, text_model_name, vision_model_name, num_classes=3):\n        super().__init__()\n        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n        self.vision_encoder = ViTModel.from_pretrained(vision_model_name)\n\n        combined_dim = self.text_encoder.config.hidden_size + self.vision_encoder.config.hidden_size\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(combined_dim, num_classes)\n        )\n\n    def forward(self, input_ids, attention_mask, pixel_values):\n        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n        text_cls = text_outputs.last_hidden_state[:, 0, :]\n\n        vision_outputs = self.vision_encoder(pixel_values=pixel_values)\n        vision_cls = vision_outputs.last_hidden_state[:, 0, :]\n\n        combined = torch.cat((text_cls, vision_cls), dim=1)\n        logits = self.classifier(combined)\n        return logits\n\nmodel = MultimodalClassifier(\"google/muril-base-cased\", \"google/vit-base-patch16-224\").to(device)\n\n# ‚úÖ 9Ô∏è‚É£ LOSS & OPTIMIZER\nclass_weights = train_df['Label_Sentiment'].value_counts().sort_index().tolist()\ntotal = sum(class_weights)\nweights = [total / c for c in class_weights]\ncriterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(weights).to(device))\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# ‚úÖ üîü TRAINING LOOP\nnum_epochs = 20\npatience = 3\npatience_counter = 0\nbest_val_loss = float('inf')\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_train_loss = 0\n    for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        pixel_values = batch['pixel_values'].to(device)\n        labels = batch['label'].to(device)\n\n        optimizer.zero_grad()\n        logits = model(input_ids, attention_mask, pixel_values)\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n\n    avg_train_loss = total_train_loss / len(train_loader)\n\n    model.eval()\n    total_val_loss = 0\n    val_predictions = []\n    val_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            pixel_values = batch['pixel_values'].to(device)\n            labels = batch['label'].to(device)\n\n            logits = model(input_ids, attention_mask, pixel_values)\n            loss = criterion(logits, labels)\n\n            total_val_loss += loss.item()\n            predictions = torch.argmax(logits, dim=1)\n            val_predictions.extend(predictions.cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = accuracy_score(val_labels, val_predictions)\n    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), \"best_multimodal_model.pt\")\n        print(\"‚úÖ Validation improved, model saved.\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"üõë Early stopping triggered at epoch {epoch+1}\")\n            break\n\n# ‚úÖ 1Ô∏è‚É£1Ô∏è‚É£ TEST EVALUATION\nprint(\"\\nüîç Loading best model for final test evaluation...\")\nmodel.load_state_dict(torch.load(\"best_multimodal_model.pt\"))\nmodel.eval()\n\ntest_predictions = []\ntest_labels = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Final Test Evaluation\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        pixel_values = batch['pixel_values'].to(device)\n        labels = batch['label'].to(device)\n\n        logits = model(input_ids, attention_mask, pixel_values)\n        predictions = torch.argmax(logits, dim=1)\n        test_predictions.extend(predictions.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\nacc = accuracy_score(test_labels, test_predictions)\nprecision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='weighted')\ncm = confusion_matrix(test_labels, test_predictions)\n\nprint(f\"Test Accuracy: {acc:.4f}\")\nprint(f\"Test Precision (Weighted): {precision:.4f}\")\nprint(f\"Test Recall (Weighted): {recall:.4f}\")\nprint(f\"Test F1-Score (Weighted): {f1:.4f}\")\nprint(f\"Confusion Matrix:\\n{cm}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-08T05:55:56.624027Z","iopub.execute_input":"2025-07-08T05:55:56.624786Z","iopub.status.idle":"2025-07-08T06:16:37.227462Z","shell.execute_reply.started":"2025-07-08T05:55:56.624739Z","shell.execute_reply":"2025-07-08T06:16:37.226659Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTrain Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:36<00:00,  1.82it/s]\nValidation Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/20] Train Loss: 0.9282 | Val Loss: 0.8530 | Val Acc: 0.6186\n‚úÖ Validation improved, model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:36<00:00,  1.83it/s]\nValidation Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/20] Train Loss: 0.6395 | Val Loss: 0.8225 | Val Acc: 0.6696\n‚úÖ Validation improved, model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:37<00:00,  1.81it/s]\nValidation Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/20] Train Loss: 0.4054 | Val Loss: 0.8342 | Val Acc: 0.6763\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:35<00:00,  1.83it/s]\nValidation Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:19<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/20] Train Loss: 0.2520 | Val Loss: 0.8964 | Val Acc: 0.7118\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:37<00:00,  1.82it/s]\nValidation Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:18<00:00,  3.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/20] Train Loss: 0.1765 | Val Loss: 0.9803 | Val Acc: 0.7095\nüõë Early stopping triggered at epoch 5\n\nüîç Loading best model for final test evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Final Test Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 113/113 [00:46<00:00,  2.46it/s]","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.6929\nTest Precision (Weighted): 0.7140\nTest Recall (Weighted): 0.6929\nTest F1-Score (Weighted): 0.6984\nConfusion Matrix:\n[[276  85  41]\n [ 45 254  54]\n [ 10  42  95]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2}]}
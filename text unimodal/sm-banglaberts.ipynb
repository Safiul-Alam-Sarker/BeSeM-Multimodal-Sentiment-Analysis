{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12355137,"sourceType":"datasetVersion","datasetId":7789382}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================\n# ‚úÖ 1Ô∏è‚É£ LIBRARIES & SETUP\n# ================================================\nimport os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nimport re\nimport string\n\n# ================================================\n# ‚úÖ 2Ô∏è‚É£ PATHS\n# ================================================\nimage_dir = \"/kaggle/input/basem/images\"\ninput_csv = \"/kaggle/input/basem/dataset.csv\"\n\n# ================================================\n# ‚úÖ 3Ô∏è‚É£ LOAD & PREPROCESS CSV\n# ================================================\ndf = pd.read_csv(input_csv)\n\nexisting_data = []\nfor _, row in df.iterrows():\n    image_filename = row['image_path']\n    full_image_path = os.path.join(image_dir, image_filename)\n    if os.path.exists(full_image_path):\n        label_converted = row['label 2'] - 1\n        existing_data.append({\n            'Image_path': full_image_path,\n            'Captions': row['extracted_text'],\n            'Label_Sentiment': label_converted\n        })\n\nprocessed_df = pd.DataFrame(existing_data)\n\n# ================================================\n# ‚úÖ 4Ô∏è‚É£ TEXT CLEANING\n# ================================================\ndef clean_text(text):\n    if pd.isna(text): return \"\"\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub(r'<.*?>', '', text)\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = \" \".join(text.split())\n    return text\n\ntrain_df, temp_df = train_test_split(processed_df, test_size=0.3, stratify=processed_df['Label_Sentiment'], random_state=42)\ntest_df, val_df = train_test_split(temp_df, test_size=1/3, stratify=temp_df['Label_Sentiment'], random_state=42)\n\nfor df_name, df_ in [('train', train_df), ('test', test_df), ('val', val_df)]:\n    df_['Captions'] = df_['Captions'].astype(str).apply(clean_text)\n    df_['label'] = df_['Label_Sentiment']\n    df_.to_csv(f'/kaggle/working/{df_name}_cleaned.csv', index=False)\n\n# ================================================\n# ‚úÖ 5Ô∏è‚É£ LOAD BANGLA BERT MODEL\n# ================================================\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbert_tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\nbert_model = AutoModel.from_pretrained(\"sagorsarker/bangla-bert-base\").to(device)\n\n# ================================================\n# ‚úÖ 6Ô∏è‚É£ TEXT-ONLY DATASET\n# ================================================\nclass TextOnlyDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length=128):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        caption = row['Captions']\n        label = row['label']\n        \n        # Tokenize text\n        inputs = self.tokenizer(\n            caption,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# ================================================\n# ‚úÖ 7Ô∏è‚É£ DATALOADERS\n# ================================================\nbatch_size = 16\n\ntrain_dataset = TextOnlyDataset(train_df, bert_tokenizer)\nval_dataset = TextOnlyDataset(val_df, bert_tokenizer)\ntest_dataset = TextOnlyDataset(test_df, bert_tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# ================================================\n# ‚úÖ 8Ô∏è‚É£ TEXT-ONLY CLASSIFICATION MODEL\n# ================================================\nclass TextOnlyClassifier(torch.nn.Module):\n    def __init__(self, bert_model, num_classes=3, dropout_rate=0.3):\n        super().__init__()\n        self.bert = bert_model\n        self.dropout = torch.nn.Dropout(dropout_rate)\n        self.classifier = torch.nn.Linear(bert_model.config.hidden_size, num_classes)\n        \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        # Use [CLS] token representation\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        return logits\n\n# ================================================\n# ‚úÖ 9Ô∏è‚É£ INITIALIZE MODEL\n# ================================================\nmodel = TextOnlyClassifier(bert_model, num_classes=3).to(device)\n\n# ================================================\n# ‚úÖ üîü LOSS & OPTIMIZER\n# ================================================\nclass_weights = train_df['label'].value_counts().sort_index().tolist()\ntotal = sum(class_weights)\nweights = [total / c for c in class_weights]\ncriterion = torch.nn.CrossEntropyLoss(weight=torch.FloatTensor(weights).to(device))\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£1Ô∏è‚É£ TRAINING LOOP\n# ================================================\nnum_epochs = 10\npatience = 3\npatience_counter = 0\nbest_val_loss = float('inf')\n\nprint(\"üöÄ Starting Text-Only Training with BanglishBERT...\")\nprint(f\"Training samples: {len(train_df)}\")\nprint(f\"Validation samples: {len(val_df)}\")\nprint(f\"Test samples: {len(test_df)}\")\nprint(f\"Class distribution: {train_df['label'].value_counts().sort_index().tolist()}\")\n\nfor epoch in range(num_epochs):\n    # ============================================================\n    # TRAINING PHASE\n    # ============================================================\n    model.train()\n    total_train_loss = 0\n    train_predictions = []\n    train_labels = []\n\n    for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        \n        optimizer.zero_grad()\n        \n        logits = model(input_ids, attention_mask)\n        loss = criterion(logits, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_train_loss += loss.item()\n        \n        # Store predictions for metrics\n        predictions = torch.argmax(logits, dim=1)\n        train_predictions.extend(predictions.cpu().numpy())\n        train_labels.extend(labels.cpu().numpy())\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_accuracy = accuracy_score(train_labels, train_predictions)\n\n    # ============================================================\n    # VALIDATION PHASE\n    # ============================================================\n    model.eval()\n    total_val_loss = 0\n    val_predictions = []\n    val_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            \n            logits = model(input_ids, attention_mask)\n            loss = criterion(logits, labels)\n            \n            total_val_loss += loss.item()\n            \n            # Store predictions for metrics\n            predictions = torch.argmax(logits, dim=1)\n            val_predictions.extend(predictions.cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = accuracy_score(val_labels, val_predictions)\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f}\")\n    print(f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n\n    # ============================================================\n    # EARLY STOPPING CHECK\n    # ============================================================\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), \"best_text_only_model.pt\")\n        print(\"‚úÖ Validation loss improved ‚Äî model saved.\")\n    else:\n        patience_counter += 1\n        print(f\"‚è∞ No improvement ‚Äî patience {patience_counter}/{patience}\")\n\n        if patience_counter >= patience:\n            print(f\"üõë Early stopping triggered at epoch {epoch+1}\")\n            break\n    print(\"-\" * 60)\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£2Ô∏è‚É£ FINAL TEST EVALUATION\n# ================================================\nprint(\"\\nüîç Loading best model for final evaluation...\")\nmodel.load_state_dict(torch.load(\"best_text_only_model.pt\"))\nmodel.eval()\n\ntest_predictions = []\ntest_labels = []\ntotal_test_loss = 0\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Final Test Evaluation\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        \n        logits = model(input_ids, attention_mask)\n        loss = criterion(logits, labels)\n        \n        total_test_loss += loss.item()\n        predictions = torch.argmax(logits, dim=1)\n        test_predictions.extend(predictions.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\n# Calculate final metrics\ntest_accuracy = accuracy_score(test_labels, test_predictions)\nprecision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='weighted')\ncm = confusion_matrix(test_labels, test_predictions)\n\n# Calculate per-class metrics\nprecision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n    test_labels, test_predictions, average=None\n)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üìä FINAL TEXT-ONLY TEST RESULTS (BanglishBERT)\")\nprint(\"=\"*60)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Precision (weighted): {precision:.4f}\")\nprint(f\"Test Recall (weighted): {recall:.4f}\")\nprint(f\"Test F1-Score (weighted): {f1:.4f}\")\nprint(f\"Test Loss: {total_test_loss/len(test_loader):.4f}\")\n\nprint(\"\\nüìà Per-Class Metrics:\")\nclass_names = ['Negative', 'Neutral', 'Positive']\nfor i, class_name in enumerate(class_names):\n    print(f\"{class_name:>8}: Precision={precision_per_class[i]:.4f}, Recall={recall_per_class[i]:.4f}, F1={f1_per_class[i]:.4f}, Support={support[i]}\")\n\nprint(f\"\\nüéØ Confusion Matrix:\")\nprint(f\"{'':>10} {'Neg':>6} {'Neu':>6} {'Pos':>6}\")\nfor i, class_name in enumerate(['Negative', 'Neutral', 'Positive']):\n    print(f\"{class_name:>10} {cm[i][0]:>6} {cm[i][1]:>6} {cm[i][2]:>6}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üí° Compare these results with your multimodal approach!\")\nprint(\"=\"*60)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T07:25:10.332437Z","iopub.execute_input":"2025-07-07T07:25:10.332712Z","iopub.status.idle":"2025-07-07T07:29:38.964516Z","shell.execute_reply.started":"2025-07-07T07:25:10.332688Z","shell.execute_reply":"2025-07-07T07:29:38.963721Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/491 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96f942c905144ffaa491296dd3e6fbef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d74ea90b31b4065a7b710ae5fb21679"}},"metadata":{}},{"name":"stderr","text":"2025-07-07 07:25:34.218581: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751873134.416235      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751873134.471254      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/660M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f629c8cdda6433a96a4ddb8aa08fb8a"}},"metadata":{}},{"name":"stdout","text":"üöÄ Starting Text-Only Training with BanglishBERT...\nTraining samples: 3156\nValidation samples: 451\nTest samples: 902\nClass distribution: [1404, 1237, 515]\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:43<00:00,  4.57it/s]\nValidation Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:01<00:00, 17.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10]\nTrain Loss: 0.9589 | Train Acc: 0.5627\nVal Loss: 1.0125 | Val Acc: 0.5455\n‚úÖ Validation loss improved ‚Äî model saved.\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:42<00:00,  4.64it/s]\nValidation Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:01<00:00, 17.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10]\nTrain Loss: 0.6976 | Train Acc: 0.7066\nVal Loss: 0.8771 | Val Acc: 0.6541\n‚úÖ Validation loss improved ‚Äî model saved.\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:42<00:00,  4.63it/s]\nValidation Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:01<00:00, 16.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10]\nTrain Loss: 0.4348 | Train Acc: 0.8241\nVal Loss: 0.9990 | Val Acc: 0.6563\n‚è∞ No improvement ‚Äî patience 1/3\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:43<00:00,  4.60it/s]\nValidation Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:01<00:00, 16.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10]\nTrain Loss: 0.2213 | Train Acc: 0.9233\nVal Loss: 1.3176 | Val Acc: 0.6497\n‚è∞ No improvement ‚Äî patience 2/3\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:43<00:00,  4.60it/s]\nValidation Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:01<00:00, 16.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10]\nTrain Loss: 0.1203 | Train Acc: 0.9566\nVal Loss: 1.5114 | Val Acc: 0.6829\n‚è∞ No improvement ‚Äî patience 3/3\nüõë Early stopping triggered at epoch 5\n\nüîç Loading best model for final evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Final Test Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:03<00:00, 16.64it/s]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüìä FINAL TEXT-ONLY TEST RESULTS (BanglishBERT)\n============================================================\nTest Accuracy: 0.6918\nTest Precision (weighted): 0.7048\nTest Recall (weighted): 0.6918\nTest F1-Score (weighted): 0.6953\nTest Loss: 0.7679\n\nüìà Per-Class Metrics:\nNegative: Precision=0.7710, Recall=0.7537, F1=0.7623, Support=402\n Neutral: Precision=0.7179, Recall=0.6346, F1=0.6737, Support=353\nPositive: Precision=0.4924, Recall=0.6599, F1=0.5640, Support=147\n\nüéØ Confusion Matrix:\n              Neg    Neu    Pos\n  Negative    303     52     47\n   Neutral     76    224     53\n  Positive     14     36     97\n\n============================================================\nüí° Compare these results with your multimodal approach!\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1}]}
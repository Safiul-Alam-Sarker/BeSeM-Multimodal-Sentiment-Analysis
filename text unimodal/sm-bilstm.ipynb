{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12355137,"sourceType":"datasetVersion","datasetId":7789382}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================\n# ‚úÖ 1Ô∏è‚É£ LIBRARIES & SETUP\n# ================================================\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nimport re\nimport string\n\n# ================================================\n# ‚úÖ 2Ô∏è‚É£ PATHS\n# ================================================\ninput_csv = \"/kaggle/input/basem/dataset.csv\"\n\n# ================================================\n# ‚úÖ 3Ô∏è‚É£ LOAD & PREPROCESS CSV\n# ================================================\ndf = pd.read_csv(input_csv)\n\nexisting_data = []\nfor _, row in df.iterrows():\n    label_converted = row['label 2'] - 1\n    existing_data.append({\n        'Captions': row['extracted_text'],\n        'Label_Sentiment': label_converted\n    })\n\nprocessed_df = pd.DataFrame(existing_data)\n\n# ================================================\n# ‚úÖ 4Ô∏è‚É£ TEXT CLEANING\n# ================================================\ndef clean_text(text):\n    if pd.isna(text): return \"\"\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub(r'<.*?>', '', text)\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = \" \".join(text.split())\n    return text\n\ntrain_df, temp_df = train_test_split(processed_df, test_size=0.3, stratify=processed_df['Label_Sentiment'], random_state=42)\ntest_df, val_df = train_test_split(temp_df, test_size=1/3, stratify=temp_df['Label_Sentiment'], random_state=42)\n\nfor df_name, df_ in [('train', train_df), ('test', test_df), ('val', val_df)]:\n    df_['Captions'] = df_['Captions'].astype(str).apply(clean_text)\n    df_['label'] = df_['Label_Sentiment']\n    df_.to_csv(f'/kaggle/working/{df_name}_cleaned.csv', index=False)\n\n# ================================================\n# ‚úÖ 5Ô∏è‚É£ LOAD BERT MODEL & TOKENIZER\n# ================================================\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbert_tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\nbert_model = AutoModel.from_pretrained(\"sagorsarker/bangla-bert-base\").to(device)\n\n# ================================================\n# ‚úÖ 6Ô∏è‚É£ TEXT-ONLY DATASET\n# ================================================\nclass TextOnlyDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length=128):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        caption = str(row['Captions'])\n        label = row['label']\n        \n        # Tokenize text\n        inputs = self.tokenizer(\n            caption,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# ================================================\n# ‚úÖ 7Ô∏è‚É£ DATALOADERS\n# ================================================\nbatch_size = 16\n\ntrain_dataset = TextOnlyDataset(train_df, bert_tokenizer)\nval_dataset = TextOnlyDataset(val_df, bert_tokenizer)\ntest_dataset = TextOnlyDataset(test_df, bert_tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# ================================================\n# ‚úÖ 8Ô∏è‚É£ BiLSTM MODEL\n# ================================================\nclass BiLSTMSentimentClassifier(nn.Module):\n    def __init__(self, bert_model, hidden_dim=256, num_layers=2, dropout=0.3, num_classes=3):\n        super(BiLSTMSentimentClassifier, self).__init__()\n        \n        self.bert = bert_model\n        self.bert_hidden_size = bert_model.config.hidden_size\n        \n        # Freeze BERT parameters (optional - you can unfreeze for fine-tuning)\n        for param in self.bert.parameters():\n            param.requires_grad = False\n        \n        # BiLSTM layer\n        self.bilstm = nn.LSTM(\n            input_size=self.bert_hidden_size,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            dropout=dropout if num_layers > 1 else 0,\n            batch_first=True,\n            bidirectional=True\n        )\n        \n        # Attention mechanism\n        self.attention = nn.Linear(hidden_dim * 2, 1)\n        \n        # Classifier\n        self.dropout = nn.Dropout(dropout)\n        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n        \n    def forward(self, input_ids, attention_mask):\n        # Get BERT embeddings\n        with torch.no_grad():\n            bert_outputs = self.bert(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n        \n        # Get sequence embeddings\n        sequence_output = bert_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n        \n        # Pass through BiLSTM\n        lstm_output, _ = self.bilstm(sequence_output)  # (batch_size, seq_len, hidden_dim * 2)\n        \n        # Apply attention mechanism\n        attention_weights = torch.softmax(self.attention(lstm_output), dim=1)  # (batch_size, seq_len, 1)\n        \n        # Apply attention mask to prevent attending to padding tokens\n        attention_mask_expanded = attention_mask.unsqueeze(-1).float()  # (batch_size, seq_len, 1)\n        attention_weights = attention_weights * attention_mask_expanded\n        \n        # Normalize attention weights\n        attention_weights = attention_weights / (attention_weights.sum(dim=1, keepdim=True) + 1e-10)\n        \n        # Apply attention to get final representation\n        attended_output = torch.sum(lstm_output * attention_weights, dim=1)  # (batch_size, hidden_dim * 2)\n        \n        # Apply dropout and classify\n        output = self.dropout(attended_output)\n        logits = self.classifier(output)\n        \n        return logits\n\n# ================================================\n# ‚úÖ 9Ô∏è‚É£ INITIALIZE MODEL\n# ================================================\nmodel = BiLSTMSentimentClassifier(bert_model, hidden_dim=256, num_layers=2, dropout=0.3, num_classes=3).to(device)\n\n# ================================================\n# ‚úÖ üîü LOSS & OPTIMIZER\n# ================================================\nclass_weights = train_df['label'].value_counts().sort_index().tolist()\ntotal = sum(class_weights)\nweights = [total / c for c in class_weights]\ncriterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(weights).to(device))\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£1Ô∏è‚É£ TRAINING LOOP\n# ================================================\nnum_epochs = 15\npatience = 5\npatience_counter = 0\nbest_val_loss = float('inf')\n\nfor epoch in range(num_epochs):\n    # ============================================================\n    # TRAINING PHASE\n    # ============================================================\n    model.train()\n    total_train_loss = 0\n\n    for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        # Forward pass\n        logits = model(input_ids, attention_mask)\n        loss = criterion(logits, labels)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        total_train_loss += loss.item()\n\n    avg_train_loss = total_train_loss / len(train_loader)\n\n    # ============================================================\n    # VALIDATION PHASE\n    # ============================================================\n    model.eval()\n    total_val_loss = 0\n    val_predictions = []\n    val_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            # Forward pass\n            logits = model(input_ids, attention_mask)\n            loss = criterion(logits, labels)\n\n            total_val_loss += loss.item()\n            \n            # Store predictions for metrics\n            predictions = torch.argmax(logits, dim=1)\n            val_predictions.extend(predictions.cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = accuracy_score(val_labels, val_predictions)\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n\n    # ============================================================\n    # EARLY STOPPING CHECK\n    # ============================================================\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), \"best_text_model.pt\")\n        print(\"‚úÖ Validation loss improved ‚Äî model saved.\")\n    else:\n        patience_counter += 1\n        print(f\"‚è∞ No improvement ‚Äî patience {patience_counter}/{patience}\")\n\n        if patience_counter >= patience:\n            print(f\"üõë Early stopping triggered at epoch {epoch+1}\")\n            break\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£2Ô∏è‚É£ FINAL TEST EVALUATION\n# ================================================\nprint(\"\\nüîç Loading best model for final evaluation...\")\nmodel.load_state_dict(torch.load(\"best_text_model.pt\"))\nmodel.eval()\n\ntest_predictions = []\ntest_labels = []\ntotal_test_loss = 0\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Final Test Evaluation\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        # Forward pass\n        logits = model(input_ids, attention_mask)\n        loss = criterion(logits, labels)\n        \n        total_test_loss += loss.item()\n        predictions = torch.argmax(logits, dim=1)\n        test_predictions.extend(predictions.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\n# Calculate final metrics\ntest_accuracy = accuracy_score(test_labels, test_predictions)\nprecision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='weighted')\ncm = confusion_matrix(test_labels, test_predictions)\n\nprint(\"\\nüìä FINAL TEST RESULTS (Text-Only BiLSTM):\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Precision: {precision:.4f}\")\nprint(f\"Test Recall: {recall:.4f}\")\nprint(f\"Test F1-Score: {f1:.4f}\")\nprint(f\"Test Loss: {total_test_loss/len(test_loader):.4f}\")\nprint(f\"\\nConfusion Matrix:\\n{cm}\")\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£3Ô∏è‚É£ DETAILED CLASSIFICATION REPORT\n# ================================================\nfrom sklearn.metrics import classification_report\n\nprint(\"\\nüìà DETAILED CLASSIFICATION REPORT:\")\ntarget_names = ['Negative', 'Neutral', 'Positive']  # Adjust based on your labels\nprint(classification_report(test_labels, test_predictions, target_names=target_names))\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£4Ô∏è‚É£ SAVE PREDICTIONS\n# ================================================\nresults_df = pd.DataFrame({\n    'true_labels': test_labels,\n    'predicted_labels': test_predictions\n})\nresults_df.to_csv('/kaggle/working/text_bilstm_predictions.csv', index=False)\nprint(\"\\nüíæ Predictions saved to 'text_bilstm_predictions.csv'\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T07:37:02.509180Z","iopub.execute_input":"2025-07-07T07:37:02.509690Z","iopub.status.idle":"2025-07-07T07:41:46.745089Z","shell.execute_reply.started":"2025-07-07T07:37:02.509665Z","shell.execute_reply":"2025-07-07T07:41:46.744319Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/491 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c55a1d3687a41aca9771c09b0cb4d1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cef201a7f0248879596eb7ac350fdab"}},"metadata":{}},{"name":"stderr","text":"2025-07-07 07:37:20.400067: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751873840.578360      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751873840.629843      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/660M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4b93dfd87344c29b05de86fa26a2f9b"}},"metadata":{}},{"name":"stderr","text":"Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:18<00:00, 10.82it/s]\nValidation Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:02<00:00, 14.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/15] Train Loss: 1.0711 | Val Loss: 1.0376 | Val Acc: 0.5743\n‚úÖ Validation loss improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:17<00:00, 11.25it/s]\nValidation Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:01<00:00, 14.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/15] Train Loss: 0.9802 | Val Loss: 0.9545 | Val Acc: 0.5322\n‚úÖ Validation loss improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:17<00:00, 11.25it/s]\nValidation Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:02<00:00, 14.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/15] Train Loss: 0.8947 | Val Loss: 0.9116 | Val Acc: 0.5854\n‚úÖ Validation loss improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:17<00:00, 11.24it/s]\nValidation Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:01<00:00, 14.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/15] Train Loss: 0.8529 | Val Loss: 0.9051 | Val Acc: 0.6164\n‚úÖ Validation loss improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:17<00:00, 11.25it/s]\nValidation Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:02<00:00, 14.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/15] Train Loss: 0.8152 | Val Loss: 0.8769 | Val Acc: 0.6142\n‚úÖ Validation loss improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:17<00:00, 11.25it/s]\nValidation Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:02<00:00, 14.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/15] Train Loss: 0.7829 | Val Loss: 0.8729 | Val Acc: 0.6408\n‚úÖ Validation loss improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:17<00:00, 11.24it/s]\nValidation Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:02<00:00, 14.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/15] Train Loss: 0.7450 | Val Loss: 0.8658 | Val Acc: 0.6231\n‚úÖ Validation loss improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:17<00:00, 11.24it/s]\nValidation Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:02<00:00, 14.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/15] Train Loss: 0.7156 | Val Loss: 0.8764 | Val Acc: 0.6341\n‚è∞ No improvement ‚Äî patience 1/5\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:17<00:00, 11.32it/s]\nValidation Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:01<00:00, 14.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/15] Train Loss: 0.6813 | Val Loss: 0.8825 | Val Acc: 0.6031\n‚è∞ No improvement ‚Äî patience 2/5\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:17<00:00, 11.32it/s]\nValidation Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:02<00:00, 14.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/15] Train Loss: 0.6476 | Val Loss: 0.9079 | Val Acc: 0.6608\n‚è∞ No improvement ‚Äî patience 3/5\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:17<00:00, 11.21it/s]\nValidation Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:02<00:00, 14.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/15] Train Loss: 0.6103 | Val Loss: 0.9158 | Val Acc: 0.6475\n‚è∞ No improvement ‚Äî patience 4/5\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:17<00:00, 11.18it/s]\nValidation Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:02<00:00, 14.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [12/15] Train Loss: 0.5543 | Val Loss: 0.9439 | Val Acc: 0.6142\n‚è∞ No improvement ‚Äî patience 5/5\nüõë Early stopping triggered at epoch 12\n\nüîç Loading best model for final evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Final Test Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:04<00:00, 14.18it/s]","output_type":"stream"},{"name":"stdout","text":"\nüìä FINAL TEST RESULTS (Text-Only BiLSTM):\nTest Accuracy: 0.6353\nTest Precision: 0.6591\nTest Recall: 0.6353\nTest F1-Score: 0.6422\nTest Loss: 0.8428\n\nConfusion Matrix:\n[[272  78  52]\n [ 69 208  76]\n [ 18  36  93]]\n\nüìà DETAILED CLASSIFICATION REPORT:\n              precision    recall  f1-score   support\n\n    Negative       0.76      0.68      0.71       402\n     Neutral       0.65      0.59      0.62       353\n    Positive       0.42      0.63      0.51       147\n\n    accuracy                           0.64       902\n   macro avg       0.61      0.63      0.61       902\nweighted avg       0.66      0.64      0.64       902\n\n\nüíæ Predictions saved to 'text_bilstm_predictions.csv'\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1}]}
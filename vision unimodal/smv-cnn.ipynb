{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f648d664",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-07T08:30:25.235508Z",
     "iopub.status.busy": "2025-07-07T08:30:25.235035Z",
     "iopub.status.idle": "2025-07-07T08:57:40.286311Z",
     "shell.execute_reply": "2025-07-07T08:57:40.285164Z"
    },
    "papermill": {
     "duration": 1635.056128,
     "end_time": "2025-07-07T08:57:40.287543",
     "exception": false,
     "start_time": "2025-07-07T08:30:25.231415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total trainable parameters: 4,854,595\n",
      "Class distribution: [1404, 1237, 515]\n",
      "Class weights: [2.247863247863248, 2.551333872271625, 6.128155339805825]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|██████████| 198/198 [02:15<00:00,  1.46it/s]\n",
      "Validation Epoch 1: 100%|██████████| 29/29 [00:15<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "Train Loss: 1.1560 | Train Acc: 0.3767\n",
      "Val Loss: 1.0546 | Val Acc: 0.4213\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n",
      "✅ Validation loss improved — model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|██████████| 198/198 [01:48<00:00,  1.83it/s]\n",
      "Validation Epoch 2: 100%|██████████| 29/29 [00:11<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20]\n",
      "Train Loss: 1.1456 | Train Acc: 0.3894\n",
      "Val Loss: 1.0285 | Val Acc: 0.4723\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n",
      "✅ Validation loss improved — model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|██████████| 198/198 [01:50<00:00,  1.79it/s]\n",
      "Validation Epoch 3: 100%|██████████| 29/29 [00:12<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20]\n",
      "Train Loss: 1.1292 | Train Acc: 0.4030\n",
      "Val Loss: 1.0145 | Val Acc: 0.4989\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n",
      "✅ Validation loss improved — model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|██████████| 198/198 [01:49<00:00,  1.81it/s]\n",
      "Validation Epoch 4: 100%|██████████| 29/29 [00:11<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20]\n",
      "Train Loss: 1.1159 | Train Acc: 0.4163\n",
      "Val Loss: 1.0101 | Val Acc: 0.5543\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n",
      "✅ Validation loss improved — model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|██████████| 198/198 [01:48<00:00,  1.82it/s]\n",
      "Validation Epoch 5: 100%|██████████| 29/29 [00:11<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20]\n",
      "Train Loss: 1.1099 | Train Acc: 0.4110\n",
      "Val Loss: 1.0166 | Val Acc: 0.5565\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n",
      "⏰ No improvement — patience 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6: 100%|██████████| 198/198 [01:47<00:00,  1.85it/s]\n",
      "Validation Epoch 6: 100%|██████████| 29/29 [00:11<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20]\n",
      "Train Loss: 1.0973 | Train Acc: 0.4281\n",
      "Val Loss: 0.9967 | Val Acc: 0.5698\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n",
      "✅ Validation loss improved — model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7: 100%|██████████| 198/198 [01:45<00:00,  1.88it/s]\n",
      "Validation Epoch 7: 100%|██████████| 29/29 [00:11<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20]\n",
      "Train Loss: 1.0895 | Train Acc: 0.4344\n",
      "Val Loss: 1.0105 | Val Acc: 0.5188\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n",
      "⏰ No improvement — patience 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8: 100%|██████████| 198/198 [01:45<00:00,  1.88it/s]\n",
      "Validation Epoch 8: 100%|██████████| 29/29 [00:11<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20]\n",
      "Train Loss: 1.0652 | Train Acc: 0.4392\n",
      "Val Loss: 0.9823 | Val Acc: 0.5277\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n",
      "✅ Validation loss improved — model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 9: 100%|██████████| 198/198 [01:45<00:00,  1.88it/s]\n",
      "Validation Epoch 9: 100%|██████████| 29/29 [00:11<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20]\n",
      "Train Loss: 1.0806 | Train Acc: 0.4496\n",
      "Val Loss: 0.9886 | Val Acc: 0.5166\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n",
      "⏰ No improvement — patience 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 10: 100%|██████████| 198/198 [01:47<00:00,  1.84it/s]\n",
      "Validation Epoch 10: 100%|██████████| 29/29 [00:11<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20]\n",
      "Train Loss: 1.0592 | Train Acc: 0.4728\n",
      "Val Loss: 0.9652 | Val Acc: 0.5676\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n",
      "✅ Validation loss improved — model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 11: 100%|██████████| 198/198 [01:45<00:00,  1.87it/s]\n",
      "Validation Epoch 11: 100%|██████████| 29/29 [00:11<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20]\n",
      "Train Loss: 1.0690 | Train Acc: 0.4617\n",
      "Val Loss: 0.9891 | Val Acc: 0.5211\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n",
      "⏰ No improvement — patience 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 12: 100%|██████████| 198/198 [01:45<00:00,  1.88it/s]\n",
      "Validation Epoch 12: 100%|██████████| 29/29 [00:11<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20]\n",
      "Train Loss: 1.0419 | Train Acc: 0.4908\n",
      "Val Loss: 0.9672 | Val Acc: 0.5211\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n",
      "⏰ No improvement — patience 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 13: 100%|██████████| 198/198 [01:46<00:00,  1.86it/s]\n",
      "Validation Epoch 13: 100%|██████████| 29/29 [00:11<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20]\n",
      "Train Loss: 1.0432 | Train Acc: 0.4810\n",
      "Val Loss: 0.9704 | Val Acc: 0.5188\n",
      "Learning Rate: 0.000050\n",
      "--------------------------------------------------\n",
      "⏰ No improvement — patience 3/3\n",
      "🛑 Early stopping triggered at epoch 13\n",
      "\n",
      "🔍 Loading best model for final evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Test Evaluation: 100%|██████████| 57/57 [00:31<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📊 FINAL TEST RESULTS (VISION-ONLY CNN):\n",
      "============================================================\n",
      "Test Accuracy: 0.5687\n",
      "Test Precision: 0.5766\n",
      "Test Recall: 0.5687\n",
      "Test F1-Score: 0.5695\n",
      "Test Loss: 0.9778\n",
      "\n",
      "Confusion Matrix:\n",
      "[[230 138  34]\n",
      " [ 90 220  43]\n",
      " [ 26  58  63]]\n",
      "\n",
      "📋 DETAILED CLASSIFICATION REPORT:\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.66      0.57      0.61       402\n",
      "     Neutral       0.53      0.62      0.57       353\n",
      "    Positive       0.45      0.43      0.44       147\n",
      "\n",
      "    accuracy                           0.57       902\n",
      "   macro avg       0.55      0.54      0.54       902\n",
      "weighted avg       0.58      0.57      0.57       902\n",
      "\n",
      "\n",
      "📈 TRAINING HISTORY SUMMARY:\n",
      "============================================================\n",
      "Best Validation Loss: 0.9652\n",
      "Best Validation Accuracy: 0.5698\n",
      "Final Training Loss: 1.0432\n",
      "Final Validation Loss: 0.9704\n",
      "Final Validation Accuracy: 0.5188\n",
      "\n",
      "✅ Vision-only CNN training completed successfully!\n",
      "📁 Model saved as 'best_vision_model.pt'\n",
      "📁 Training history saved as 'vision_training_history.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# ✅ 1️⃣ LIBRARIES & SETUP\n",
    "# ================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "# ================================================\n",
    "# ✅ 2️⃣ PATHS\n",
    "# ================================================\n",
    "image_dir = \"/kaggle/input/basem/images\"\n",
    "input_csv = \"/kaggle/input/basem/dataset.csv\"\n",
    "\n",
    "# ================================================\n",
    "# ✅ 3️⃣ LOAD & PREPROCESS CSV\n",
    "# ================================================\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "existing_data = []\n",
    "for _, row in df.iterrows():\n",
    "    image_filename = row['image_path']\n",
    "    full_image_path = os.path.join(image_dir, image_filename)\n",
    "    if os.path.exists(full_image_path):\n",
    "        label_converted = row['label 2'] - 1\n",
    "        existing_data.append({\n",
    "            'Image_path': full_image_path,\n",
    "            'Label_Sentiment': label_converted\n",
    "        })\n",
    "\n",
    "processed_df = pd.DataFrame(existing_data)\n",
    "\n",
    "# ================================================\n",
    "# ✅ 4️⃣ DATA SPLITTING\n",
    "# ================================================\n",
    "train_df, temp_df = train_test_split(processed_df, test_size=0.3, stratify=processed_df['Label_Sentiment'], random_state=42)\n",
    "test_df, val_df = train_test_split(temp_df, test_size=1/3, stratify=temp_df['Label_Sentiment'], random_state=42)\n",
    "\n",
    "# Save splits for reference\n",
    "for df_name, df_ in [('train', train_df), ('test', test_df), ('val', val_df)]:\n",
    "    df_['label'] = df_['Label_Sentiment']\n",
    "    df_.to_csv(f'/kaggle/working/{df_name}_vision_only.csv', index=False)\n",
    "\n",
    "# ================================================\n",
    "# ✅ 5️⃣ DEVICE SETUP\n",
    "# ================================================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ================================================\n",
    "# ✅ 6️⃣ IMAGE TRANSFORMS\n",
    "# ================================================\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ================================================\n",
    "# ✅ 7️⃣ VISION DATASET\n",
    "# ================================================\n",
    "class VisionDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row['Image_path']).convert('RGB')\n",
    "        label = row['label']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# ================================================\n",
    "# ✅ 8️⃣ DATALOADERS\n",
    "# ================================================\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = VisionDataset(train_df, transform=train_transform)\n",
    "val_dataset = VisionDataset(val_df, transform=val_test_transform)\n",
    "test_dataset = VisionDataset(test_df, transform=val_test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ================================================\n",
    "# ✅ 9️⃣ CNN MODEL DEFINITION\n",
    "# ================================================\n",
    "class VisionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(VisionCNN, self).__init__()\n",
    "        \n",
    "        # First Convolutional Block\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout2 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout3 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        # Fourth Convolutional Block\n",
    "        self.conv7 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.conv8 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(512)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout4 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc_bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc_dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc_bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc_dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Second block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Third block\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Fourth block\n",
    "        x = F.relu(self.bn7(self.conv7(x)))\n",
    "        x = F.relu(self.bn8(self.conv8(x)))\n",
    "        x = self.pool4(x)\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc_bn1(self.fc1(x)))\n",
    "        x = self.fc_dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.fc_bn2(self.fc2(x)))\n",
    "        x = self.fc_dropout2(x)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# ================================================\n",
    "# ✅ 🔟 MODEL INITIALIZATION\n",
    "# ================================================\n",
    "model = VisionCNN(num_classes=3).to(device)\n",
    "\n",
    "# Print model summary\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total trainable parameters: {count_parameters(model):,}\")\n",
    "\n",
    "# ================================================\n",
    "# ✅ 1️⃣1️⃣ LOSS & OPTIMIZER\n",
    "# ================================================\n",
    "# Calculate class weights for balanced training\n",
    "class_weights = train_df['label'].value_counts().sort_index().tolist()\n",
    "total = sum(class_weights)\n",
    "weights = [total / c for c in class_weights]\n",
    "print(f\"Class distribution: {class_weights}\")\n",
    "print(f\"Class weights: {weights}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(weights).to(device))\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "\n",
    "# ================================================\n",
    "# ✅ 1️⃣2️⃣ TRAINING LOOP\n",
    "# ================================================\n",
    "num_epochs = 20\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "training_history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_accuracy': []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ============================================================\n",
    "    # TRAINING PHASE\n",
    "    # ============================================================\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_predictions = []\n",
    "    train_labels = []\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Store predictions for training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_predictions.extend(predicted.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "\n",
    "    # ============================================================\n",
    "    # VALIDATION PHASE\n",
    "    # ============================================================\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            # Store predictions for metrics\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_predictions.extend(predicted.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Store history\n",
    "    training_history['train_loss'].append(avg_train_loss)\n",
    "    training_history['val_loss'].append(avg_val_loss)\n",
    "    training_history['val_accuracy'].append(val_accuracy)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # ============================================================\n",
    "    # EARLY STOPPING CHECK\n",
    "    # ============================================================\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_vision_model.pt\")\n",
    "        print(\"✅ Validation loss improved — model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"⏰ No improvement — patience {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"🛑 Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# ================================================\n",
    "# ✅ 1️⃣3️⃣ FINAL TEST EVALUATION\n",
    "# ================================================\n",
    "print(\"\\n🔍 Loading best model for final evaluation...\")\n",
    "model.load_state_dict(torch.load(\"best_vision_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "total_test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Final Test Evaluation\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        total_test_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_predictions.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate final metrics\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='weighted')\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 FINAL TEST RESULTS (VISION-ONLY CNN):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1-Score: {f1:.4f}\")\n",
    "print(f\"Test Loss: {total_test_loss/len(test_loader):.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "\n",
    "# ================================================\n",
    "# ✅ 1️⃣4️⃣ DETAILED CLASSIFICATION REPORT\n",
    "# ================================================\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\n📋 DETAILED CLASSIFICATION REPORT:\")\n",
    "print(\"=\"*60)\n",
    "target_names = ['Negative', 'Neutral', 'Positive']\n",
    "print(classification_report(test_labels, test_predictions, target_names=target_names))\n",
    "\n",
    "# ================================================\n",
    "# ✅ 1️⃣5️⃣ TRAINING HISTORY SUMMARY\n",
    "# ================================================\n",
    "print(\"\\n📈 TRAINING HISTORY SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Validation Loss: {min(training_history['val_loss']):.4f}\")\n",
    "print(f\"Best Validation Accuracy: {max(training_history['val_accuracy']):.4f}\")\n",
    "print(f\"Final Training Loss: {training_history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {training_history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {training_history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Save training history\n",
    "import pickle\n",
    "with open('/kaggle/working/vision_training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(training_history, f)\n",
    "\n",
    "print(\"\\n✅ Vision-only CNN training completed successfully!\")\n",
    "print(\"📁 Model saved as 'best_vision_model.pt'\")\n",
    "print(\"📁 Training history saved as 'vision_training_history.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7789382,
     "sourceId": 12355137,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1641.827731,
   "end_time": "2025-07-07T08:57:43.035247",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-07T08:30:21.207516",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

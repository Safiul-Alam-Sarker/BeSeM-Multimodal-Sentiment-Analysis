{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12355137,"sourceType":"datasetVersion","datasetId":7789382}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================\n# ‚úÖ 1Ô∏è‚É£ LIBRARIES & SETUP\n# ================================================\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nimport torchvision.transforms as transforms\nfrom torchvision import models\n\n# ================================================\n# ‚úÖ 2Ô∏è‚É£ PATHS\n# ================================================\nimage_dir = \"/kaggle/input/basem/images\"\ninput_csv = \"/kaggle/input/basem/dataset.csv\"\n\n# ================================================\n# ‚úÖ 3Ô∏è‚É£ LOAD & PREPROCESS CSV\n# ================================================\ndf = pd.read_csv(input_csv)\n\nexisting_data = []\nfor _, row in df.iterrows():\n    image_filename = row['image_path']\n    full_image_path = os.path.join(image_dir, image_filename)\n    if os.path.exists(full_image_path):\n        label_converted = row['label 2'] - 1\n        existing_data.append({\n            'Image_path': full_image_path,\n            'Label_Sentiment': label_converted\n        })\n\nprocessed_df = pd.DataFrame(existing_data)\n\n# ================================================\n# ‚úÖ 4Ô∏è‚É£ DATA SPLITS\n# ================================================\ntrain_df, temp_df = train_test_split(processed_df, test_size=0.3, stratify=processed_df['Label_Sentiment'], random_state=42)\ntest_df, val_df = train_test_split(temp_df, test_size=1/3, stratify=temp_df['Label_Sentiment'], random_state=42)\n\nfor df_name, df_ in [('train', train_df), ('test', test_df), ('val', val_df)]:\n    df_['label'] = df_['Label_Sentiment']\n    df_.to_csv(f'/kaggle/working/{df_name}_vision_only.csv', index=False)\n\n# ================================================\n# ‚úÖ 5Ô∏è‚É£ DEVICE & TRANSFORMS\n# ================================================\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Data augmentation for training\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# No augmentation for validation/test\nval_test_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# ================================================\n# ‚úÖ 6Ô∏è‚É£ VISION-ONLY DATASET\n# ================================================\nclass VisionOnlyDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(row['Image_path']).convert('RGB')\n        label = row['label']\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# ================================================\n# ‚úÖ 7Ô∏è‚É£ DATALOADERS\n# ================================================\nbatch_size = 16\n\ntrain_dataset = VisionOnlyDataset(train_df, transform=train_transform)\nval_dataset = VisionOnlyDataset(val_df, transform=val_test_transform)\ntest_dataset = VisionOnlyDataset(test_df, transform=val_test_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n# ================================================\n# ‚úÖ 8Ô∏è‚É£ DENSENET161 MODEL\n# ================================================\nclass DenseNet161Classifier(nn.Module):\n    def __init__(self, num_classes=3, pretrained=True):\n        super(DenseNet161Classifier, self).__init__()\n        self.densenet = models.densenet161(pretrained=pretrained)\n        \n        # Replace the classifier layer\n        num_features = self.densenet.classifier.in_features\n        self.densenet.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(num_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n    \n    def forward(self, x):\n        return self.densenet(x)\n\n# ================================================\n# ‚úÖ 9Ô∏è‚É£ MODEL INITIALIZATION\n# ================================================\nmodel = DenseNet161Classifier(num_classes=3, pretrained=True).to(device)\n\n# ================================================\n# ‚úÖ üîü LOSS & OPTIMIZER\n# ================================================\n# Calculate class weights for balanced training\nclass_weights = train_df['label'].value_counts().sort_index().tolist()\ntotal = sum(class_weights)\nweights = [total / c for c in class_weights]\ncriterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(weights).to(device))\n\n# Optimizer with different learning rates for feature extractor and classifier\noptimizer = AdamW([\n    {'params': model.densenet.features.parameters(), 'lr': 1e-5},  # Lower LR for pretrained features\n    {'params': model.densenet.classifier.parameters(), 'lr': 1e-4}  # Higher LR for new classifier\n], weight_decay=1e-4)\n\n# Learning rate scheduler\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£1Ô∏è‚É£ TRAINING LOOP\n# ================================================\nnum_epochs = 25\npatience = 5\npatience_counter = 0\nbest_val_loss = float('inf')\n\nprint(f\"Training on {len(train_dataset)} samples\")\nprint(f\"Validating on {len(val_dataset)} samples\")\nprint(f\"Testing on {len(test_dataset)} samples\")\n\nfor epoch in range(num_epochs):\n    # ============================================================\n    # TRAINING PHASE\n    # ============================================================\n    model.train()\n    total_train_loss = 0\n    train_predictions = []\n    train_labels = []\n\n    for images, labels in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_train_loss += loss.item()\n        \n        # Store predictions for training accuracy\n        predictions = torch.argmax(outputs, dim=1)\n        train_predictions.extend(predictions.cpu().numpy())\n        train_labels.extend(labels.cpu().numpy())\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_accuracy = accuracy_score(train_labels, train_predictions)\n\n    # ============================================================\n    # VALIDATION PHASE\n    # ============================================================\n    model.eval()\n    total_val_loss = 0\n    val_predictions = []\n    val_labels = []\n\n    with torch.no_grad():\n        for images, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            total_val_loss += loss.item()\n            \n            # Store predictions for metrics\n            predictions = torch.argmax(outputs, dim=1)\n            val_predictions.extend(predictions.cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = accuracy_score(val_labels, val_predictions)\n    \n    # Update learning rate\n    scheduler.step()\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f}\")\n    print(f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n    print(\"-\" * 50)\n\n    # ============================================================\n    # EARLY STOPPING CHECK\n    # ============================================================\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), \"best_densenet161_model.pt\")\n        print(\"‚úÖ Validation loss improved ‚Äî model saved.\")\n    else:\n        patience_counter += 1\n        print(f\"‚è∞ No improvement ‚Äî patience {patience_counter}/{patience}\")\n\n        if patience_counter >= patience:\n            print(f\"üõë Early stopping triggered at epoch {epoch+1}\")\n            break\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£2Ô∏è‚É£ FINAL TEST EVALUATION\n# ================================================\nprint(\"\\nüîç Loading best model for final evaluation...\")\nmodel.load_state_dict(torch.load(\"best_densenet161_model.pt\"))\nmodel.eval()\n\ntest_predictions = []\ntest_labels = []\ntotal_test_loss = 0\n\nwith torch.no_grad():\n    for images, labels in tqdm(test_loader, desc=\"Final Test Evaluation\"):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        total_test_loss += loss.item()\n        \n        predictions = torch.argmax(outputs, dim=1)\n        test_predictions.extend(predictions.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\n# Calculate final metrics\ntest_accuracy = accuracy_score(test_labels, test_predictions)\nprecision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='weighted')\ncm = confusion_matrix(test_labels, test_predictions)\n\nprint(\"\\nüìä FINAL TEST RESULTS (Vision-Only DenseNet161):\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Precision: {precision:.4f}\")\nprint(f\"Test Recall: {recall:.4f}\")\nprint(f\"Test F1-Score: {f1:.4f}\")\nprint(f\"Test Loss: {total_test_loss/len(test_loader):.4f}\")\nprint(f\"\\nConfusion Matrix:\\n{cm}\")\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£3Ô∏è‚É£ CLASS-WISE METRICS\n# ================================================\nprint(\"\\nüìã CLASS-WISE METRICS:\")\nprecision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n    test_labels, test_predictions, average=None\n)\n\nclass_names = ['Negative', 'Neutral', 'Positive']\nfor i, class_name in enumerate(class_names):\n    print(f\"{class_name}: Precision={precision_per_class[i]:.4f}, Recall={recall_per_class[i]:.4f}, F1={f1_per_class[i]:.4f}, Support={support[i]}\")\n\nprint(f\"\\nDataset Distribution:\")\nprint(f\"Training samples: {len(train_df)}\")\nprint(f\"Validation samples: {len(val_df)}\")\nprint(f\"Test samples: {len(test_df)}\")\nprint(f\"Class distribution in training set:\")\nprint(train_df['label'].value_counts().sort_index())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:15:14.041996Z","iopub.execute_input":"2025-07-07T08:15:14.042808Z","iopub.status.idle":"2025-07-07T08:23:58.170368Z","shell.execute_reply.started":"2025-07-07T08:15:14.042771Z","shell.execute_reply":"2025-07-07T08:23:58.169261Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet161_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet161_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /root/.cache/torch/hub/checkpoints/densenet161-8d451a50.pth\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 110M/110M [00:00<00:00, 158MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Training on 3156 samples\nValidating on 451 samples\nTesting on 902 samples\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:59<00:00,  3.31it/s]\nValidation Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:07<00:00,  3.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/25]\nTrain Loss: 1.0386 | Train Acc: 0.4743\nVal Loss: 0.8988 | Val Acc: 0.6341\nLearning Rate: 0.000010\n--------------------------------------------------\n‚úÖ Validation loss improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:53<00:00,  3.69it/s]\nValidation Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:06<00:00,  4.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/25]\nTrain Loss: 0.9241 | Train Acc: 0.5551\nVal Loss: 0.8815 | Val Acc: 0.6297\nLearning Rate: 0.000010\n--------------------------------------------------\n‚úÖ Validation loss improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:53<00:00,  3.69it/s]\nValidation Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:06<00:00,  4.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/25]\nTrain Loss: 0.8717 | Train Acc: 0.6030\nVal Loss: 0.8719 | Val Acc: 0.6253\nLearning Rate: 0.000010\n--------------------------------------------------\n‚úÖ Validation loss improved ‚Äî model saved.\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:53<00:00,  3.71it/s]\nValidation Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:06<00:00,  4.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/25]\nTrain Loss: 0.8296 | Train Acc: 0.6128\nVal Loss: 0.8859 | Val Acc: 0.6408\nLearning Rate: 0.000010\n--------------------------------------------------\n‚è∞ No improvement ‚Äî patience 1/5\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:53<00:00,  3.69it/s]\nValidation Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:06<00:00,  4.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/25]\nTrain Loss: 0.7903 | Train Acc: 0.6404\nVal Loss: 0.8882 | Val Acc: 0.6297\nLearning Rate: 0.000010\n--------------------------------------------------\n‚è∞ No improvement ‚Äî patience 2/5\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:54<00:00,  3.67it/s]\nValidation Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:06<00:00,  4.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/25]\nTrain Loss: 0.7458 | Train Acc: 0.6559\nVal Loss: 0.8861 | Val Acc: 0.6319\nLearning Rate: 0.000010\n--------------------------------------------------\n‚è∞ No improvement ‚Äî patience 3/5\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:54<00:00,  3.66it/s]\nValidation Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:06<00:00,  4.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/25]\nTrain Loss: 0.7027 | Train Acc: 0.6844\nVal Loss: 0.9169 | Val Acc: 0.6253\nLearning Rate: 0.000001\n--------------------------------------------------\n‚è∞ No improvement ‚Äî patience 4/5\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [00:54<00:00,  3.67it/s]\nValidation Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:06<00:00,  4.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/25]\nTrain Loss: 0.6355 | Train Acc: 0.7148\nVal Loss: 0.9284 | Val Acc: 0.6120\nLearning Rate: 0.000001\n--------------------------------------------------\n‚è∞ No improvement ‚Äî patience 5/5\nüõë Early stopping triggered at epoch 8\n\nüîç Loading best model for final evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Final Test Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:14<00:00,  4.03it/s]","output_type":"stream"},{"name":"stdout","text":"\nüìä FINAL TEST RESULTS (Vision-Only DenseNet161):\nTest Accuracy: 0.6319\nTest Precision: 0.6488\nTest Recall: 0.6319\nTest F1-Score: 0.6368\nTest Loss: 0.8193\n\nConfusion Matrix:\n[[257  96  49]\n [ 72 224  57]\n [ 15  43  89]]\n\nüìã CLASS-WISE METRICS:\nNegative: Precision=0.7471, Recall=0.6393, F1=0.6890, Support=402\nNeutral: Precision=0.6171, Recall=0.6346, F1=0.6257, Support=353\nPositive: Precision=0.4564, Recall=0.6054, F1=0.5205, Support=147\n\nDataset Distribution:\nTraining samples: 3156\nValidation samples: 451\nTest samples: 902\nClass distribution in training set:\nlabel\n0    1404\n1    1237\n2     515\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1}]}
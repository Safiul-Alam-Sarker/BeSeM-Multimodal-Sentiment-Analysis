{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12355137,"sourceType":"datasetVersion","datasetId":7789382}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================\n# ‚úÖ 1Ô∏è‚É£ LIBRARIES & SETUP\n# ================================================\nimport os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import AutoImageProcessor, SwinForImageClassification\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nimport torch.nn as nn\n\n# ================================================\n# ‚úÖ 2Ô∏è‚É£ PATHS\n# ================================================\nimage_dir = \"/kaggle/input/basem/images\"\ninput_csv = \"/kaggle/input/basem/dataset.csv\"\n\n# ================================================\n# ‚úÖ 3Ô∏è‚É£ LOAD & PREPROCESS CSV\n# ================================================\ndf = pd.read_csv(input_csv)\n\nexisting_data = []\nfor _, row in df.iterrows():\n    image_filename = row['image_path']\n    full_image_path = os.path.join(image_dir, image_filename)\n    if os.path.exists(full_image_path):\n        label_converted = row['label 2'] - 1\n        existing_data.append({\n            'Image_path': full_image_path,\n            'Label_Sentiment': label_converted\n        })\n\nprocessed_df = pd.DataFrame(existing_data)\n\n# ================================================\n# ‚úÖ 4Ô∏è‚É£ DATA SPLITS\n# ================================================\ntrain_df, temp_df = train_test_split(processed_df, test_size=0.3, stratify=processed_df['Label_Sentiment'], random_state=42)\ntest_df, val_df = train_test_split(temp_df, test_size=1/3, stratify=temp_df['Label_Sentiment'], random_state=42)\n\n# Add label column for consistency\nfor df_name, df_ in [('train', train_df), ('test', test_df), ('val', val_df)]:\n    df_['label'] = df_['Label_Sentiment']\n    df_.to_csv(f'/kaggle/working/{df_name}_vision_only.csv', index=False)\n\nprint(f\"Train samples: {len(train_df)}, Val samples: {len(val_df)}, Test samples: {len(test_df)}\")\n\n# ================================================\n# ‚úÖ 5Ô∏è‚É£ LOAD SWIN TRANSFORMER\n# ================================================\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load Swin Transformer for image classification\nmodel_name = \"microsoft/swin-base-patch4-window7-224\"\nprocessor = AutoImageProcessor.from_pretrained(model_name)\n\n# Load model with ignore_mismatched_sizes to handle classifier mismatch\nswin_model = SwinForImageClassification.from_pretrained(\n    model_name,\n    num_labels=3,  # Set number of classes to 3\n    ignore_mismatched_sizes=True  # Ignore size mismatch for classifier\n)\nswin_model = swin_model.to(device)\n\n# ================================================\n# ‚úÖ 6Ô∏è‚É£ VISION DATASET\n# ================================================\nclass VisionDataset(Dataset):\n    def __init__(self, df, processor):\n        self.df = df\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(row['Image_path']).convert('RGB')\n        label = row['label']\n        \n        # Process image with Swin processor\n        inputs = self.processor(image, return_tensors=\"pt\")\n        pixel_values = inputs['pixel_values'].squeeze(0)  # Remove batch dimension\n        \n        return pixel_values, label\n\ndef collate_fn(batch):\n    pixel_values, labels = zip(*batch)\n    pixel_values = torch.stack(pixel_values)\n    labels = torch.tensor(labels)\n    return pixel_values, labels\n\n# ================================================\n# ‚úÖ 7Ô∏è‚É£ DATALOADERS\n# ================================================\nbatch_size = 8\n\ntrain_loader = DataLoader(VisionDataset(train_df, processor), batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(VisionDataset(val_df, processor), batch_size=batch_size, collate_fn=collate_fn)\ntest_loader = DataLoader(VisionDataset(test_df, processor), batch_size=batch_size, collate_fn=collate_fn)\n\n# ================================================\n# ‚úÖ 8Ô∏è‚É£ LOSS & OPTIMIZER\n# ================================================\n# Calculate class weights for balanced training\nclass_weights = train_df['label'].value_counts().sort_index().tolist()\ntotal = sum(class_weights)\nweights = [total / c for c in class_weights]\nprint(f\"Class distribution: {class_weights}\")\nprint(f\"Class weights: {weights}\")\n\ncriterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(weights).to(device))\noptimizer = AdamW(swin_model.parameters(), lr=1e-5)  # Lower learning rate for pre-trained model\n\n# ================================================\n# ‚úÖ 9Ô∏è‚É£ TRAINING LOOP\n# ================================================\nnum_epochs = 20\npatience = 3\npatience_counter = 0\nbest_val_loss = float('inf')\n\nprint(\"üöÄ Starting training...\")\n\nfor epoch in range(num_epochs):\n    # ============================================================\n    # TRAINING PHASE\n    # ============================================================\n    swin_model.train()\n    total_train_loss = 0\n    train_predictions = []\n    train_labels = []\n\n    for pixel_values, labels in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n        pixel_values = pixel_values.to(device)\n        labels = labels.to(device)\n        \n        outputs = swin_model(pixel_values=pixel_values, labels=labels)\n        loss = outputs.loss\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        total_train_loss += loss.item()\n        \n        # Store predictions for accuracy calculation\n        predictions = torch.argmax(outputs.logits, dim=1)\n        train_predictions.extend(predictions.cpu().numpy())\n        train_labels.extend(labels.cpu().numpy())\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_accuracy = accuracy_score(train_labels, train_predictions)\n\n    # ============================================================\n    # VALIDATION PHASE\n    # ============================================================\n    swin_model.eval()\n    total_val_loss = 0\n    val_predictions = []\n    val_labels = []\n\n    with torch.no_grad():\n        for pixel_values, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n            pixel_values = pixel_values.to(device)\n            labels = labels.to(device)\n            \n            outputs = swin_model(pixel_values=pixel_values, labels=labels)\n            loss = outputs.loss\n            \n            total_val_loss += loss.item()\n            \n            # Store predictions for metrics\n            predictions = torch.argmax(outputs.logits, dim=1)\n            val_predictions.extend(predictions.cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = accuracy_score(val_labels, val_predictions)\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f}\")\n    print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n\n    # ============================================================\n    # EARLY STOPPING CHECK\n    # ============================================================\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(swin_model.state_dict(), \"best_swin_model.pt\")\n        print(\"‚úÖ Validation loss improved ‚Äî model saved.\")\n    else:\n        patience_counter += 1\n        print(f\"‚è∞ No improvement ‚Äî patience {patience_counter}/{patience}\")\n\n        if patience_counter >= patience:\n            print(f\"üõë Early stopping triggered at epoch {epoch+1}\")\n            break\n    print(\"-\" * 50)\n\n# ================================================\n# ‚úÖ üîü FINAL TEST EVALUATION\n# ================================================\nprint(\"\\nüîç Loading best model for final evaluation...\")\nswin_model.load_state_dict(torch.load(\"best_swin_model.pt\"))\nswin_model.eval()\n\ntest_predictions = []\ntest_labels = []\ntotal_test_loss = 0\n\nwith torch.no_grad():\n    for pixel_values, labels in tqdm(test_loader, desc=\"Final Test Evaluation\"):\n        pixel_values = pixel_values.to(device)\n        labels = labels.to(device)\n        \n        outputs = swin_model(pixel_values=pixel_values, labels=labels)\n        loss = outputs.loss\n        \n        total_test_loss += loss.item()\n        \n        predictions = torch.argmax(outputs.logits, dim=1)\n        test_predictions.extend(predictions.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\n# Calculate final metrics\ntest_accuracy = accuracy_score(test_labels, test_predictions)\nprecision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='weighted')\ncm = confusion_matrix(test_labels, test_predictions)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üìä FINAL TEST RESULTS - VISION ONLY (SWIN TRANSFORMER)\")\nprint(\"=\"*60)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Precision: {precision:.4f}\")\nprint(f\"Test Recall: {recall:.4f}\")\nprint(f\"Test F1-Score: {f1:.4f}\")\nprint(f\"Test Loss: {total_test_loss/len(test_loader):.4f}\")\nprint(f\"\\nConfusion Matrix:\\n{cm}\")\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£1Ô∏è‚É£ DETAILED CLASSIFICATION REPORT\n# ================================================\nfrom sklearn.metrics import classification_report\n\nprint(\"\\nüìã Detailed Classification Report:\")\nprint(classification_report(test_labels, test_predictions, \n                          target_names=['Negative', 'Neutral', 'Positive']))\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£2Ô∏è‚É£ SAVE RESULTS\n# ================================================\nresults = {\n    'test_accuracy': test_accuracy,\n    'test_precision': precision,\n    'test_recall': recall,\n    'test_f1': f1,\n    'test_loss': total_test_loss/len(test_loader),\n    'confusion_matrix': cm.tolist()\n}\n\nimport json\nwith open('/kaggle/working/swin_vision_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\"\\n‚úÖ Results saved to 'swin_vision_results.json'\")\nprint(\"üéØ Vision-only model evaluation complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:24:29.506344Z","iopub.execute_input":"2025-07-07T08:24:29.506967Z","iopub.status.idle":"2025-07-07T08:43:52.146369Z","shell.execute_reply.started":"2025-07-07T08:24:29.506912Z","shell.execute_reply":"2025-07-07T08:43:52.145457Z"}},"outputs":[{"name":"stderr","text":"2025-07-07 08:24:40.486085: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751876680.682515      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751876680.735618      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Train samples: 3156, Val samples: 451, Test samples: 902\nUsing device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6170bd545a7d428a8323c4efc0e49aad"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc953a8efdcf4831adf80568aa801175"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/352M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0a7f6445da7473cb397b88ce45449f5"}},"metadata":{}},{"name":"stderr","text":"Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([3, 1024]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Class distribution: [1404, 1237, 515]\nClass weights: [2.247863247863248, 2.551333872271625, 6.128155339805825]\nüöÄ Starting training...\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:33<00:00,  1.85it/s]\nValidation Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:23<00:00,  2.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/20]\n  Train Loss: 0.8610 | Train Acc: 0.6001\n  Val Loss: 0.7841 | Val Acc: 0.6452\n‚úÖ Validation loss improved ‚Äî model saved.\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:08<00:00,  2.09it/s]\nValidation Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/20]\n  Train Loss: 0.6577 | Train Acc: 0.7177\n  Val Loss: 0.7746 | Val Acc: 0.6696\n‚úÖ Validation loss improved ‚Äî model saved.\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:09<00:00,  2.08it/s]\nValidation Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:20<00:00,  2.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/20]\n  Train Loss: 0.4867 | Train Acc: 0.8013\n  Val Loss: 0.8658 | Val Acc: 0.6519\n‚è∞ No improvement ‚Äî patience 1/3\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:09<00:00,  2.09it/s]\nValidation Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/20]\n  Train Loss: 0.3452 | Train Acc: 0.8739\n  Val Loss: 0.9082 | Val Acc: 0.6585\n‚è∞ No improvement ‚Äî patience 2/3\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 395/395 [03:09<00:00,  2.09it/s]\nValidation Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:21<00:00,  2.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/20]\n  Train Loss: 0.2771 | Train Acc: 0.8929\n  Val Loss: 0.9997 | Val Acc: 0.6386\n‚è∞ No improvement ‚Äî patience 3/3\nüõë Early stopping triggered at epoch 5\n\nüîç Loading best model for final evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Final Test Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 113/113 [00:45<00:00,  2.48it/s]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüìä FINAL TEST RESULTS - VISION ONLY (SWIN TRANSFORMER)\n============================================================\nTest Accuracy: 0.6818\nTest Precision: 0.6935\nTest Recall: 0.6818\nTest F1-Score: 0.6787\nTest Loss: 0.7119\n\nConfusion Matrix:\n[[283 108  11]\n [ 75 266  12]\n [ 21  60  66]]\n\nüìã Detailed Classification Report:\n              precision    recall  f1-score   support\n\n    Negative       0.75      0.70      0.72       402\n     Neutral       0.61      0.75      0.68       353\n    Positive       0.74      0.45      0.56       147\n\n    accuracy                           0.68       902\n   macro avg       0.70      0.64      0.65       902\nweighted avg       0.69      0.68      0.68       902\n\n\n‚úÖ Results saved to 'swin_vision_results.json'\nüéØ Vision-only model evaluation complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12355137,"sourceType":"datasetVersion","datasetId":7789382}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================\n# ‚úÖ 1Ô∏è‚É£ LIBRARIES & SETUP\n# ================================================\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom torchvision import transforms, models\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n\n# ================================================\n# ‚úÖ 2Ô∏è‚É£ PATHS\n# ================================================\nimage_dir = \"/kaggle/input/basem/images\"\ninput_csv = \"/kaggle/input/basem/dataset.csv\"\n\n# ================================================\n# ‚úÖ 3Ô∏è‚É£ LOAD & PREPROCESS CSV\n# ================================================\ndf = pd.read_csv(input_csv)\n\nexisting_data = []\nfor _, row in df.iterrows():\n    image_filename = row['image_path']\n    full_image_path = os.path.join(image_dir, image_filename)\n    if os.path.exists(full_image_path):\n        label_converted = row['label 2'] - 1\n        existing_data.append({\n            'Image_path': full_image_path,\n            'Label_Sentiment': label_converted\n        })\n\nprocessed_df = pd.DataFrame(existing_data)\n\n# ================================================\n# ‚úÖ 4Ô∏è‚É£ DATA SPLITTING\n# ================================================\ntrain_df, temp_df = train_test_split(processed_df, test_size=0.3, stratify=processed_df['Label_Sentiment'], random_state=42)\ntest_df, val_df = train_test_split(temp_df, test_size=1/3, stratify=temp_df['Label_Sentiment'], random_state=42)\n\nfor df_name, df_ in [('train', train_df), ('test', test_df), ('val', val_df)]:\n    df_['label'] = df_['Label_Sentiment']\n    df_.to_csv(f'/kaggle/working/{df_name}_vision_only.csv', index=False)\n\n# ================================================\n# ‚úÖ 5Ô∏è‚É£ IMAGE TRANSFORMS\n# ================================================\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_test_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# ================================================\n# ‚úÖ 6Ô∏è‚É£ DATASET CLASS\n# ================================================\nclass VisionOnlyDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(row['Image_path']).convert('RGB')\n        label = row['label']\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n# ================================================\n# ‚úÖ 7Ô∏è‚É£ DATALOADERS\n# ================================================\nbatch_size = 16\n\ntrain_loader = DataLoader(\n    VisionOnlyDataset(train_df, train_transform), \n    batch_size=batch_size, \n    shuffle=True\n)\nval_loader = DataLoader(\n    VisionOnlyDataset(val_df, val_test_transform), \n    batch_size=batch_size, \n    shuffle=False\n)\ntest_loader = DataLoader(\n    VisionOnlyDataset(test_df, val_test_transform), \n    batch_size=batch_size, \n    shuffle=False\n)\n\n# ================================================\n# ‚úÖ 8Ô∏è‚É£ VGG16 MODEL\n# ================================================\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nclass VGG16Classifier(nn.Module):\n    def __init__(self, num_classes=3):\n        super(VGG16Classifier, self).__init__()\n        # Load pre-trained VGG16\n        self.vgg16 = models.vgg16(pretrained=True)\n        \n        # Freeze early layers (optional - comment out if you want to fine-tune all layers)\n        for param in self.vgg16.features.parameters():\n            param.requires_grad = False\n        \n        # Replace the classifier\n        self.vgg16.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, num_classes)\n        )\n    \n    def forward(self, x):\n        return self.vgg16(x)\n\nmodel = VGG16Classifier(num_classes=3).to(device)\n\n# ================================================\n# ‚úÖ 9Ô∏è‚É£ LOSS & OPTIMIZER\n# ================================================\nclass_weights = train_df['label'].value_counts().sort_index().tolist()\ntotal = sum(class_weights)\nweights = [total / c for c in class_weights]\ncriterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(weights).to(device))\noptimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n\n# ================================================\n# ‚úÖ üîü TRAINING LOOP\n# ================================================\nnum_epochs = 20\npatience = 3\npatience_counter = 0\nbest_val_loss = float('inf')\n\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    # ============================================================\n    # TRAINING PHASE\n    # ============================================================\n    model.train()\n    total_train_loss = 0\n    train_predictions = []\n    train_labels = []\n\n    for images, labels in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_train_loss += loss.item()\n        \n        # Store predictions for metrics\n        predictions = torch.argmax(outputs, dim=1)\n        train_predictions.extend(predictions.cpu().numpy())\n        train_labels.extend(labels.cpu().numpy())\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    train_accuracy = accuracy_score(train_labels, train_predictions)\n\n    # ============================================================\n    # VALIDATION PHASE\n    # ============================================================\n    model.eval()\n    total_val_loss = 0\n    val_predictions = []\n    val_labels = []\n\n    with torch.no_grad():\n        for images, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            total_val_loss += loss.item()\n            \n            # Store predictions for metrics\n            predictions = torch.argmax(outputs, dim=1)\n            val_predictions.extend(predictions.cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_accuracy = accuracy_score(val_labels, val_predictions)\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n    print(f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f}\")\n    print(f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n\n    # ============================================================\n    # EARLY STOPPING CHECK\n    # ============================================================\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), \"best_vgg16_model.pt\")\n        print(\"‚úÖ Validation loss improved ‚Äî model saved.\")\n    else:\n        patience_counter += 1\n        print(f\"‚è∞ No improvement ‚Äî patience {patience_counter}/{patience}\")\n\n        if patience_counter >= patience:\n            print(f\"üõë Early stopping triggered at epoch {epoch+1}\")\n            break\n    print(\"-\" * 50)\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£1Ô∏è‚É£ FINAL TEST EVALUATION\n# ================================================\nprint(\"\\nüîç Loading best model for final evaluation...\")\nmodel.load_state_dict(torch.load(\"best_vgg16_model.pt\"))\nmodel.eval()\n\ntest_predictions = []\ntest_labels = []\ntotal_test_loss = 0\n\nwith torch.no_grad():\n    for images, labels in tqdm(test_loader, desc=\"Final Test Evaluation\"):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        total_test_loss += loss.item()\n        \n        predictions = torch.argmax(outputs, dim=1)\n        test_predictions.extend(predictions.cpu().numpy())\n        test_labels.extend(labels.cpu().numpy())\n\n# Calculate final metrics\ntest_accuracy = accuracy_score(test_labels, test_predictions)\nprecision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='weighted')\ncm = confusion_matrix(test_labels, test_predictions)\n\nprint(\"\\nüìä FINAL TEST RESULTS (Vision-Only VGG16):\")\nprint(\"=\" * 50)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Test Precision: {precision:.4f}\")\nprint(f\"Test Recall: {recall:.4f}\")\nprint(f\"Test F1-Score: {f1:.4f}\")\nprint(f\"Test Loss: {total_test_loss/len(test_loader):.4f}\")\nprint(f\"\\nConfusion Matrix:\\n{cm}\")\n\n# ================================================\n# ‚úÖ 1Ô∏è‚É£2Ô∏è‚É£ DETAILED METRICS BY CLASS\n# ================================================\nprecision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n    test_labels, test_predictions, average=None\n)\n\nprint(\"\\nüìà DETAILED METRICS BY CLASS:\")\nprint(\"=\" * 50)\nfor i in range(len(precision_per_class)):\n    print(f\"Class {i}: Precision={precision_per_class[i]:.4f}, Recall={recall_per_class[i]:.4f}, F1={f1_per_class[i]:.4f}, Support={support[i]}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:45:10.933004Z","iopub.execute_input":"2025-07-07T08:45:10.933257Z","iopub.status.idle":"2025-07-07T08:55:06.561552Z","shell.execute_reply.started":"2025-07-07T08:45:10.933235Z","shell.execute_reply":"2025-07-07T08:55:06.560750Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 528M/528M [00:02<00:00, 232MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [01:52<00:00,  1.76it/s]\nValidation Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:13<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/20]\nTrain Loss: 1.0923 | Train Acc: 0.4895\nVal Loss: 0.9162 | Val Acc: 0.5698\n‚úÖ Validation loss improved ‚Äî model saved.\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [01:34<00:00,  2.10it/s]\nValidation Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:11<00:00,  2.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/20]\nTrain Loss: 0.9082 | Train Acc: 0.5779\nVal Loss: 0.8958 | Val Acc: 0.5854\n‚úÖ Validation loss improved ‚Äî model saved.\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [01:34<00:00,  2.10it/s]\nValidation Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:11<00:00,  2.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/20]\nTrain Loss: 0.8382 | Train Acc: 0.6229\nVal Loss: 0.9193 | Val Acc: 0.6164\n‚è∞ No improvement ‚Äî patience 1/3\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [01:32<00:00,  2.14it/s]\nValidation Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:11<00:00,  2.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/20]\nTrain Loss: 0.7809 | Train Acc: 0.6502\nVal Loss: 1.0010 | Val Acc: 0.6142\n‚è∞ No improvement ‚Äî patience 2/3\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [01:32<00:00,  2.15it/s]\nValidation Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:11<00:00,  2.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/20]\nTrain Loss: 0.7105 | Train Acc: 0.6746\nVal Loss: 0.9971 | Val Acc: 0.6120\n‚è∞ No improvement ‚Äî patience 3/3\nüõë Early stopping triggered at epoch 5\n\nüîç Loading best model for final evaluation...\n","output_type":"stream"},{"name":"stderr","text":"Final Test Evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57/57 [00:26<00:00,  2.14it/s]","output_type":"stream"},{"name":"stdout","text":"\nüìä FINAL TEST RESULTS (Vision-Only VGG16):\n==================================================\nTest Accuracy: 0.5876\nTest Precision: 0.6131\nTest Recall: 0.5876\nTest F1-Score: 0.5920\nTest Loss: 0.8738\n\nConfusion Matrix:\n[[221 136  45]\n [ 62 228  63]\n [ 19  47  81]]\n\nüìà DETAILED METRICS BY CLASS:\n==================================================\nClass 0: Precision=0.7318, Recall=0.5498, F1=0.6278, Support=402\nClass 1: Precision=0.5547, Recall=0.6459, F1=0.5969, Support=353\nClass 2: Precision=0.4286, Recall=0.5510, F1=0.4821, Support=147\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1}]}